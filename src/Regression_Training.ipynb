{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\" \n",
    "import keras\n",
    "import sent2vec\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq\n",
    "from seq2seq.models import Seq2Seq\n",
    "from keras.utils import multi_gpu_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "from keras.utils import *\n",
    "from sklearn.model_selection import *\n",
    "import random\n",
    "from random import shuffle\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from keras.utils.generic_utils import *\n",
    "from keras import regularizers\n",
    "import string\n",
    "import unicodedata as udata\n",
    "import pickle\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "import pause, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import *\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    111023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'henningsen',\n",
       " 'hammermann',\n",
       " 'izvxeiprjn',\n",
       " 'rechtstreek',\n",
       " 'worsley',\n",
       " 'bastrykin',\n",
       " 'revolutioni',\n",
       " 'reach',\n",
       " 'cistercensi',\n",
       " 'xl',\n",
       " 'antioch',\n",
       " 'morguli',\n",
       " 'hofft',\n",
       " 'rollback',\n",
       " 'excess',\n",
       " 'luen',\n",
       " 'monetizaci',\n",
       " 'swa',\n",
       " 'tutankhamun',\n",
       " 'motoko',\n",
       " 'feh',\n",
       " 'naonwant',\n",
       " 'zenko',\n",
       " 'simeon',\n",
       " 'cheeah',\n",
       " 'signif',\n",
       " 'unrip',\n",
       " 'igeprev',\n",
       " 'resent',\n",
       " 'pdv',\n",
       " 'fgjsuog',\n",
       " 'troubadour',\n",
       " 'hugger',\n",
       " 'uhg',\n",
       " 'jnplnrlefp',\n",
       " 'quarto',\n",
       " 'fruth',\n",
       " 'devletli',\n",
       " 'presstv',\n",
       " 'baarns',\n",
       " 'paschi',\n",
       " 'lioness',\n",
       " 'upham',\n",
       " 'rigoberta',\n",
       " 'vladeck',\n",
       " 'screencrush',\n",
       " 'ahrar',\n",
       " 'kekoa',\n",
       " 'peluch',\n",
       " 'clair',\n",
       " 'walsh',\n",
       " 'crowdsourc',\n",
       " 'winemak',\n",
       " 'aceh',\n",
       " 'nonwork',\n",
       " 'aest',\n",
       " 'unterhaltung',\n",
       " 'fencer',\n",
       " 'questr',\n",
       " 'podestadear',\n",
       " 'auc',\n",
       " 'upsidedowncross',\n",
       " 'stevekingia',\n",
       " 'karencol',\n",
       " 'defed',\n",
       " 'nawa',\n",
       " 'frontgat',\n",
       " 'huntress',\n",
       " 'braveri',\n",
       " 'haze',\n",
       " 'juleanna',\n",
       " 'wineberg',\n",
       " 'ocrbi',\n",
       " 'leake',\n",
       " 'boydton',\n",
       " 'destekles',\n",
       " 'antonin',\n",
       " 'pouvaient',\n",
       " 'antisept',\n",
       " 'hammerhead',\n",
       " 'palma',\n",
       " 'beschwichtigung',\n",
       " 'teaophob',\n",
       " 'tongxi',\n",
       " 'coburn',\n",
       " 'repercuss',\n",
       " 'dumpi',\n",
       " 'ofili',\n",
       " 'allis',\n",
       " 'birdcag',\n",
       " 'angeliefert',\n",
       " 'hyperkinet',\n",
       " 'mollman',\n",
       " 'thabti',\n",
       " 'deaver',\n",
       " 'coordin',\n",
       " 'nonstandard',\n",
       " 'willliam',\n",
       " 'agit',\n",
       " 'onsen',\n",
       " 'theodor',\n",
       " 'trepak',\n",
       " 'transport',\n",
       " 'nevil',\n",
       " 'anselfi',\n",
       " 'gunna',\n",
       " 'rom',\n",
       " 'brennen',\n",
       " 'gatl',\n",
       " 'marde',\n",
       " 'sujeten',\n",
       " 'meral',\n",
       " 'unheilbaren',\n",
       " 'defraud',\n",
       " 'dilacerar',\n",
       " 'magnim',\n",
       " 'organisieren',\n",
       " 'lashio',\n",
       " 'dorada',\n",
       " 'nidaa',\n",
       " 'flimflammeri',\n",
       " 'seamstress',\n",
       " 'ultima',\n",
       " 'orlean',\n",
       " 'limbaugh',\n",
       " 'connoisseur',\n",
       " 'toshaway',\n",
       " 'weiterkommen',\n",
       " 'bardstown',\n",
       " 'holzer',\n",
       " 'nosh',\n",
       " 'allup',\n",
       " 'unforc',\n",
       " 'vigilantcitizen',\n",
       " 'bhakt',\n",
       " 'mocke',\n",
       " 'microturkeyleak',\n",
       " 'sciencemustfal',\n",
       " 'broverman',\n",
       " 'pressitut',\n",
       " 'schlafli',\n",
       " 'conakri',\n",
       " 'hairbraid',\n",
       " 'fishkil',\n",
       " 'stromsto',\n",
       " 'danko',\n",
       " 'julin',\n",
       " 'toner',\n",
       " 'incarcer',\n",
       " 'modernmarket',\n",
       " 'freeston',\n",
       " 'towara',\n",
       " 'tolonew',\n",
       " 'tribuneseditori',\n",
       " 'edward',\n",
       " 'ileana',\n",
       " 'ramael',\n",
       " 'udeid',\n",
       " 'chavismo',\n",
       " 'orquestar',\n",
       " 'lepresti',\n",
       " 'journalist',\n",
       " 'longuich',\n",
       " 'kentoz',\n",
       " 'mezei',\n",
       " 'bamberg',\n",
       " 'morph',\n",
       " 'needler',\n",
       " 'tasti',\n",
       " 'spielz',\n",
       " 'squirter',\n",
       " 'dexedrin',\n",
       " 'anxious',\n",
       " 'ruvolo',\n",
       " 'chirr',\n",
       " 'votefraud',\n",
       " 'reowhnmm',\n",
       " 'logix',\n",
       " 'waterway',\n",
       " 'wrought',\n",
       " 'jernov',\n",
       " 'poshest',\n",
       " 'colchest',\n",
       " 'enrob',\n",
       " 'mountevan',\n",
       " 'entrail',\n",
       " 'rulu',\n",
       " 'irredeem',\n",
       " 'eliseo',\n",
       " 'abrita',\n",
       " 'prohibit',\n",
       " 'cayeron',\n",
       " 'criminalit',\n",
       " 'syryjska',\n",
       " 'seaford',\n",
       " 'arabl',\n",
       " 'negava',\n",
       " 'passerbi',\n",
       " 'undeclar',\n",
       " 'scour',\n",
       " 'crewmemb',\n",
       " 'chapman',\n",
       " 'aun',\n",
       " 'dodderi',\n",
       " 'commercial',\n",
       " 'suakhshqbl',\n",
       " 'lycra',\n",
       " 'localhost',\n",
       " 'esenci',\n",
       " 'absicht',\n",
       " 'targaryen',\n",
       " 'bispo',\n",
       " 'bildgewaltigen',\n",
       " 'sparen',\n",
       " 'memoir',\n",
       " 'greeaaaaat',\n",
       " 'preocuparm',\n",
       " 'roadtripolog',\n",
       " 'dorothi',\n",
       " 'adder',\n",
       " 'himar',\n",
       " 'hossam',\n",
       " 'reprezentowan',\n",
       " 'ban',\n",
       " 'portsmouth',\n",
       " 'paycheck',\n",
       " 'fascinaci',\n",
       " 'holmesjosh',\n",
       " 'float',\n",
       " 'fruitloop',\n",
       " 'summon',\n",
       " 'lauri',\n",
       " 'unrevis',\n",
       " 'wase',\n",
       " 'dedic',\n",
       " 'downingtown',\n",
       " 'bibliotheek',\n",
       " 'horseshit',\n",
       " 'disfavor',\n",
       " 'kander',\n",
       " 'pokeno',\n",
       " 'bewegungssensoren',\n",
       " 'clower',\n",
       " 'acoplan',\n",
       " 'perjor',\n",
       " 'starnet',\n",
       " 'obiettivo',\n",
       " 'anchovi',\n",
       " 'cyberwar',\n",
       " 'homoseksuel',\n",
       " 'socrat',\n",
       " 'dispensari',\n",
       " 'respondist',\n",
       " 'uneven',\n",
       " 'wallen',\n",
       " 'espinoz',\n",
       " 'anteced',\n",
       " 'teasingli',\n",
       " 'steria',\n",
       " 'provocaron',\n",
       " 'thurdsday',\n",
       " 'recomiendo',\n",
       " 'outdo',\n",
       " 'tomscheck',\n",
       " 'seventi',\n",
       " 'obummacar',\n",
       " 'fenech',\n",
       " 'beladi',\n",
       " 'obedecen',\n",
       " 'thief',\n",
       " 'ghaith',\n",
       " 'microagress',\n",
       " 'kannten',\n",
       " 'scrimmag',\n",
       " 'machinima',\n",
       " 'exampl',\n",
       " 'temperament',\n",
       " 'gewartet',\n",
       " 'laudrup',\n",
       " 'stalenbread',\n",
       " 'gemzel',\n",
       " 'tutankhamen',\n",
       " 'smokestack',\n",
       " 'schlitz',\n",
       " 'flonas',\n",
       " 'honeycutt',\n",
       " 'rafeca',\n",
       " 'terroir',\n",
       " 'maria',\n",
       " 'coartada',\n",
       " 'dallaspoliceshoot',\n",
       " 'recoveryless',\n",
       " 'turca',\n",
       " 'pineal',\n",
       " 'temp',\n",
       " 'contribuci',\n",
       " 'greatmakerei',\n",
       " 'mekler',\n",
       " 'catholic',\n",
       " 'submit',\n",
       " 'alsnog',\n",
       " 'farfetch',\n",
       " 'unpardon',\n",
       " 'olio',\n",
       " 'kindercar',\n",
       " 'marzilli',\n",
       " 'alania',\n",
       " 'roosterfish',\n",
       " 'scarier',\n",
       " 'peppi',\n",
       " 'shaivit',\n",
       " 'pandith',\n",
       " 'elaborar',\n",
       " 'cpwetlyy',\n",
       " 'leaden',\n",
       " 'cadelenin',\n",
       " 'bekomm',\n",
       " 'snowbird',\n",
       " 'duval',\n",
       " 'clumsi',\n",
       " 'dreharbeiten',\n",
       " 'lineswoman',\n",
       " 'absent',\n",
       " 'downstair',\n",
       " 'compart',\n",
       " 'tagesspiegel',\n",
       " 'zilh',\n",
       " 'idrocarburi',\n",
       " 'basement',\n",
       " 'mutaz',\n",
       " 'hoenggy',\n",
       " 'clubtothehead',\n",
       " 'repgracemeng',\n",
       " 'dawneywawney',\n",
       " 'vagiano',\n",
       " 'tempera',\n",
       " 'snf',\n",
       " 'eyal',\n",
       " 'ustreasuri',\n",
       " 'caper',\n",
       " 'munjan',\n",
       " 'dunbar',\n",
       " 'feldshuh',\n",
       " 'dyson',\n",
       " 'holog',\n",
       " 'winterfel',\n",
       " 'chauffeur',\n",
       " 'intracrani',\n",
       " 'hordaland',\n",
       " 'hive',\n",
       " 'khishchnik',\n",
       " 'assinatura',\n",
       " 'aspartam',\n",
       " 'roywoodjr',\n",
       " 'grittili',\n",
       " 'takecharg',\n",
       " 'verstandig',\n",
       " 'paktika',\n",
       " 'pagani',\n",
       " 'nashvil',\n",
       " 'safekeep',\n",
       " 'susan',\n",
       " 'gling',\n",
       " 'goodnewsnetwork',\n",
       " 'indianan',\n",
       " 'gila',\n",
       " 'mirochnikoff',\n",
       " 'giudic',\n",
       " 'papyboom',\n",
       " 'blisteri',\n",
       " 'mefeat',\n",
       " 'xiaolu',\n",
       " 'ccrww',\n",
       " 'mataban',\n",
       " 'mercosur',\n",
       " 'robock',\n",
       " 'slown',\n",
       " 'superbias',\n",
       " 'deviant',\n",
       " 'ginzberg',\n",
       " 'culum',\n",
       " 'enriquez',\n",
       " 'diaryofarosi',\n",
       " 'fpe',\n",
       " 'flodmand',\n",
       " 'farhi',\n",
       " 'bashar',\n",
       " 'uci',\n",
       " 'dicaprio',\n",
       " 'haager',\n",
       " 'rockford',\n",
       " 'enamorado',\n",
       " 'tabli',\n",
       " 'purman',\n",
       " 'chrislim',\n",
       " 'metrosexu',\n",
       " 'frumenti',\n",
       " 'apoiar',\n",
       " 'dom',\n",
       " 'impun',\n",
       " 'befehl',\n",
       " 'shulman',\n",
       " 'birkbeck',\n",
       " 'hostel',\n",
       " 'brasilia',\n",
       " 'meixel',\n",
       " 'strat',\n",
       " 'cagr',\n",
       " 'dando',\n",
       " 'bumpi',\n",
       " 'swann',\n",
       " 'cufflink',\n",
       " 'sxlmblck',\n",
       " 'bolivia',\n",
       " 'webzilla',\n",
       " 'handyman',\n",
       " 'mckibben',\n",
       " 'kickass',\n",
       " 'zaheer',\n",
       " 'autograph',\n",
       " 'timestalk',\n",
       " 'hawkishli',\n",
       " 'listo',\n",
       " 'bexley',\n",
       " 'brownle',\n",
       " 'ronenbergman',\n",
       " 'locklear',\n",
       " 'alcadipani',\n",
       " 'cassiu',\n",
       " 'stinki',\n",
       " 'besch',\n",
       " 'mezuza',\n",
       " 'ziel',\n",
       " 'instruccion',\n",
       " 'zavascki',\n",
       " 'garden',\n",
       " 'nonhero',\n",
       " 'bancada',\n",
       " 'leerink',\n",
       " 'birdland',\n",
       " 'elovich',\n",
       " 'naddour',\n",
       " 'fasquel',\n",
       " 'ingeni',\n",
       " 'supercilli',\n",
       " 'quenqua',\n",
       " 'underappreci',\n",
       " 'prefiero',\n",
       " 'infecti',\n",
       " 'zheleznyak',\n",
       " 'heartandstrok',\n",
       " 'whereabout',\n",
       " 'mav',\n",
       " 'jendayi',\n",
       " 'selber',\n",
       " 'verm',\n",
       " 'glycat',\n",
       " 'xviii',\n",
       " 'thew',\n",
       " 'balco',\n",
       " 'ceec',\n",
       " 'paralyt',\n",
       " 'layer',\n",
       " 'hypoxia',\n",
       " 'asociado',\n",
       " 'avrupa',\n",
       " 'uu',\n",
       " 'zxpw',\n",
       " 'consitut',\n",
       " 'opgevolgd',\n",
       " 'ortaya',\n",
       " 'biro',\n",
       " 'gemayel',\n",
       " 'mwh',\n",
       " 'francisco',\n",
       " 'camil',\n",
       " 'bullrich',\n",
       " 'refe',\n",
       " 'colla',\n",
       " 'mercedez',\n",
       " 'yad',\n",
       " 'tillabl',\n",
       " 'pracharak',\n",
       " 'valthepoet',\n",
       " 'irrespect',\n",
       " 'duckworth',\n",
       " 'longbow',\n",
       " 'roxanna',\n",
       " 'cripp',\n",
       " 'wonderlich',\n",
       " 'agricultur',\n",
       " 'khe',\n",
       " 'teleportiert',\n",
       " 'ignio',\n",
       " 'grandstand',\n",
       " 'chilton',\n",
       " 'majorana',\n",
       " 'persbureau',\n",
       " 'ciudadano',\n",
       " 'unexcus',\n",
       " 'cliteri',\n",
       " 'tipsheet',\n",
       " 'quadriparet',\n",
       " 'baeza',\n",
       " 'postdoc',\n",
       " 'douthat',\n",
       " 'cwp',\n",
       " 'moumni',\n",
       " 'patho',\n",
       " 'antinuclear',\n",
       " 'weverton',\n",
       " 'sireci',\n",
       " 'rolli',\n",
       " 'unfurl',\n",
       " 'puntland',\n",
       " 'puent',\n",
       " 'saveusa',\n",
       " 'dond',\n",
       " 'nagelneuen',\n",
       " 'iz',\n",
       " 'vienen',\n",
       " 'samo',\n",
       " 'ambisjonar',\n",
       " 'raphael',\n",
       " 'altura',\n",
       " 'psychopaten',\n",
       " 'gevorgyan',\n",
       " 'rambo',\n",
       " 'nachdem',\n",
       " 'bintumi',\n",
       " 'tingl',\n",
       " 'hatter',\n",
       " 'pilaf',\n",
       " 'therightist',\n",
       " 'infrastructur',\n",
       " 'sciutto',\n",
       " 'suicidio',\n",
       " 'grundlegend',\n",
       " 'enclos',\n",
       " 'soltren',\n",
       " 'goetz',\n",
       " 'design',\n",
       " 'sharmina',\n",
       " 'brexitantrum',\n",
       " 'rentingy',\n",
       " 'faraj',\n",
       " 'dustbin',\n",
       " 'tischner',\n",
       " 'eingetragen',\n",
       " 'khurana',\n",
       " 'zaina',\n",
       " 'bioterror',\n",
       " 'provinz',\n",
       " 'nextgen',\n",
       " 'sinokino',\n",
       " 'sona',\n",
       " 'colvin',\n",
       " 'aysha',\n",
       " 'mb',\n",
       " 'salomon',\n",
       " 'someth',\n",
       " 'redhack',\n",
       " 'gfmucci',\n",
       " 'carioca',\n",
       " 'accelynn',\n",
       " 'guitron',\n",
       " 'lewanci',\n",
       " 'tajani',\n",
       " 'ckton',\n",
       " 'maneuv',\n",
       " 'lamppost',\n",
       " 'fatu',\n",
       " 'aftreden',\n",
       " 'mcmann',\n",
       " 'vecina',\n",
       " 'grainn',\n",
       " 'zika',\n",
       " 'mello',\n",
       " 'dolomit',\n",
       " 'coomi',\n",
       " 'zarco',\n",
       " 'wharv',\n",
       " 'scl',\n",
       " 'damscu',\n",
       " 'weergepraat',\n",
       " 'phanio',\n",
       " 'cymbal',\n",
       " 'transvers',\n",
       " 'vulgamor',\n",
       " 'lape',\n",
       " 'zappala',\n",
       " 'towsend',\n",
       " 'geneva',\n",
       " 'sensori',\n",
       " 'kundun',\n",
       " 'pedophilia',\n",
       " 'beautician',\n",
       " 'freimuth',\n",
       " 'stampa',\n",
       " 'wolfsangel',\n",
       " 'dystop',\n",
       " 'agxiik',\n",
       " 'cesi',\n",
       " 'skynewsbreak',\n",
       " 'persol',\n",
       " 'prejudicando',\n",
       " 'elf',\n",
       " 'premiado',\n",
       " 'primar',\n",
       " 'ghe',\n",
       " 'revelaci',\n",
       " 'succe',\n",
       " 'skandalen',\n",
       " 'verdammten',\n",
       " 'etterforskinga',\n",
       " 'genannten',\n",
       " 'lavaron',\n",
       " 'kesten',\n",
       " 'irr',\n",
       " 'dreamlin',\n",
       " 'truli',\n",
       " 'thatcher',\n",
       " 'leitenden',\n",
       " 'dunlap',\n",
       " 'boatlift',\n",
       " 'smb',\n",
       " 'anta',\n",
       " 'miscommun',\n",
       " 'duke',\n",
       " 'delia',\n",
       " 'factsheet',\n",
       " 'apfelfest',\n",
       " 'het',\n",
       " 'slinki',\n",
       " 'davidoff',\n",
       " 'conceited',\n",
       " 'heuchelei',\n",
       " 'booka',\n",
       " 'oiler',\n",
       " 'sinologist',\n",
       " 'mindstorm',\n",
       " 'macintosh',\n",
       " 'gigantesca',\n",
       " 'szkuki',\n",
       " 'gig',\n",
       " 'perendzhiyev',\n",
       " 'reencontrado',\n",
       " 'similar',\n",
       " 'mariachi',\n",
       " 'caisson',\n",
       " 'mcnee',\n",
       " 'shay',\n",
       " 'regift',\n",
       " 'fig',\n",
       " 'sanlam',\n",
       " 'akihito',\n",
       " 'lamplight',\n",
       " 'gingham',\n",
       " 'conro',\n",
       " 'voladora',\n",
       " 'maoist',\n",
       " 'patridg',\n",
       " 'prickett',\n",
       " 'steilberg',\n",
       " 'illustrer',\n",
       " 'berniesteach',\n",
       " 'trocadero',\n",
       " 'mixolog',\n",
       " 'kohli',\n",
       " 'uah',\n",
       " 'constel',\n",
       " 'kindergartn',\n",
       " 'mirar',\n",
       " 'krijgen',\n",
       " 'bergum',\n",
       " 'seppuku',\n",
       " 'candidata',\n",
       " 'nudg',\n",
       " 'limeston',\n",
       " 'clintonland',\n",
       " 'dwarfism',\n",
       " 'percipi',\n",
       " 'sabra',\n",
       " 'pusey',\n",
       " 'jafra',\n",
       " 'aider',\n",
       " 'balat',\n",
       " 'bryson',\n",
       " 'iliac',\n",
       " 'keiko',\n",
       " 'hitchhik',\n",
       " 'buttiglion',\n",
       " 'delirium',\n",
       " 'dbp',\n",
       " 'enlai',\n",
       " 'knottiest',\n",
       " 'finanziaria',\n",
       " 'dharun',\n",
       " 'invite',\n",
       " 'byway',\n",
       " 'nasjonalgarden',\n",
       " 'churchman',\n",
       " 'awad',\n",
       " 'pace',\n",
       " 'lpez',\n",
       " 'chambless',\n",
       " 'mouhajir',\n",
       " 'xyagiuxw',\n",
       " 'precipit',\n",
       " 'nanprev',\n",
       " 'audacia',\n",
       " 'dysbiosi',\n",
       " 'wussten',\n",
       " 'deguzman',\n",
       " 'reservado',\n",
       " 'exculpatori',\n",
       " 'jimrenacci',\n",
       " 'vqoi',\n",
       " 'cowi',\n",
       " 'vietnman',\n",
       " 'wfmi',\n",
       " 'wizbat',\n",
       " 'npr',\n",
       " 'lozzi',\n",
       " 'cofina',\n",
       " 'warmup',\n",
       " 'joybird',\n",
       " 'scrutini',\n",
       " 'videoctr',\n",
       " 'tobbi',\n",
       " 'canna',\n",
       " 'budget',\n",
       " 'forestland',\n",
       " 'livraison',\n",
       " 'auch',\n",
       " 'petici',\n",
       " 'diction',\n",
       " 'denizind',\n",
       " 'vauxhal',\n",
       " 'predynast',\n",
       " 'redflagnew',\n",
       " 'mukherjea',\n",
       " 'huntli',\n",
       " 'aqsvh',\n",
       " 'cafe',\n",
       " 'rudin',\n",
       " 'dronabinol',\n",
       " 'khorsabad',\n",
       " 'dantzscher',\n",
       " 'pilshik',\n",
       " 'objectif',\n",
       " 'tambora',\n",
       " 'englishman',\n",
       " 'israelindependenceday',\n",
       " 'saranac',\n",
       " 'andrelton',\n",
       " 'chrysal',\n",
       " 'basil',\n",
       " 'marubeni',\n",
       " 'engineeringwis',\n",
       " 'schaumburg',\n",
       " 'chinamasa',\n",
       " 'hartland',\n",
       " 'morningstar',\n",
       " 'lvmhestim',\n",
       " 'truethat',\n",
       " 'mcx',\n",
       " 'cydney',\n",
       " 'mare',\n",
       " 'perv',\n",
       " 'kokajli',\n",
       " 'personnel',\n",
       " 'arteriosclerot',\n",
       " 'fireman',\n",
       " 'portugues',\n",
       " 'remount',\n",
       " 'origami',\n",
       " 'objektiven',\n",
       " 'ferg',\n",
       " 'counten',\n",
       " 'tgl',\n",
       " 'indestruct',\n",
       " 'hookup',\n",
       " 'compos',\n",
       " 'haggai',\n",
       " 'buhruf',\n",
       " 'kashmiriyat',\n",
       " 'mcmath',\n",
       " 'jetson',\n",
       " 'saidi',\n",
       " 'aunt',\n",
       " 'boj',\n",
       " 'foreverychild',\n",
       " 'spacey',\n",
       " 'rental',\n",
       " 'bearsden',\n",
       " 'powerhold',\n",
       " 'sugare',\n",
       " 'saroli',\n",
       " 'somberli',\n",
       " 'willmor',\n",
       " 'whine',\n",
       " 'dalmia',\n",
       " 'veraz',\n",
       " 'fe',\n",
       " 'prognosi',\n",
       " 'haj',\n",
       " 'horasani',\n",
       " 'usdefensewatch',\n",
       " 'yitzhak',\n",
       " 'openhand',\n",
       " 'ndxg',\n",
       " 'requiem',\n",
       " 'mrl',\n",
       " 'gawp',\n",
       " 'madelin',\n",
       " 'rzt',\n",
       " 'moen',\n",
       " 'recycli',\n",
       " 'algerian',\n",
       " 'ongo',\n",
       " 'partyleadership',\n",
       " 'klansman',\n",
       " 'yorker',\n",
       " 'gemelli',\n",
       " 'convict',\n",
       " 'kraus',\n",
       " 'delphic',\n",
       " 'mitgemacht',\n",
       " 'diaper',\n",
       " 'cadillac',\n",
       " 'riayet',\n",
       " 'coyot',\n",
       " 'nineteenth',\n",
       " 'potu',\n",
       " 'heien',\n",
       " 'voyou',\n",
       " 'ashland',\n",
       " 'sherizi',\n",
       " 'yonder',\n",
       " 'mmr',\n",
       " 'butdoesthismeanicanwinagrammi',\n",
       " 'bene',\n",
       " 'wvmihyzuhz',\n",
       " 'farinha',\n",
       " 'yalow',\n",
       " 'volvic',\n",
       " 'speech',\n",
       " 'donoghu',\n",
       " 'damariscotta',\n",
       " 'julianna',\n",
       " 'nrp',\n",
       " 'yelizaveta',\n",
       " 'bangui',\n",
       " 'decrypt',\n",
       " 'underdiagnos',\n",
       " 'changeless',\n",
       " 'malunterricht',\n",
       " 'hizo',\n",
       " 'englischen',\n",
       " 'gti',\n",
       " 'stargaz',\n",
       " 'unsavouri',\n",
       " 'injera',\n",
       " 'farragut',\n",
       " 'immobl',\n",
       " 'girolamo',\n",
       " 'slump',\n",
       " 'tencent',\n",
       " 'iauliai',\n",
       " 'oldu',\n",
       " 'indica',\n",
       " 'yav',\n",
       " 'bosa',\n",
       " 'slim',\n",
       " 'osso',\n",
       " 'arctischepinguin',\n",
       " 'gie',\n",
       " 'sistema',\n",
       " 'benz',\n",
       " 'barefield',\n",
       " 'heiqiaocun',\n",
       " 'maxick',\n",
       " 'ritterschlag',\n",
       " 'anatomi',\n",
       " 'fuad',\n",
       " 'triphosph',\n",
       " 'cartwright',\n",
       " 'precaut',\n",
       " 'conscienti',\n",
       " 'ibiza',\n",
       " 'imaplaneiac',\n",
       " 'amtsvorg',\n",
       " 'rud',\n",
       " 'jn',\n",
       " 'nagorno',\n",
       " 'mindfulcop',\n",
       " 'hado',\n",
       " 'respons',\n",
       " 'bacchi',\n",
       " 'afaf',\n",
       " 'choo',\n",
       " 'chatusriphithak',\n",
       " 'jovial',\n",
       " 'curcumin',\n",
       " 'address',\n",
       " 'luoyang',\n",
       " 'kamer',\n",
       " 'maggi',\n",
       " 'aislamiento',\n",
       " 'grotesqu',\n",
       " 'minski',\n",
       " 'mccullough',\n",
       " 'angelon',\n",
       " 'squadra',\n",
       " 'odel',\n",
       " 'exhum',\n",
       " 'bitfinex',\n",
       " 'hinterlassen',\n",
       " 'herserverof',\n",
       " 'nepali',\n",
       " 'alfon',\n",
       " 'dumbbel',\n",
       " 'taliban',\n",
       " 'gubernatori',\n",
       " 'verdankt',\n",
       " 'dout',\n",
       " 'tormento',\n",
       " 'forebear',\n",
       " 'mescalin',\n",
       " 'johno',\n",
       " 'posh',\n",
       " 'enseign',\n",
       " 'serl',\n",
       " 'nullifi',\n",
       " 'laennec',\n",
       " 'fairview',\n",
       " 'oldi',\n",
       " 'frantic',\n",
       " 'danodradio',\n",
       " 'stud',\n",
       " 'himaya',\n",
       " 'lago',\n",
       " 'uyiwzcxlxwqpk',\n",
       " 'firesid',\n",
       " 'weatherworn',\n",
       " 'briger',\n",
       " 'sandpap',\n",
       " 'overcook',\n",
       " 'condi',\n",
       " 'moulag',\n",
       " 'vitaliy',\n",
       " 'sostegno',\n",
       " 'pompadour',\n",
       " 'wffpilel',\n",
       " 'hithem',\n",
       " 'abgeschliffen',\n",
       " 'chakra',\n",
       " 'kitchmamp',\n",
       " 'lpga',\n",
       " 'nappi',\n",
       " 'santorno',\n",
       " 'psyochot',\n",
       " 'herit',\n",
       " 'knuckl',\n",
       " 'bunt',\n",
       " 'bunzel',\n",
       " 'starz',\n",
       " 'almasdarnew',\n",
       " 'visum',\n",
       " 'juluka',\n",
       " 'niggardli',\n",
       " 'natesilv',\n",
       " 'ineluct',\n",
       " 'eagerli',\n",
       " 'hindu',\n",
       " 'pneumonia',\n",
       " 'alexpietrowski',\n",
       " 'kbed',\n",
       " 'investiss',\n",
       " 'attia',\n",
       " 'sunb',\n",
       " 'kyriaki',\n",
       " 'av',\n",
       " 'churchogogu',\n",
       " 'dusseldorf',\n",
       " 'ev',\n",
       " 'lactos',\n",
       " 'carretera',\n",
       " 'andreotta',\n",
       " 'pre',\n",
       " 'devletini',\n",
       " 'transmit',\n",
       " 'quietli',\n",
       " 'globalnej',\n",
       " 'icasualti',\n",
       " 'trumpworld',\n",
       " 'abad',\n",
       " 'bereich',\n",
       " 'whoo',\n",
       " ...}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = pd.read_csv('data/reg_data_token.csv')\n",
    "voc_list = training.text.tolist()\n",
    "voc_list_str = [str(x) for x in voc_list]\n",
    "# voc_list_len = [len(x) for x in voc_list_str]\n",
    "voc_string = ''.join(voc_list_str)\n",
    "voc_set = set(voc_string.split(' '))\n",
    "try:\n",
    "    voc_set.remove('')\n",
    "except:\n",
    "    pass\n",
    "print('   ',len(voc_set))\n",
    "voc_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33610"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1539.312791613646 2361.826171146817 2 62702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(65.0, 695.0, 2373.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(np.mean(voc_list_len) , np.std(voc_list_len) , np.min(voc_list_len) , np.max(voc_list_len) )\n",
    "# np.quantile(voc_list_len,0.25) , np.quantile(voc_list_len,0.5) , np.quantile(voc_list_len,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1539.312791613646, 33006)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.mean(voc_list_len) , len(voc_list_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer words to Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/33006 [02:39<162:33:07, 17.73s/it]"
     ]
    }
   ],
   "source": [
    "# voc_list = list(voc_set)\n",
    "# num_df = pd.DataFrame(data={'term':voc_list})\n",
    "# train_X = []\n",
    "# for news in tqdm(voc_list_str):\n",
    "#     trainX = []\n",
    "#     for term in news:\n",
    "#         try:\n",
    "#             idx = num_df[num_df['term'] == term].index[0]\n",
    "#         except:\n",
    "#             pass\n",
    "#         trainX.append(idx)\n",
    "#     train_X.append(np.array(trainX))\n",
    "# #     num_df\n",
    "# # num_df[num_df['term'] == 'ck'].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235.94686105325795 366.8819509536414 1 10268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10.0, 105.0, 361.0, 235.94686105325795)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(voc_list_str)\n",
    "tokens_enc = tokenizer.texts_to_sequences(voc_list_str)\n",
    "tokens_enc_len = [len(tokens) for tokens in tokens_enc]\n",
    "print(np.mean(tokens_enc_len) , np.std(tokens_enc_len) , np.min(tokens_enc_len) , np.max(tokens_enc_len) )\n",
    "np.quantile(tokens_enc_len,0.25) , np.quantile(tokens_enc_len,0.5) , np.quantile(tokens_enc_len,0.75) , np.mean(tokens_enc_len)\n",
    "# mean = 237\n",
    "# iqr = 885/887.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 100)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.clf()\n",
    "sns.distplot(sorted(tokens_enc_len),bins=3361,hist_kws={'cumulative':False},kde_kws={'cumulative':True})\n",
    "plt.xlim(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(obj=tokenizer,file=open('output/tokenizer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1.0\n",
      "    2.0\n",
      "    4.0\n",
      "    4.0\n",
      "    5.0\n",
      "    5.0\n",
      "    5.0\n",
      "    6.0\n",
      "    6.0\n",
      "    6.0\n",
      "    6.0\n",
      "    7.0\n",
      "    7.0\n",
      "    7.0\n",
      "    7.0\n",
      "    8.0\n",
      "    8.0\n",
      "    8.0\n",
      "    8.0\n",
      "    9.0\n",
      "    9.0\n",
      "    9.0\n",
      "    9.0\n",
      "    9.0\n",
      "    10.0\n",
      "    10.0\n",
      "    10.0\n",
      "    11.0\n",
      "    11.0\n",
      "    11.0\n",
      "    12.0\n",
      "    12.0\n",
      "    12.0\n",
      "    13.0\n",
      "    13.0\n",
      "    14.0\n",
      "    14.0\n",
      "    15.0\n",
      "    16.0\n",
      "    17.0\n",
      "    18.0\n",
      "    20.0\n",
      "    26.779999999998836\n",
      "    40.0\n",
      "    53.0\n",
      "    62.05000000000109\n",
      "    71.0\n",
      "    80.0\n",
      "    89.0\n",
      "    97.0\n",
      "    105.0\n",
      "    114.0\n",
      "    122.0\n",
      "    130.0\n",
      "    139.0\n",
      "    147.0\n",
      "    155.0\n",
      "    164.0\n",
      "    171.0\n",
      "    179.0\n",
      "    187.0\n",
      "    196.0\n",
      "    204.0\n",
      "    214.0\n",
      "    224.0\n",
      "    234.85000000000218\n",
      "    244.0\n",
      "    255.0\n",
      "    266.0\n",
      "    279.0\n",
      "    292.0\n",
      "    306.0\n",
      "    318.0\n",
      "    332.0\n",
      "    346.0\n",
      "    361.0\n",
      "    375.0\n",
      "    390.0\n",
      "    407.0\n",
      "    422.0\n",
      "    437.0\n",
      "    454.0\n",
      "    471.0\n",
      "    490.0\n",
      "    509.0\n",
      "    527.0\n",
      "    546.0\n",
      "    565.0\n",
      "    584.0\n",
      "    604.0\n",
      "    623.0\n",
      "    646.0\n",
      "    670.0\n",
      "    697.3700000000026\n",
      "    732.0\n",
      "    775.5500000000029\n",
      "    836.0\n",
      "    940.0\n",
      "    1106.8199999999997\n",
      "    1534.9099999999962\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print('   ',np.quantile(tokens_enc_len,i*0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(tokens_enc_len,0.9655)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33610, 33610)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY = training.label.tolist()\n",
    "len(trainY) , len(tokens_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49933, 49933)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 237\n",
    "train_X = []\n",
    "train_Y = []\n",
    "for tokens,label in zip(tokens_enc,trainY):\n",
    "    length = len(tokens)\n",
    "    iter_ = int(length / max_len)\n",
    "    if iter_ > 0:\n",
    "        temp = tokens\n",
    "        for it in range(iter_):\n",
    "#             bound = (it+1)*max_len\n",
    "            pre = temp[:max_len]\n",
    "            temp = temp[max_len:]\n",
    "#             print('   ',len(pre))\n",
    "            train_X.append(pre)\n",
    "            train_Y.append(label)\n",
    "            if (len(temp) <= max_len) and (len(temp)>=9):\n",
    "#                 print('   ',len(temp))\n",
    "                train_X.append(temp)\n",
    "                train_Y.append(label)\n",
    "    else:\n",
    "        if len(tokens) >= 9:\n",
    "            train_X.append(tokens)\n",
    "            train_Y.append(label)\n",
    "len(train_X) , len(train_Y)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_ = [len(x) for x in train_X]\n",
    "np.max(len_) , np.min(len_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49933, 237), (49933,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = pad_sequences(train_X, maxlen=max_len, padding= 'post' )\n",
    "train_Y = np.array(train_Y)\n",
    "train_X.shape , train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33610, 361), (33610,))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_X = pad_sequences(tokens_enc, maxlen=361, padding= 'post' )\n",
    "# train_Y = np.array(trainY)\n",
    "# train_X.shape , train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110894"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_valid_set(X_all, Y_all, percentage):\n",
    "    all_data_size = len(X_all)\n",
    "    valid_data_size = int(floor(all_data_size * percentage))\n",
    "\n",
    "    X_all, Y_all = _shuffle(X_all, Y_all)\n",
    "\n",
    "    X_train, Y_train = X_all[0:valid_data_size], Y_all[0:valid_data_size]\n",
    "    X_valid, Y_valid = X_all[valid_data_size:], Y_all[valid_data_size:]\n",
    "\n",
    "    return X_train, Y_train, X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, Y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "#     print(X.shape, Y.shape)\n",
    "    return (X[randomize], Y[randomize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_valid_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5888ce050459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_valid_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'split_valid_set' is not defined"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, valid_X, valid_Y = split_valid_set(train_X, train_Y, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(obj=(train_X,train_Y,valid_X,valid_Y),file=open('data/train_set.pkl','wb'))\n",
    "train_X, train_Y, valid_X, valid_Y = pickle.load(open('data/train_set.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 237)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 237, 128)          14194432  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 237, 64)           73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 237, 64)           256       \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 237, 128)          66048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 237, 128)          512       \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               74112     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 14,435,265\n",
      "Trainable params: 14,434,241\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_len = train_X.shape[1]\n",
    "\n",
    "inputs = Input(shape=(max_len,))\n",
    "emb_vec = Embedding(input_dim=np.max(train_X),output_dim=128,input_length=max_len)(inputs)\n",
    "conv = Conv1D(64,kernel_size=(9,), strides=1, padding='causal', data_format='channels_last')(emb_vec)\n",
    "bn = BatchNormalization()(conv)\n",
    "BiLSTM = Bidirectional(LSTM(64,return_sequences=True,dropout=0.1,recurrent_dropout=0.1,kernel_initializer='lecun_normal'))(bn)#64\n",
    "bn = BatchNormalization()(BiLSTM)\n",
    "rnn = Bidirectional(GRU(64,return_sequences=False,dropout=0.2,recurrent_dropout=0.2,kernel_initializer='lecun_normal'))(bn) #64\n",
    "bn1 = BatchNormalization()(rnn) # +FC*3? selu? 64? 3rd Dense shortcut from rnn/? kernel_initializer? dropout\n",
    "dense = Dense(128,activation='selu',kernel_initializer='lecun_normal')(bn1)\n",
    "bn2 = BatchNormalization()(dense)\n",
    "do = Dropout(0.3)(bn2)\n",
    "bn = Concatenate()([do,bn1])\n",
    "dense = Dense(64,activation='selu',kernel_initializer='lecun_normal')(bn)\n",
    "do = Dropout(0.4)(dense)\n",
    "bn = BatchNormalization()(do)\n",
    "bn = Concatenate()([bn,bn2])\n",
    "dense = Dense(64,activation='selu',kernel_initializer='lecun_normal')(bn)\n",
    "do = Dropout(0.5)(dense)\n",
    "bn = BatchNormalization()(do)\n",
    "output = Dense(1,activation='relu',kernel_initializer='lecun_normal')(bn)\n",
    "\n",
    "model = Model(inputs,output)\n",
    "# model = multi_gpu_model(model,gpus=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 237\n"
     ]
    }
   ],
   "source": [
    "opt=Adam() #Nadam()\n",
    "batchSize=2048 #32\n",
    "patien=30\n",
    "epoch=1000\n",
    "# hidden_dims=128\n",
    "# io_dim=128\n",
    "# input_lengths=train_X.shape[1] #profile_Q3\n",
    "# output_lengths= train_Y2.shape[1]#rep_max size\n",
    "# depths=1\n",
    "# dp = 0.01\n",
    "saveP = 'model/Reg_keras2.h5' #1: 00010: val_mean_absolute_error improved from 0.10455 to 0.09934, saving model to model/Reg_keras.h5\n",
    "logD = './model/logs/'\n",
    "history = History()\n",
    "print(\"input:\",train_X.shape[1])#,'output_length:',train_Y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44939 samples, validate on 4994 samples\n",
      "Epoch 1/1000\n",
      "44939/44939 [==============================] - 72s 2ms/step - loss: 0.0121 - mean_absolute_error: 0.0705 - val_loss: 0.0868 - val_mean_absolute_error: 0.1548\n",
      "\n",
      "Epoch 00001: val_mean_absolute_error improved from inf to 0.15485, saving model to model/Reg_keras2.h5\n",
      "Epoch 2/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0128 - mean_absolute_error: 0.0705 - val_loss: 0.0845 - val_mean_absolute_error: 0.1556\n",
      "\n",
      "Epoch 00002: val_mean_absolute_error did not improve from 0.15485\n",
      "Epoch 3/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0106 - mean_absolute_error: 0.0664 - val_loss: 0.0872 - val_mean_absolute_error: 0.1533\n",
      "\n",
      "Epoch 00003: val_mean_absolute_error improved from 0.15485 to 0.15325, saving model to model/Reg_keras2.h5\n",
      "Epoch 4/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0088 - mean_absolute_error: 0.0625 - val_loss: 0.0831 - val_mean_absolute_error: 0.1394\n",
      "\n",
      "Epoch 00004: val_mean_absolute_error improved from 0.15325 to 0.13942, saving model to model/Reg_keras2.h5\n",
      "Epoch 5/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0079 - mean_absolute_error: 0.0599 - val_loss: 0.0777 - val_mean_absolute_error: 0.1246\n",
      "\n",
      "Epoch 00005: val_mean_absolute_error improved from 0.13942 to 0.12463, saving model to model/Reg_keras2.h5\n",
      "Epoch 6/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0076 - mean_absolute_error: 0.0599 - val_loss: 0.0775 - val_mean_absolute_error: 0.1133\n",
      "\n",
      "Epoch 00006: val_mean_absolute_error improved from 0.12463 to 0.11330, saving model to model/Reg_keras2.h5\n",
      "Epoch 7/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0073 - mean_absolute_error: 0.0585 - val_loss: 0.0783 - val_mean_absolute_error: 0.1120\n",
      "\n",
      "Epoch 00007: val_mean_absolute_error improved from 0.11330 to 0.11198, saving model to model/Reg_keras2.h5\n",
      "Epoch 8/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0066 - mean_absolute_error: 0.0568 - val_loss: 0.0773 - val_mean_absolute_error: 0.1077\n",
      "\n",
      "Epoch 00008: val_mean_absolute_error improved from 0.11198 to 0.10771, saving model to model/Reg_keras2.h5\n",
      "Epoch 9/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0062 - mean_absolute_error: 0.0563 - val_loss: 0.0767 - val_mean_absolute_error: 0.1110\n",
      "\n",
      "Epoch 00009: val_mean_absolute_error did not improve from 0.10771\n",
      "Epoch 10/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0061 - mean_absolute_error: 0.0551 - val_loss: 0.0758 - val_mean_absolute_error: 0.1122\n",
      "\n",
      "Epoch 00010: val_mean_absolute_error did not improve from 0.10771\n",
      "Epoch 11/1000\n",
      "44939/44939 [==============================] - 68s 2ms/step - loss: 0.0060 - mean_absolute_error: 0.0555 - val_loss: 0.0784 - val_mean_absolute_error: 0.1178\n",
      "\n",
      "Epoch 00011: val_mean_absolute_error did not improve from 0.10771\n",
      "Epoch 12/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0062 - mean_absolute_error: 0.0555 - val_loss: 0.0747 - val_mean_absolute_error: 0.1124\n",
      "\n",
      "Epoch 00012: val_mean_absolute_error did not improve from 0.10771\n",
      "Epoch 13/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0060 - mean_absolute_error: 0.0548 - val_loss: 0.0743 - val_mean_absolute_error: 0.1177\n",
      "\n",
      "Epoch 00013: val_mean_absolute_error did not improve from 0.10771\n",
      "Epoch 14/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0055 - mean_absolute_error: 0.0535 - val_loss: 0.0785 - val_mean_absolute_error: 0.1088\n",
      "\n",
      "Epoch 00014: val_mean_absolute_error did not improve from 0.10771\n",
      "Epoch 15/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0055 - mean_absolute_error: 0.0535 - val_loss: 0.0743 - val_mean_absolute_error: 0.1052\n",
      "\n",
      "Epoch 00015: val_mean_absolute_error improved from 0.10771 to 0.10520, saving model to model/Reg_keras2.h5\n",
      "Epoch 16/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0054 - mean_absolute_error: 0.0530 - val_loss: 0.0728 - val_mean_absolute_error: 0.1143\n",
      "\n",
      "Epoch 00016: val_mean_absolute_error did not improve from 0.10520\n",
      "Epoch 17/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0055 - mean_absolute_error: 0.0535 - val_loss: 0.0730 - val_mean_absolute_error: 0.1138\n",
      "\n",
      "Epoch 00017: val_mean_absolute_error did not improve from 0.10520\n",
      "Epoch 18/1000\n",
      "44939/44939 [==============================] - 68s 2ms/step - loss: 0.0053 - mean_absolute_error: 0.0529 - val_loss: 0.0771 - val_mean_absolute_error: 0.1096\n",
      "\n",
      "Epoch 00018: val_mean_absolute_error did not improve from 0.10520\n",
      "Epoch 19/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0052 - mean_absolute_error: 0.0521 - val_loss: 0.0759 - val_mean_absolute_error: 0.1076\n",
      "\n",
      "Epoch 00019: val_mean_absolute_error did not improve from 0.10520\n",
      "Epoch 20/1000\n",
      "44939/44939 [==============================] - 68s 2ms/step - loss: 0.0054 - mean_absolute_error: 0.0532 - val_loss: 0.0765 - val_mean_absolute_error: 0.1103\n",
      "\n",
      "Epoch 00020: val_mean_absolute_error did not improve from 0.10520\n",
      "Epoch 21/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0050 - mean_absolute_error: 0.0520 - val_loss: 0.0776 - val_mean_absolute_error: 0.1162\n",
      "\n",
      "Epoch 00021: val_mean_absolute_error did not improve from 0.10520\n",
      "Epoch 22/1000\n",
      "44939/44939 [==============================] - 67s 2ms/step - loss: 0.0051 - mean_absolute_error: 0.0522 - val_loss: 0.0784 - val_mean_absolute_error: 0.1124\n",
      "\n",
      "Epoch 00022: val_mean_absolute_error did not improve from 0.10520\n",
      "Epoch 23/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0048 - mean_absolute_error: 0.0510 - val_loss: 0.0779 - val_mean_absolute_error: 0.1090\n",
      "\n",
      "Epoch 00023: val_mean_absolute_error did not improve from 0.10520\n",
      "Epoch 24/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0051 - mean_absolute_error: 0.0514 - val_loss: 0.0742 - val_mean_absolute_error: 0.1080\n",
      "\n",
      "Epoch 00024: val_mean_absolute_error did not improve from 0.10520\n",
      "Epoch 25/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0048 - mean_absolute_error: 0.0507 - val_loss: 0.0749 - val_mean_absolute_error: 0.1152\n",
      "\n",
      "Epoch 00025: val_mean_absolute_error did not improve from 0.10520\n",
      "Epoch 26/1000\n",
      "44939/44939 [==============================] - 67s 2ms/step - loss: 0.0058 - mean_absolute_error: 0.0531 - val_loss: 0.0754 - val_mean_absolute_error: 0.1141\n",
      "\n",
      "Epoch 00026: val_mean_absolute_error did not improve from 0.10520\n",
      "Epoch 27/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0051 - mean_absolute_error: 0.0518 - val_loss: 0.0773 - val_mean_absolute_error: 0.1174\n",
      "\n",
      "Epoch 00027: val_mean_absolute_error did not improve from 0.10520\n",
      "Epoch 28/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0049 - mean_absolute_error: 0.0509 - val_loss: 0.0759 - val_mean_absolute_error: 0.1099\n",
      "\n",
      "Epoch 00028: val_mean_absolute_error did not improve from 0.10520\n",
      "Epoch 29/1000\n",
      "44939/44939 [==============================] - 68s 2ms/step - loss: 0.0044 - mean_absolute_error: 0.0498 - val_loss: 0.0754 - val_mean_absolute_error: 0.1107\n",
      "\n",
      "Epoch 00029: val_mean_absolute_error did not improve from 0.10520\n",
      "Epoch 30/1000\n",
      "44939/44939 [==============================] - 68s 2ms/step - loss: 0.0041 - mean_absolute_error: 0.0487 - val_loss: 0.0734 - val_mean_absolute_error: 0.1127\n",
      "\n",
      "Epoch 00030: val_mean_absolute_error did not improve from 0.10520\n",
      "Epoch 31/1000\n",
      "44939/44939 [==============================] - 68s 2ms/step - loss: 0.0043 - mean_absolute_error: 0.0495 - val_loss: 0.0762 - val_mean_absolute_error: 0.1051\n",
      "\n",
      "Epoch 00031: val_mean_absolute_error improved from 0.10520 to 0.10512, saving model to model/Reg_keras2.h5\n",
      "Epoch 32/1000\n",
      "44939/44939 [==============================] - 68s 2ms/step - loss: 0.0041 - mean_absolute_error: 0.0487 - val_loss: 0.0740 - val_mean_absolute_error: 0.1104\n",
      "\n",
      "Epoch 00032: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 33/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0041 - mean_absolute_error: 0.0486 - val_loss: 0.0749 - val_mean_absolute_error: 0.1129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 34/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0041 - mean_absolute_error: 0.0486 - val_loss: 0.0751 - val_mean_absolute_error: 0.1132\n",
      "\n",
      "Epoch 00034: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 35/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0040 - mean_absolute_error: 0.0487 - val_loss: 0.0751 - val_mean_absolute_error: 0.1144\n",
      "\n",
      "Epoch 00035: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 36/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0041 - mean_absolute_error: 0.0482 - val_loss: 0.0746 - val_mean_absolute_error: 0.1115\n",
      "\n",
      "Epoch 00036: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 37/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0042 - mean_absolute_error: 0.0487 - val_loss: 0.0760 - val_mean_absolute_error: 0.1122\n",
      "\n",
      "Epoch 00037: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 38/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0042 - mean_absolute_error: 0.0480 - val_loss: 0.0757 - val_mean_absolute_error: 0.1062\n",
      "\n",
      "Epoch 00038: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 39/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0049 - mean_absolute_error: 0.0500 - val_loss: 0.0773 - val_mean_absolute_error: 0.1119\n",
      "\n",
      "Epoch 00039: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 40/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0044 - mean_absolute_error: 0.0490 - val_loss: 0.0763 - val_mean_absolute_error: 0.1105\n",
      "\n",
      "Epoch 00040: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 41/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0047 - mean_absolute_error: 0.0498 - val_loss: 0.0751 - val_mean_absolute_error: 0.1090\n",
      "\n",
      "Epoch 00041: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 42/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0043 - mean_absolute_error: 0.0488 - val_loss: 0.0777 - val_mean_absolute_error: 0.1105\n",
      "\n",
      "Epoch 00042: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 43/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0039 - mean_absolute_error: 0.0477 - val_loss: 0.0760 - val_mean_absolute_error: 0.1087\n",
      "\n",
      "Epoch 00043: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 44/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0038 - mean_absolute_error: 0.0473 - val_loss: 0.0767 - val_mean_absolute_error: 0.1101\n",
      "\n",
      "Epoch 00044: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 45/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0041 - mean_absolute_error: 0.0486 - val_loss: 0.0733 - val_mean_absolute_error: 0.1200\n",
      "\n",
      "Epoch 00045: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 46/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0041 - mean_absolute_error: 0.0486 - val_loss: 0.0726 - val_mean_absolute_error: 0.1077\n",
      "\n",
      "Epoch 00046: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 47/1000\n",
      "44939/44939 [==============================] - 67s 2ms/step - loss: 0.0038 - mean_absolute_error: 0.0472 - val_loss: 0.0732 - val_mean_absolute_error: 0.1094\n",
      "\n",
      "Epoch 00047: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 48/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0038 - mean_absolute_error: 0.0470 - val_loss: 0.0719 - val_mean_absolute_error: 0.1093\n",
      "\n",
      "Epoch 00048: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 49/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0038 - mean_absolute_error: 0.0473 - val_loss: 0.0731 - val_mean_absolute_error: 0.1075\n",
      "\n",
      "Epoch 00049: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 50/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0036 - mean_absolute_error: 0.0464 - val_loss: 0.0732 - val_mean_absolute_error: 0.1057\n",
      "\n",
      "Epoch 00050: val_mean_absolute_error did not improve from 0.10512\n",
      "Epoch 51/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0036 - mean_absolute_error: 0.0464 - val_loss: 0.0733 - val_mean_absolute_error: 0.1051\n",
      "\n",
      "Epoch 00051: val_mean_absolute_error improved from 0.10512 to 0.10507, saving model to model/Reg_keras2.h5\n",
      "Epoch 52/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0036 - mean_absolute_error: 0.0458 - val_loss: 0.0747 - val_mean_absolute_error: 0.1086\n",
      "\n",
      "Epoch 00052: val_mean_absolute_error did not improve from 0.10507\n",
      "Epoch 53/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0047 - mean_absolute_error: 0.0484 - val_loss: 0.0742 - val_mean_absolute_error: 0.1051\n",
      "\n",
      "Epoch 00053: val_mean_absolute_error did not improve from 0.10507\n",
      "Epoch 54/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0051 - mean_absolute_error: 0.0494 - val_loss: 0.0742 - val_mean_absolute_error: 0.1075\n",
      "\n",
      "Epoch 00054: val_mean_absolute_error did not improve from 0.10507\n",
      "Epoch 55/1000\n",
      "44939/44939 [==============================] - 68s 2ms/step - loss: 0.0046 - mean_absolute_error: 0.0482 - val_loss: 0.0766 - val_mean_absolute_error: 0.1059\n",
      "\n",
      "Epoch 00055: val_mean_absolute_error did not improve from 0.10507\n",
      "Epoch 56/1000\n",
      "44939/44939 [==============================] - 67s 2ms/step - loss: 0.0044 - mean_absolute_error: 0.0483 - val_loss: 0.0752 - val_mean_absolute_error: 0.1045\n",
      "\n",
      "Epoch 00056: val_mean_absolute_error improved from 0.10507 to 0.10455, saving model to model/Reg_keras2.h5\n",
      "Epoch 57/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0041 - mean_absolute_error: 0.0477 - val_loss: 0.0748 - val_mean_absolute_error: 0.1065\n",
      "\n",
      "Epoch 00057: val_mean_absolute_error did not improve from 0.10455\n",
      "Epoch 58/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0042 - mean_absolute_error: 0.0480 - val_loss: 0.0762 - val_mean_absolute_error: 0.1091\n",
      "\n",
      "Epoch 00058: val_mean_absolute_error did not improve from 0.10455\n",
      "Epoch 59/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0041 - mean_absolute_error: 0.0475 - val_loss: 0.0716 - val_mean_absolute_error: 0.1044\n",
      "\n",
      "Epoch 00059: val_mean_absolute_error improved from 0.10455 to 0.10441, saving model to model/Reg_keras2.h5\n",
      "Epoch 60/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0040 - mean_absolute_error: 0.0469 - val_loss: 0.0730 - val_mean_absolute_error: 0.1009\n",
      "\n",
      "Epoch 00060: val_mean_absolute_error improved from 0.10441 to 0.10091, saving model to model/Reg_keras2.h5\n",
      "Epoch 61/1000\n",
      "44939/44939 [==============================] - 68s 2ms/step - loss: 0.0037 - mean_absolute_error: 0.0462 - val_loss: 0.0751 - val_mean_absolute_error: 0.1048\n",
      "\n",
      "Epoch 00061: val_mean_absolute_error did not improve from 0.10091\n",
      "Epoch 62/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0035 - mean_absolute_error: 0.0458 - val_loss: 0.0710 - val_mean_absolute_error: 0.1049\n",
      "\n",
      "Epoch 00062: val_mean_absolute_error did not improve from 0.10091\n",
      "Epoch 63/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0034 - mean_absolute_error: 0.0451 - val_loss: 0.0728 - val_mean_absolute_error: 0.1045\n",
      "\n",
      "Epoch 00063: val_mean_absolute_error did not improve from 0.10091\n",
      "Epoch 64/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0034 - mean_absolute_error: 0.0451 - val_loss: 0.0726 - val_mean_absolute_error: 0.0999\n",
      "\n",
      "Epoch 00064: val_mean_absolute_error improved from 0.10091 to 0.09991, saving model to model/Reg_keras2.h5\n",
      "Epoch 65/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0034 - mean_absolute_error: 0.0453 - val_loss: 0.0723 - val_mean_absolute_error: 0.1040\n",
      "\n",
      "Epoch 00065: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 66/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0033 - mean_absolute_error: 0.0446 - val_loss: 0.0723 - val_mean_absolute_error: 0.1022\n",
      "\n",
      "Epoch 00066: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 67/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0034 - mean_absolute_error: 0.0452 - val_loss: 0.0730 - val_mean_absolute_error: 0.1002\n",
      "\n",
      "Epoch 00067: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 68/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0033 - mean_absolute_error: 0.0447 - val_loss: 0.0727 - val_mean_absolute_error: 0.1040\n",
      "\n",
      "Epoch 00068: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 69/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0032 - mean_absolute_error: 0.0439 - val_loss: 0.0729 - val_mean_absolute_error: 0.1033\n",
      "\n",
      "Epoch 00069: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 70/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0032 - mean_absolute_error: 0.0443 - val_loss: 0.0733 - val_mean_absolute_error: 0.1031\n",
      "\n",
      "Epoch 00070: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 71/1000\n",
      "44939/44939 [==============================] - 68s 2ms/step - loss: 0.0032 - mean_absolute_error: 0.0441 - val_loss: 0.0730 - val_mean_absolute_error: 0.1027\n",
      "\n",
      "Epoch 00071: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 72/1000\n",
      "44939/44939 [==============================] - 68s 2ms/step - loss: 0.0032 - mean_absolute_error: 0.0440 - val_loss: 0.0730 - val_mean_absolute_error: 0.1049\n",
      "\n",
      "Epoch 00072: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 73/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0031 - mean_absolute_error: 0.0436 - val_loss: 0.0734 - val_mean_absolute_error: 0.1053\n",
      "\n",
      "Epoch 00073: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 74/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0031 - mean_absolute_error: 0.0439 - val_loss: 0.0737 - val_mean_absolute_error: 0.1079\n",
      "\n",
      "Epoch 00074: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 75/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0031 - mean_absolute_error: 0.0436 - val_loss: 0.0718 - val_mean_absolute_error: 0.1052\n",
      "\n",
      "Epoch 00075: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 76/1000\n",
      "44939/44939 [==============================] - 67s 2ms/step - loss: 0.0031 - mean_absolute_error: 0.0437 - val_loss: 0.0726 - val_mean_absolute_error: 0.1023\n",
      "\n",
      "Epoch 00076: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 77/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0032 - mean_absolute_error: 0.0443 - val_loss: 0.0726 - val_mean_absolute_error: 0.1040\n",
      "\n",
      "Epoch 00077: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 78/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0032 - mean_absolute_error: 0.0440 - val_loss: 0.0729 - val_mean_absolute_error: 0.1044\n",
      "\n",
      "Epoch 00078: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 79/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0031 - mean_absolute_error: 0.0435 - val_loss: 0.0734 - val_mean_absolute_error: 0.1031\n",
      "\n",
      "Epoch 00079: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 80/1000\n",
      "44939/44939 [==============================] - 68s 2ms/step - loss: 0.0031 - mean_absolute_error: 0.0435 - val_loss: 0.0731 - val_mean_absolute_error: 0.1042\n",
      "\n",
      "Epoch 00080: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 81/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0031 - mean_absolute_error: 0.0435 - val_loss: 0.0728 - val_mean_absolute_error: 0.1050\n",
      "\n",
      "Epoch 00081: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 82/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0031 - mean_absolute_error: 0.0434 - val_loss: 0.0731 - val_mean_absolute_error: 0.1059\n",
      "\n",
      "Epoch 00082: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 83/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0031 - mean_absolute_error: 0.0431 - val_loss: 0.0732 - val_mean_absolute_error: 0.1067\n",
      "\n",
      "Epoch 00083: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 84/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0032 - mean_absolute_error: 0.0441 - val_loss: 0.0741 - val_mean_absolute_error: 0.1047\n",
      "\n",
      "Epoch 00084: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 85/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0030 - mean_absolute_error: 0.0428 - val_loss: 0.0743 - val_mean_absolute_error: 0.1050\n",
      "\n",
      "Epoch 00085: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 86/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0031 - mean_absolute_error: 0.0433 - val_loss: 0.0746 - val_mean_absolute_error: 0.1046\n",
      "\n",
      "Epoch 00086: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 87/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0031 - mean_absolute_error: 0.0432 - val_loss: 0.0746 - val_mean_absolute_error: 0.1043\n",
      "\n",
      "Epoch 00087: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 88/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0030 - mean_absolute_error: 0.0429 - val_loss: 0.0742 - val_mean_absolute_error: 0.1067\n",
      "\n",
      "Epoch 00088: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 89/1000\n",
      "44939/44939 [==============================] - 66s 1ms/step - loss: 0.0031 - mean_absolute_error: 0.0433 - val_loss: 0.0748 - val_mean_absolute_error: 0.1048\n",
      "\n",
      "Epoch 00089: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 90/1000\n",
      "44939/44939 [==============================] - 67s 1ms/step - loss: 0.0031 - mean_absolute_error: 0.0435 - val_loss: 0.0745 - val_mean_absolute_error: 0.1054\n",
      "\n",
      "Epoch 00090: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 91/1000\n",
      "44939/44939 [==============================] - 68s 2ms/step - loss: 0.0031 - mean_absolute_error: 0.0434 - val_loss: 0.0748 - val_mean_absolute_error: 0.1048\n",
      "\n",
      "Epoch 00091: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 92/1000\n",
      "44939/44939 [==============================] - 68s 2ms/step - loss: 0.0030 - mean_absolute_error: 0.0427 - val_loss: 0.0747 - val_mean_absolute_error: 0.1044\n",
      "\n",
      "Epoch 00092: val_mean_absolute_error did not improve from 0.09991\n",
      "Epoch 00092: early stopping\n"
     ]
    }
   ],
   "source": [
    "# model = load_model(saveP+\"_all.h5\")\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "callback=[\n",
    "    ReduceLROnPlateau(monitor='loss', factor=0.5, patience=int(patien/3),min_lr=1e-6,mode='min' ),\n",
    "    EarlyStopping(patience=patien,monitor='val_loss',verbose=1),\n",
    "    ModelCheckpoint(saveP,monitor='val_mean_absolute_error',verbose=1,save_best_only=True, save_weights_only=True),\n",
    "    TensorBoard(log_dir=logD), \n",
    "    history,\n",
    "]\n",
    "model.fit(train_X, train_Y,\n",
    "                epochs=epoch,\n",
    "                batch_size=batchSize,\n",
    "                shuffle=True,\n",
    "                validation_data=(valid_X, valid_Y),\n",
    "                callbacks=callback, \n",
    "#                 class_weight='auto'\n",
    "                )\n",
    "model.save(saveP+\"_all.h5\") #184sec/0.04/0.19\n",
    "# 127s 3ms/step - loss: 0.0072 - mean_absolute_error: 0.0380 - val_loss: 0.0920 - val_mean_absolute_error: 0.1414"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd time\n",
    "* fix back lyers\n",
    "* lower lr\n",
    "* lower patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 237\n"
     ]
    }
   ],
   "source": [
    "opt=Nadam(lr=0.0009) \n",
    "batchSize=512\n",
    "patien=15\n",
    "epoch=100\n",
    "# hidden_dims=128\n",
    "# io_dim=128\n",
    "# input_lengths=train_X.shape[1] #profile_Q3\n",
    "# output_lengths= train_Y2.shape[1]#rep_max size\n",
    "# depths=1\n",
    "# dp = 0.01\n",
    "saveP = 'model/Reg_keras2.h5' #2: val_mean_absolute_error did not improve from 0.10362\n",
    "logD = './model/logs/'\n",
    "history = History()\n",
    "print(\"input:\",train_X.shape[1])#,'output_length:',train_Y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44939 samples, validate on 4994 samples\n",
      "Epoch 1/100\n",
      "44939/44939 [==============================] - 316s 7ms/step - loss: 0.0065 - mean_absolute_error: 0.0558 - val_loss: 0.0741 - val_mean_absolute_error: 0.1188\n",
      "\n",
      "Epoch 00001: val_mean_absolute_error improved from inf to 0.11881, saving model to model/Reg_keras2.h5\n",
      "Epoch 2/100\n",
      "44939/44939 [==============================] - 305s 7ms/step - loss: 0.0074 - mean_absolute_error: 0.0575 - val_loss: 0.0706 - val_mean_absolute_error: 0.1224\n",
      "\n",
      "Epoch 00002: val_mean_absolute_error did not improve from 0.11881\n",
      "Epoch 3/100\n",
      "44939/44939 [==============================] - 305s 7ms/step - loss: 0.0074 - mean_absolute_error: 0.0569 - val_loss: 0.0754 - val_mean_absolute_error: 0.1106\n",
      "\n",
      "Epoch 00003: val_mean_absolute_error improved from 0.11881 to 0.11057, saving model to model/Reg_keras2.h5\n",
      "Epoch 4/100\n",
      "44939/44939 [==============================] - 306s 7ms/step - loss: 0.0067 - mean_absolute_error: 0.0558 - val_loss: 0.0714 - val_mean_absolute_error: 0.1056\n",
      "\n",
      "Epoch 00004: val_mean_absolute_error improved from 0.11057 to 0.10557, saving model to model/Reg_keras2.h5\n",
      "Epoch 5/100\n",
      "44939/44939 [==============================] - 303s 7ms/step - loss: 0.0062 - mean_absolute_error: 0.0555 - val_loss: 0.0693 - val_mean_absolute_error: 0.1191\n",
      "\n",
      "Epoch 00005: val_mean_absolute_error did not improve from 0.10557\n",
      "Epoch 6/100\n",
      "44939/44939 [==============================] - 306s 7ms/step - loss: 0.0059 - mean_absolute_error: 0.0546 - val_loss: 0.0739 - val_mean_absolute_error: 0.1082\n",
      "\n",
      "Epoch 00006: val_mean_absolute_error did not improve from 0.10557\n",
      "Epoch 7/100\n",
      "44939/44939 [==============================] - 307s 7ms/step - loss: 0.0055 - mean_absolute_error: 0.0536 - val_loss: 0.0689 - val_mean_absolute_error: 0.1116\n",
      "\n",
      "Epoch 00007: val_mean_absolute_error did not improve from 0.10557\n",
      "Epoch 8/100\n",
      "44939/44939 [==============================] - 306s 7ms/step - loss: 0.0052 - mean_absolute_error: 0.0527 - val_loss: 0.0762 - val_mean_absolute_error: 0.1132\n",
      "\n",
      "Epoch 00008: val_mean_absolute_error did not improve from 0.10557\n",
      "Epoch 9/100\n",
      "44939/44939 [==============================] - 307s 7ms/step - loss: 0.0054 - mean_absolute_error: 0.0541 - val_loss: 0.0730 - val_mean_absolute_error: 0.1052\n",
      "\n",
      "Epoch 00009: val_mean_absolute_error improved from 0.10557 to 0.10519, saving model to model/Reg_keras2.h5\n",
      "Epoch 10/100\n",
      "44939/44939 [==============================] - 307s 7ms/step - loss: 0.0053 - mean_absolute_error: 0.0529 - val_loss: 0.0743 - val_mean_absolute_error: 0.1123\n",
      "\n",
      "Epoch 00010: val_mean_absolute_error did not improve from 0.10519\n",
      "Epoch 11/100\n",
      "44939/44939 [==============================] - 305s 7ms/step - loss: 0.0054 - mean_absolute_error: 0.0531 - val_loss: 0.0739 - val_mean_absolute_error: 0.1101\n",
      "\n",
      "Epoch 00011: val_mean_absolute_error did not improve from 0.10519\n",
      "Epoch 12/100\n",
      "44939/44939 [==============================] - 307s 7ms/step - loss: 0.0051 - mean_absolute_error: 0.0521 - val_loss: 0.0696 - val_mean_absolute_error: 0.1173\n",
      "\n",
      "Epoch 00012: val_mean_absolute_error did not improve from 0.10519\n",
      "Epoch 13/100\n",
      "44939/44939 [==============================] - 306s 7ms/step - loss: 0.0053 - mean_absolute_error: 0.0535 - val_loss: 0.0728 - val_mean_absolute_error: 0.1178\n",
      "\n",
      "Epoch 00013: val_mean_absolute_error did not improve from 0.10519\n",
      "Epoch 14/100\n",
      "44939/44939 [==============================] - 306s 7ms/step - loss: 0.0054 - mean_absolute_error: 0.0535 - val_loss: 0.0712 - val_mean_absolute_error: 0.1064\n",
      "\n",
      "Epoch 00014: val_mean_absolute_error did not improve from 0.10519\n",
      "Epoch 15/100\n",
      "44939/44939 [==============================] - 308s 7ms/step - loss: 0.0051 - mean_absolute_error: 0.0526 - val_loss: 0.0717 - val_mean_absolute_error: 0.1121\n",
      "\n",
      "Epoch 00015: val_mean_absolute_error did not improve from 0.10519\n",
      "Epoch 16/100\n",
      "44939/44939 [==============================] - 306s 7ms/step - loss: 0.0056 - mean_absolute_error: 0.0536 - val_loss: 0.0722 - val_mean_absolute_error: 0.1121\n",
      "\n",
      "Epoch 00016: val_mean_absolute_error did not improve from 0.10519\n",
      "Epoch 17/100\n",
      "44939/44939 [==============================] - 307s 7ms/step - loss: 0.0058 - mean_absolute_error: 0.0533 - val_loss: 0.0711 - val_mean_absolute_error: 0.1098\n",
      "\n",
      "Epoch 00017: val_mean_absolute_error did not improve from 0.10519\n",
      "Epoch 18/100\n",
      "44939/44939 [==============================] - 306s 7ms/step - loss: 0.0048 - mean_absolute_error: 0.0511 - val_loss: 0.0696 - val_mean_absolute_error: 0.1127\n",
      "\n",
      "Epoch 00018: val_mean_absolute_error did not improve from 0.10519\n",
      "Epoch 19/100\n",
      "44939/44939 [==============================] - 307s 7ms/step - loss: 0.0045 - mean_absolute_error: 0.0513 - val_loss: 0.0696 - val_mean_absolute_error: 0.1084\n",
      "\n",
      "Epoch 00019: val_mean_absolute_error did not improve from 0.10519\n",
      "Epoch 20/100\n",
      "44939/44939 [==============================] - 305s 7ms/step - loss: 0.0044 - mean_absolute_error: 0.0514 - val_loss: 0.0715 - val_mean_absolute_error: 0.1002\n",
      "\n",
      "Epoch 00020: val_mean_absolute_error improved from 0.10519 to 0.10024, saving model to model/Reg_keras2.h5\n",
      "Epoch 21/100\n",
      "44939/44939 [==============================] - 304s 7ms/step - loss: 0.0039 - mean_absolute_error: 0.0487 - val_loss: 0.0703 - val_mean_absolute_error: 0.1056\n",
      "\n",
      "Epoch 00021: val_mean_absolute_error did not improve from 0.10024\n",
      "Epoch 22/100\n",
      "44939/44939 [==============================] - 305s 7ms/step - loss: 0.0041 - mean_absolute_error: 0.0497 - val_loss: 0.0705 - val_mean_absolute_error: 0.1036\n",
      "\n",
      "Epoch 00022: val_mean_absolute_error did not improve from 0.10024\n",
      "Epoch 00022: early stopping\n"
     ]
    }
   ],
   "source": [
    "# model = load_model(saveP+\"_all.h5\")\n",
    "model.load_weights(saveP)\n",
    "model = multi_gpu_model(model,gpus=2)\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "\n",
    "callback=[\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=int(patien/1.5),min_lr=1e-6,mode='min' ),\n",
    "    EarlyStopping(patience=patien,monitor='val_loss',verbose=1),\n",
    "    ModelCheckpoint(saveP,monitor='val_mean_absolute_error',verbose=1,save_best_only=True, save_weights_only=True),\n",
    "    TensorBoard(log_dir=logD), \n",
    "    history,\n",
    "]\n",
    "model.fit(train_X, train_Y,\n",
    "                epochs=epoch,\n",
    "                batch_size=batchSize,\n",
    "                shuffle=True,\n",
    "                validation_data=(valid_X, valid_Y),\n",
    "                callbacks=callback, \n",
    "                class_weight='auto'\n",
    "                )\n",
    "model.save(saveP+\"_all.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4994/4994 [==============================] - 210s 42ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06722560542694177, 0.10202433797875833]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = load_model('model/Reg_keras2.h5_all.h5')\n",
    "model2.load_weights('model/Reg_keras2.h5')\n",
    "model2.evaluate(valid_X,valid_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "original:\n",
    "* 0 => fake\n",
    "* 1 => true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5200, 5200)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = pickle.load(open('output/tokenizer.pkl','rb'))\n",
    "testing = pd.read_csv('result/testing_kaggle.csv')\n",
    "test_list = testing['text'].astype(str).tolist()\n",
    "test_id = testing.id.astype(int).tolist()\n",
    "test_data = tokenizer.texts_to_sequences(test_list)\n",
    "len(test_data) , len(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10856, 10856)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 237\n",
    "test_X = []\n",
    "test_ID = []\n",
    "for tokens,id_ in zip(test_data,test_id):\n",
    "    length = len(tokens)\n",
    "    iter_ = int(length / max_len)\n",
    "    if iter_ > 0:\n",
    "        temp = tokens\n",
    "        for it in range(iter_):\n",
    "#             bound = (it+1)*max_len\n",
    "            pre = temp[:max_len]\n",
    "            temp = temp[max_len:]\n",
    "#             print('   ',len(pre))\n",
    "            test_X.append(pre)\n",
    "            test_ID.append(id_)\n",
    "            if (len(temp) <= max_len) and (len(temp)>=9):\n",
    "#                 print('   ',len(temp))\n",
    "                test_X.append(temp)\n",
    "                test_ID.append(id_)\n",
    "    else:\n",
    "#         if len(tokens) >= 9:\n",
    "        test_X.append(tokens)\n",
    "        test_ID.append(id_)\n",
    "len(test_X) , len(test_ID) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10856, 237), (10856,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = pad_sequences(test_X, maxlen=max_len, padding= 'post' )\n",
    "test_ID = np.array(test_ID)\n",
    "test_X.shape , test_ID.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5200"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(test_ID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model/Reg_keras.h5_all.h5')\n",
    "model.load_weights('model/Reg_keras.h5')\n",
    "ans1 = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9977354 ],\n",
       "       [0.9962249 ],\n",
       "       [0.99586725],\n",
       "       ...,\n",
       "       [0.99617416],\n",
       "       [0.99723536],\n",
       "       [0.9980172 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10856, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = load_model('model/Reg_keras2.h5_all.h5')\n",
    "model2.load_weights('model/Reg_keras2.h5')\n",
    "ans2 = model2.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10856, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine\n",
    "* 1 => fake\n",
    "    * <=0.5 -> 1\n",
    "* 0 => true\n",
    "    * \\>0.5 -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>0.999392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20800</td>\n",
       "      <td>0.998562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20800</td>\n",
       "      <td>0.998482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20801</td>\n",
       "      <td>0.004707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20802</td>\n",
       "      <td>0.005377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20802</td>\n",
       "      <td>0.003506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20803</td>\n",
       "      <td>0.999858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20803</td>\n",
       "      <td>0.997517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20804</td>\n",
       "      <td>0.003488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20805</td>\n",
       "      <td>0.005277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20806</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20807</td>\n",
       "      <td>0.003950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20807</td>\n",
       "      <td>0.449685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20808</td>\n",
       "      <td>0.001446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20808</td>\n",
       "      <td>0.008863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20809</td>\n",
       "      <td>0.581511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20810</td>\n",
       "      <td>0.004766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20810</td>\n",
       "      <td>0.004062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20810</td>\n",
       "      <td>0.004550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20810</td>\n",
       "      <td>0.006808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20810</td>\n",
       "      <td>0.004325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20810</td>\n",
       "      <td>0.003358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20810</td>\n",
       "      <td>0.007157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20811</td>\n",
       "      <td>0.004958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20812</td>\n",
       "      <td>0.003811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20812</td>\n",
       "      <td>0.002835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20812</td>\n",
       "      <td>0.004497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20813</td>\n",
       "      <td>0.005211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20813</td>\n",
       "      <td>0.007223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20814</td>\n",
       "      <td>0.005799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10826</th>\n",
       "      <td>25987</td>\n",
       "      <td>1.000837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10827</th>\n",
       "      <td>25987</td>\n",
       "      <td>1.000144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10828</th>\n",
       "      <td>25987</td>\n",
       "      <td>0.999434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10829</th>\n",
       "      <td>25988</td>\n",
       "      <td>0.999469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10830</th>\n",
       "      <td>25988</td>\n",
       "      <td>0.996846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10831</th>\n",
       "      <td>25988</td>\n",
       "      <td>0.998526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10832</th>\n",
       "      <td>25989</td>\n",
       "      <td>0.003643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10833</th>\n",
       "      <td>25990</td>\n",
       "      <td>1.000234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10834</th>\n",
       "      <td>25991</td>\n",
       "      <td>0.999835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10835</th>\n",
       "      <td>25991</td>\n",
       "      <td>0.999786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10836</th>\n",
       "      <td>25992</td>\n",
       "      <td>0.006008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10837</th>\n",
       "      <td>25993</td>\n",
       "      <td>0.999642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10838</th>\n",
       "      <td>25993</td>\n",
       "      <td>0.998936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10839</th>\n",
       "      <td>25994</td>\n",
       "      <td>0.997444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10840</th>\n",
       "      <td>25995</td>\n",
       "      <td>0.984017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10841</th>\n",
       "      <td>25995</td>\n",
       "      <td>0.999604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10842</th>\n",
       "      <td>25995</td>\n",
       "      <td>0.997521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10843</th>\n",
       "      <td>25995</td>\n",
       "      <td>1.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10844</th>\n",
       "      <td>25995</td>\n",
       "      <td>0.999689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10845</th>\n",
       "      <td>25995</td>\n",
       "      <td>0.998857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10846</th>\n",
       "      <td>25995</td>\n",
       "      <td>0.026362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10847</th>\n",
       "      <td>25996</td>\n",
       "      <td>0.999240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10848</th>\n",
       "      <td>25996</td>\n",
       "      <td>0.997191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10849</th>\n",
       "      <td>25997</td>\n",
       "      <td>0.986399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10850</th>\n",
       "      <td>25997</td>\n",
       "      <td>0.999487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10851</th>\n",
       "      <td>25997</td>\n",
       "      <td>1.000258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10852</th>\n",
       "      <td>25998</td>\n",
       "      <td>0.004951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10853</th>\n",
       "      <td>25999</td>\n",
       "      <td>0.999514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10854</th>\n",
       "      <td>25999</td>\n",
       "      <td>0.999853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10855</th>\n",
       "      <td>25999</td>\n",
       "      <td>0.999802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10856 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     score\n",
       "0      20800  0.999392\n",
       "1      20800  0.998562\n",
       "2      20800  0.998482\n",
       "3      20801  0.004707\n",
       "4      20802  0.005377\n",
       "5      20802  0.003506\n",
       "6      20803  0.999858\n",
       "7      20803  0.997517\n",
       "8      20804  0.003488\n",
       "9      20805  0.005277\n",
       "10     20806  0.999990\n",
       "11     20807  0.003950\n",
       "12     20807  0.449685\n",
       "13     20808  0.001446\n",
       "14     20808  0.008863\n",
       "15     20809  0.581511\n",
       "16     20810  0.004766\n",
       "17     20810  0.004062\n",
       "18     20810  0.004550\n",
       "19     20810  0.006808\n",
       "20     20810  0.004325\n",
       "21     20810  0.003358\n",
       "22     20810  0.007157\n",
       "23     20811  0.004958\n",
       "24     20812  0.003811\n",
       "25     20812  0.002835\n",
       "26     20812  0.004497\n",
       "27     20813  0.005211\n",
       "28     20813  0.007223\n",
       "29     20814  0.005799\n",
       "...      ...       ...\n",
       "10826  25987  1.000837\n",
       "10827  25987  1.000144\n",
       "10828  25987  0.999434\n",
       "10829  25988  0.999469\n",
       "10830  25988  0.996846\n",
       "10831  25988  0.998526\n",
       "10832  25989  0.003643\n",
       "10833  25990  1.000234\n",
       "10834  25991  0.999835\n",
       "10835  25991  0.999786\n",
       "10836  25992  0.006008\n",
       "10837  25993  0.999642\n",
       "10838  25993  0.998936\n",
       "10839  25994  0.997444\n",
       "10840  25995  0.984017\n",
       "10841  25995  0.999604\n",
       "10842  25995  0.997521\n",
       "10843  25995  1.000045\n",
       "10844  25995  0.999689\n",
       "10845  25995  0.998857\n",
       "10846  25995  0.026362\n",
       "10847  25996  0.999240\n",
       "10848  25996  0.997191\n",
       "10849  25997  0.986399\n",
       "10850  25997  0.999487\n",
       "10851  25997  1.000258\n",
       "10852  25998  0.004951\n",
       "10853  25999  0.999514\n",
       "10854  25999  0.999853\n",
       "10855  25999  0.999802\n",
       "\n",
       "[10856 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = (ans1 + ans2)/2\n",
    "ans = np.squeeze(ans)\n",
    "ans_df = pd.DataFrame(data={'id':list(test_ID),'score':list(ans)})\n",
    "ans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_bin(s):\n",
    "#     print(df.score)\n",
    "    if s>0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>0.998812</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>0.004707</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>0.998688</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20805</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20806</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20807</td>\n",
       "      <td>0.226817</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20808</td>\n",
       "      <td>0.005155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20809</td>\n",
       "      <td>0.581511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20810</td>\n",
       "      <td>0.005004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20811</td>\n",
       "      <td>0.004958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20812</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20813</td>\n",
       "      <td>0.006217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20814</td>\n",
       "      <td>0.005799</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20815</td>\n",
       "      <td>0.958660</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20816</td>\n",
       "      <td>0.998987</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20817</td>\n",
       "      <td>0.997706</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20818</td>\n",
       "      <td>0.010523</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20819</td>\n",
       "      <td>1.000560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20820</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20821</td>\n",
       "      <td>0.030407</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20822</td>\n",
       "      <td>0.084259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20823</td>\n",
       "      <td>0.004124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20824</td>\n",
       "      <td>0.513162</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20825</td>\n",
       "      <td>0.844091</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20826</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20827</td>\n",
       "      <td>0.999068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20828</td>\n",
       "      <td>0.987519</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20829</td>\n",
       "      <td>0.542143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>25970</td>\n",
       "      <td>0.268771</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5171</th>\n",
       "      <td>25971</td>\n",
       "      <td>0.999524</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5172</th>\n",
       "      <td>25972</td>\n",
       "      <td>0.004347</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5173</th>\n",
       "      <td>25973</td>\n",
       "      <td>0.274942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5174</th>\n",
       "      <td>25974</td>\n",
       "      <td>0.005559</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5175</th>\n",
       "      <td>25975</td>\n",
       "      <td>0.998409</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5176</th>\n",
       "      <td>25976</td>\n",
       "      <td>0.998103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5177</th>\n",
       "      <td>25977</td>\n",
       "      <td>1.000717</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>25978</td>\n",
       "      <td>0.999594</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>25979</td>\n",
       "      <td>0.999161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5180</th>\n",
       "      <td>25980</td>\n",
       "      <td>0.998924</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5181</th>\n",
       "      <td>25981</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5182</th>\n",
       "      <td>25982</td>\n",
       "      <td>0.721022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5183</th>\n",
       "      <td>25983</td>\n",
       "      <td>0.999622</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5184</th>\n",
       "      <td>25984</td>\n",
       "      <td>1.000905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185</th>\n",
       "      <td>25985</td>\n",
       "      <td>0.906434</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5186</th>\n",
       "      <td>25986</td>\n",
       "      <td>0.253662</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5187</th>\n",
       "      <td>25987</td>\n",
       "      <td>1.000138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5188</th>\n",
       "      <td>25988</td>\n",
       "      <td>0.998280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5189</th>\n",
       "      <td>25989</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5190</th>\n",
       "      <td>25990</td>\n",
       "      <td>1.000234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>25991</td>\n",
       "      <td>0.999810</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>25992</td>\n",
       "      <td>0.006008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5193</th>\n",
       "      <td>25993</td>\n",
       "      <td>0.999289</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>25994</td>\n",
       "      <td>0.997444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>25995</td>\n",
       "      <td>0.858014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>25996</td>\n",
       "      <td>0.998215</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>25997</td>\n",
       "      <td>0.995381</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>25998</td>\n",
       "      <td>0.004951</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5199</th>\n",
       "      <td>25999</td>\n",
       "      <td>0.999723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     score  label\n",
       "0     20800  0.998812      0\n",
       "1     20801  0.004707      1\n",
       "2     20802  0.004441      1\n",
       "3     20803  0.998688      0\n",
       "4     20804  0.003488      1\n",
       "5     20805  0.005277      1\n",
       "6     20806  0.999990      0\n",
       "7     20807  0.226817      1\n",
       "8     20808  0.005155      1\n",
       "9     20809  0.581511      0\n",
       "10    20810  0.005004      1\n",
       "11    20811  0.004958      1\n",
       "12    20812  0.003715      1\n",
       "13    20813  0.006217      1\n",
       "14    20814  0.005799      1\n",
       "15    20815  0.958660      0\n",
       "16    20816  0.998987      0\n",
       "17    20817  0.997706      0\n",
       "18    20818  0.010523      1\n",
       "19    20819  1.000560      0\n",
       "20    20820  0.009402      1\n",
       "21    20821  0.030407      1\n",
       "22    20822  0.084259      1\n",
       "23    20823  0.004124      1\n",
       "24    20824  0.513162      0\n",
       "25    20825  0.844091      0\n",
       "26    20826  0.003795      1\n",
       "27    20827  0.999068      0\n",
       "28    20828  0.987519      0\n",
       "29    20829  0.542143      0\n",
       "...     ...       ...    ...\n",
       "5170  25970  0.268771      1\n",
       "5171  25971  0.999524      0\n",
       "5172  25972  0.004347      1\n",
       "5173  25973  0.274942      1\n",
       "5174  25974  0.005559      1\n",
       "5175  25975  0.998409      0\n",
       "5176  25976  0.998103      0\n",
       "5177  25977  1.000717      0\n",
       "5178  25978  0.999594      0\n",
       "5179  25979  0.999161      0\n",
       "5180  25980  0.998924      0\n",
       "5181  25981  0.009441      1\n",
       "5182  25982  0.721022      0\n",
       "5183  25983  0.999622      0\n",
       "5184  25984  1.000905      0\n",
       "5185  25985  0.906434      0\n",
       "5186  25986  0.253662      1\n",
       "5187  25987  1.000138      0\n",
       "5188  25988  0.998280      0\n",
       "5189  25989  0.003643      1\n",
       "5190  25990  1.000234      0\n",
       "5191  25991  0.999810      0\n",
       "5192  25992  0.006008      1\n",
       "5193  25993  0.999289      0\n",
       "5194  25994  0.997444      0\n",
       "5195  25995  0.858014      0\n",
       "5196  25996  0.998215      0\n",
       "5197  25997  0.995381      0\n",
       "5198  25998  0.004951      1\n",
       "5199  25999  0.999723      0\n",
       "\n",
       "[5200 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ans_df['label']=1\n",
    "ans_df = ans_df.groupby('id').mean().reset_index()\n",
    "ans_df['label'] = ans_df['score'].apply(transfer_bin)\n",
    "ans_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ans_df.drop('score',axis=1,inplace=True)\n",
    "ans_df.to_csv('output/kaggle_testing3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2539"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_df.label.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pickle.load(open('output/tokenizer.pkl','rb'))\n",
    "model = load_model('model/Reg_keras.h5_all.h5')\n",
    "model.load_weights('model/Reg_keras.h5')\n",
    "model2 = load_model('model/Reg_keras2.h5_all.h5')\n",
    "model2.load_weights('model/Reg_keras2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stop_words.txt') as f:\n",
    "    stop_words_list = f.read().splitlines() #stop_list1\n",
    "stop_list2 = pickle.load(open('data/stop_list2.pkl','rb'))\n",
    "ps = PorterStemmer() # Stemming\n",
    "stop_words = set(stopwords.words('english')) #Stopword\n",
    "short = ['.', ',', '\"', \"\\'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', \"'at\",\n",
    "         \"_\",\"`\",\"\\'\\'\",\"--\",\"``\",\".,\",\"//\",\":\",\"___\",'_the','-',\"'em\",\".com\",\n",
    "                   '\\'s','\\'m','\\'re','\\'ll','\\'d','n\\'t','shan\\'t',\"...\",\"\\'ve\",'u']\n",
    "stop_words_list.extend(short)\n",
    "stop_words_list.extend(stop_list2)\n",
    "stop_words.update(stop_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    tokens = [i for i in word_tokenize(texts.lower()) if i not in stop_words]  # Tokenization.# Lowercasing\n",
    "    token_result = ''\n",
    "    token_result_ = ''\n",
    "    for i,token in enumerate(tokens): #list2str\n",
    "        token_result += ps.stem(token) + ' '\n",
    "    token_result = ''.join([i for i in token_result if not i.isdigit()])\n",
    "    token_result = [i for i in word_tokenize(token_result) if i not in stop_words]\n",
    "    for i,token in enumerate(token_result):\n",
    "        token_result_ += token + ' '\n",
    "    return token_result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a news content(text length should > 9): The United States is at historic record highs of individuals being apprehended on the border from countries with terrorist ties such as Pakistan or Afghanistan or Syria.\n"
     ]
    }
   ],
   "source": [
    "test_text = input('Enter a news content(text length should > 9): ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [str(test_text)]\n",
    "test_df = pd.DataFrame(data={'text':test_list})\n",
    "test_list = test_df['text'].apply(preprocess).tolist()\n",
    "test_id = [0]\n",
    "test_data = tokenizer.texts_to_sequences(test_list)\n",
    "# len(test_data) , len(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 237\n",
    "test_X = []\n",
    "test_ID = []\n",
    "for tokens,id_ in zip(test_data,test_id):\n",
    "    length = len(tokens)\n",
    "    iter_ = int(length / max_len)\n",
    "    if iter_ > 0:\n",
    "        temp = tokens\n",
    "        for it in range(iter_):\n",
    "#             bound = (it+1)*max_len\n",
    "            pre = temp[:max_len]\n",
    "            temp = temp[max_len:]\n",
    "#             print('   ',len(pre))\n",
    "            test_X.append(pre)\n",
    "            test_ID.append(id_)\n",
    "            if (len(temp) <= max_len) and (len(temp)>=9):\n",
    "#                 print('   ',len(temp))\n",
    "                test_X.append(temp)\n",
    "                test_ID.append(id_)\n",
    "    else:\n",
    "#         if len(tokens) >= 9:\n",
    "        test_X.append(tokens)\n",
    "        test_ID.append(id_)\n",
    "# len(test_X) , len(test_ID) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pad_sequences(test_X, maxlen=max_len, padding= 'post' )\n",
    "test_ID = np.array(test_ID)\n",
    "# test_X.shape , test_ID.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans1 = model.predict(test_X)\n",
    "ans2 = model2.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025374526157975197"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = (ans1 + ans2)/2\n",
    "ans = np.squeeze(ans)\n",
    "try:\n",
    "    ans_df = pd.DataFrame(data={'id':[int(test_ID)],'score':[float(ans)]})\n",
    "except:\n",
    "    ans_df = pd.DataFrame(data={'id':test_ID,'score':list(ans)})\n",
    "ans_df = ans_df.groupby('id').mean().reset_index()\n",
    "ans = float(ans_df.score.values)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I got you! Lier,Lier,Pants-on-Fire!!\n"
     ]
    }
   ],
   "source": [
    "if ans < 0.1:\n",
    "    print(' ','I got you! Lier,Lier,Pants on Fire!!')\n",
    "elif ans <= 0.2:\n",
    "    print(' ','Oh!You are fake news!')\n",
    "elif ans <0.4:\n",
    "    print(' ','Well..It\\'s mostly fake though..')\n",
    "elif ans <=0.6:\n",
    "    print(' ','Hm~~It\\'s half-true and half-fake...')\n",
    "elif ans <=0.9:\n",
    "    print(' ','Um~It\\'s mostly true^^')\n",
    "else:\n",
    "    print(' ','It\\'s a true story~~!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
