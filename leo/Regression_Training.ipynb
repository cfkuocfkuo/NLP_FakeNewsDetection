{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\" \n",
    "import keras\n",
    "import sent2vec\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq\n",
    "from seq2seq.models import Seq2Seq\n",
    "from keras.utils import multi_gpu_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "from keras.utils import *\n",
    "from sklearn.model_selection import *\n",
    "import random\n",
    "from random import shuffle\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from keras.utils.generic_utils import *\n",
    "from keras import regularizers\n",
    "import string\n",
    "import unicodedata as udata\n",
    "import pickle\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "import pause, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import *\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    111023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cypherpunk',\n",
       " 'scowl',\n",
       " 'ilovaisk',\n",
       " 'khafi',\n",
       " 'nayarit',\n",
       " 'dalkey',\n",
       " 'ncaa',\n",
       " 'onu',\n",
       " 'burj',\n",
       " 'shantal',\n",
       " 'leftwich',\n",
       " 'niezal',\n",
       " 'waken',\n",
       " 'clearer',\n",
       " 'mauriupol',\n",
       " 'vormachtstellung',\n",
       " 'irvington',\n",
       " 'segurament',\n",
       " 'mazi',\n",
       " 'lewington',\n",
       " 'backblast',\n",
       " 'stesso',\n",
       " 'escuela',\n",
       " 'kasperski',\n",
       " 'steingart',\n",
       " 'shrew',\n",
       " 'iucfcrsg',\n",
       " 'agrichem',\n",
       " 'julianna',\n",
       " 'longev',\n",
       " 'roywoodjr',\n",
       " 'howarth',\n",
       " 'preshow',\n",
       " 'ruder',\n",
       " 'yoon',\n",
       " 'raatz',\n",
       " 'balibous',\n",
       " 'cisgend',\n",
       " 'jamon',\n",
       " 'day',\n",
       " 'hirsch',\n",
       " 'latham',\n",
       " 'lang',\n",
       " 'wootan',\n",
       " 'praxi',\n",
       " 'bylock',\n",
       " 'sternshowaddict',\n",
       " 'mari',\n",
       " 'apm',\n",
       " 'sohr',\n",
       " 'feticid',\n",
       " 'alimov',\n",
       " 'beigebracht',\n",
       " 'cynergi',\n",
       " 'elzeyadi',\n",
       " 'baccellieri',\n",
       " 'paladio',\n",
       " 'humblebrag',\n",
       " 'brade',\n",
       " 'achil',\n",
       " 'flourish',\n",
       " 'opep',\n",
       " 'ecuatoriano',\n",
       " 'interestingli',\n",
       " 'sanctifi',\n",
       " 'redict',\n",
       " 'genotox',\n",
       " 'monetari',\n",
       " 'asidero',\n",
       " 'twattish',\n",
       " 'horst',\n",
       " 'spackman',\n",
       " 'uresti',\n",
       " 'poliv',\n",
       " 'margaeri',\n",
       " 'conquistar',\n",
       " 'ballcap',\n",
       " 'oteachjohn',\n",
       " 'jostl',\n",
       " 'noncompetit',\n",
       " 'qrator',\n",
       " 'lamarr',\n",
       " 'grazziotin',\n",
       " 'cam',\n",
       " 'felin',\n",
       " 'pilfer',\n",
       " 'hemings',\n",
       " 'maasssaid',\n",
       " 'gesprekken',\n",
       " 'footag',\n",
       " 'machida',\n",
       " 'surest',\n",
       " 'overjoy',\n",
       " 'terrapin',\n",
       " 'trajectori',\n",
       " 'kayle',\n",
       " 'keeneland',\n",
       " 'toss',\n",
       " 'embol',\n",
       " 'pianism',\n",
       " 'mcmansion',\n",
       " 'garantir',\n",
       " 'bioinsecticid',\n",
       " 'savithramma',\n",
       " 'cuatro',\n",
       " 'teamet',\n",
       " 'biegen',\n",
       " 'regul',\n",
       " 'sorprendi',\n",
       " 'landgenoten',\n",
       " 'fabel',\n",
       " 'lcmtlfe',\n",
       " 'bromin',\n",
       " 'shou',\n",
       " 'afyar',\n",
       " 'nonread',\n",
       " 'alberg',\n",
       " 'arrabye',\n",
       " 'empanada',\n",
       " 'ultralow',\n",
       " 'greensboro',\n",
       " 'figura',\n",
       " 'teamax',\n",
       " 'recipi',\n",
       " 'irrev',\n",
       " 'zorn',\n",
       " 'conduir',\n",
       " 'braxton',\n",
       " 'muster',\n",
       " 'yugoslavia',\n",
       " 'volksdeutsch',\n",
       " 'crematori',\n",
       " 'aasif',\n",
       " 'inert',\n",
       " 'yourcenar',\n",
       " 'widvey',\n",
       " 'penguala',\n",
       " 'dont',\n",
       " 'cryptom',\n",
       " 'innkeep',\n",
       " 'mbe',\n",
       " 'lamak',\n",
       " 'discoteca',\n",
       " 'teatim',\n",
       " 'dh',\n",
       " 'coleen',\n",
       " 'maybush',\n",
       " 'itsgoingdown',\n",
       " 'east',\n",
       " 'goodel',\n",
       " 'elid',\n",
       " 'lorelai',\n",
       " 'basada',\n",
       " 'www',\n",
       " 'ploie',\n",
       " 'sqoq',\n",
       " 'hauslohn',\n",
       " 'souffrent',\n",
       " 'phew',\n",
       " 'hurriyah',\n",
       " 'estrategia',\n",
       " 'escalad',\n",
       " 'veon',\n",
       " 'frn',\n",
       " 'mendica',\n",
       " 'verfolgung',\n",
       " 'unii',\n",
       " 'prolaps',\n",
       " 'tensor',\n",
       " 'ofili',\n",
       " 'curioso',\n",
       " 'disperso',\n",
       " 'vandross',\n",
       " 'karimov',\n",
       " 'moviedom',\n",
       " 'maza',\n",
       " 'cozier',\n",
       " 'shaul',\n",
       " 'disconnected',\n",
       " 'snowsho',\n",
       " 'demitiu',\n",
       " 'perrett',\n",
       " 'workman',\n",
       " 'elmar',\n",
       " 'frolic',\n",
       " 'bradstreet',\n",
       " 'reconciliaton',\n",
       " 'cyberintrud',\n",
       " 'azraq',\n",
       " 'letteron',\n",
       " 'loot',\n",
       " 'terrorism',\n",
       " 'srebrenica',\n",
       " 'qaboun',\n",
       " 'anglic',\n",
       " 'brult',\n",
       " 'declin',\n",
       " 'entstandenen',\n",
       " 'druggist',\n",
       " 'hallucin',\n",
       " 'austan',\n",
       " 'bensouda',\n",
       " 'maxx',\n",
       " 'stepanova',\n",
       " 'carapac',\n",
       " 'concussiom',\n",
       " 'louderback',\n",
       " 'spiral',\n",
       " 'mazzaroth',\n",
       " 'madhu',\n",
       " 'chidiac',\n",
       " 'frag',\n",
       " 'ayah',\n",
       " 'sferic',\n",
       " 'cielsa',\n",
       " 'bennet',\n",
       " 'maglion',\n",
       " 'gate',\n",
       " 'fuallofy',\n",
       " 'parlera',\n",
       " 'gevorgyan',\n",
       " 'cremo',\n",
       " 'ensembl',\n",
       " 'semani',\n",
       " 'afr',\n",
       " 'trilog',\n",
       " 'ostlund',\n",
       " 'correlaci',\n",
       " 'petruk',\n",
       " 'starboard',\n",
       " 'rpi',\n",
       " 'felo',\n",
       " 'ckwunsch',\n",
       " 'apuesta',\n",
       " 'alge',\n",
       " 'aispuro',\n",
       " 'alegando',\n",
       " 'mirrle',\n",
       " 'qezjfcweeh',\n",
       " 'rockhound',\n",
       " 'flyth',\n",
       " 'bermen',\n",
       " 'egemenli',\n",
       " 'armpit',\n",
       " 'ruah',\n",
       " 'femen',\n",
       " 'bekleniyor',\n",
       " 'odish',\n",
       " 'recraft',\n",
       " 'truell',\n",
       " 'arnais',\n",
       " 'analyz',\n",
       " 'ook',\n",
       " 'occ',\n",
       " 'appli',\n",
       " 'egot',\n",
       " 'ndono',\n",
       " 'awn',\n",
       " 'flopola',\n",
       " 'koreaha',\n",
       " 'pharoah',\n",
       " 'opinion',\n",
       " 'aridawari',\n",
       " 'ristorant',\n",
       " 'creara',\n",
       " 'mobieltj',\n",
       " 'minefield',\n",
       " 'barnett',\n",
       " 'filipino',\n",
       " 'peninsula',\n",
       " 'prioriti',\n",
       " 'hedaya',\n",
       " 'balaz',\n",
       " 'sanitarium',\n",
       " 'countrysid',\n",
       " 'velopp',\n",
       " 'malusi',\n",
       " 'mislaid',\n",
       " 'command',\n",
       " 'worldwid',\n",
       " 'aqualad',\n",
       " 'glockner',\n",
       " 'stealthi',\n",
       " 'mi',\n",
       " 'waynemadsenreport',\n",
       " 'hilft',\n",
       " 'aprovecharlo',\n",
       " 'hidden',\n",
       " 'infofield',\n",
       " 'mislykt',\n",
       " 'ajeena',\n",
       " 'libi',\n",
       " 'lorri',\n",
       " 'ilrc',\n",
       " 'gertz',\n",
       " 'presentars',\n",
       " 'smart',\n",
       " 'crisper',\n",
       " 'montejo',\n",
       " 'nouri',\n",
       " 'overpack',\n",
       " 'frankensteinish',\n",
       " 'fond',\n",
       " 'cardio',\n",
       " 'vallejo',\n",
       " 'embrey',\n",
       " 'aaj',\n",
       " 'mrcheckpoint',\n",
       " 'grandchildren',\n",
       " 'squirt',\n",
       " 'underactiv',\n",
       " 'trinitrotoluen',\n",
       " 'brimp',\n",
       " 'hurti',\n",
       " 'ghislain',\n",
       " 'comerciai',\n",
       " 'villag',\n",
       " 'shazad',\n",
       " 'panatag',\n",
       " 'deveolp',\n",
       " 'elysium',\n",
       " 'responsibl',\n",
       " 'francotirador',\n",
       " 'godblessamerica',\n",
       " 'vpaym',\n",
       " 'gepland',\n",
       " 'unerkl',\n",
       " 'gounod',\n",
       " 'eyeo',\n",
       " 'spart',\n",
       " 'conselho',\n",
       " 'escamoteado',\n",
       " 'conspiraci',\n",
       " 'trole',\n",
       " 'lewiston',\n",
       " 'taught',\n",
       " 'hoskin',\n",
       " 'buisson',\n",
       " 'katayoon',\n",
       " 'frontag',\n",
       " 'blubutterfli',\n",
       " 'culb',\n",
       " 'hl',\n",
       " 'ramach',\n",
       " 'miro',\n",
       " 'pasars',\n",
       " 'wandira',\n",
       " 'individualis',\n",
       " 'painter',\n",
       " 'macaray',\n",
       " 'identifica',\n",
       " 'handymusik',\n",
       " 'aplaz',\n",
       " 'schlosser',\n",
       " 'schrill',\n",
       " 'useabl',\n",
       " 'faniel',\n",
       " 'rupprecht',\n",
       " 'beaufort',\n",
       " 'frigat',\n",
       " 'establecido',\n",
       " 'je',\n",
       " 'angiogen',\n",
       " 'meechai',\n",
       " 'nasfaa',\n",
       " 'reedley',\n",
       " 'insenst',\n",
       " 'kilgor',\n",
       " 'cleaner',\n",
       " 'petroyuan',\n",
       " 'farmal',\n",
       " 'wolv',\n",
       " 'rummag',\n",
       " 'engordado',\n",
       " 'tudiant',\n",
       " 'taberski',\n",
       " 'nandi',\n",
       " 'valeriya',\n",
       " 'hagglund',\n",
       " 'saud',\n",
       " 'berschritten',\n",
       " 'nake',\n",
       " 'negligencia',\n",
       " 'quotabl',\n",
       " 'tuft',\n",
       " 'ganglia',\n",
       " 'hardwir',\n",
       " 'dojo',\n",
       " 'gewaltigen',\n",
       " 'bootlegg',\n",
       " 'skime',\n",
       " 'fx',\n",
       " 'question',\n",
       " 'ettalhi',\n",
       " 'aground',\n",
       " 'bundesnachrichtendienst',\n",
       " 'papa',\n",
       " 'forbad',\n",
       " 'nonendang',\n",
       " 'adegbil',\n",
       " 'coreligionist',\n",
       " 'tixtla',\n",
       " 'ardley',\n",
       " 'naccio',\n",
       " 'viewfind',\n",
       " 'cscl',\n",
       " 'saleh',\n",
       " 'tuc',\n",
       " 'mpa',\n",
       " 'feral',\n",
       " 'crowder',\n",
       " 'sharmini',\n",
       " 'tampa',\n",
       " 'ronnlund',\n",
       " 'wgrz',\n",
       " 'bankrupt',\n",
       " 'rizzo',\n",
       " 'trt',\n",
       " 'arbeitsf',\n",
       " 'trepper',\n",
       " 'cricket',\n",
       " 'dabney',\n",
       " 'mumbo',\n",
       " 'heteron',\n",
       " 'bispehnol',\n",
       " 'founderchurch',\n",
       " 'shikha',\n",
       " 'vernacular',\n",
       " 'crean',\n",
       " 'yiftah',\n",
       " 'wiata',\n",
       " 'denial',\n",
       " 'valu',\n",
       " 'duncanfyf',\n",
       " 'bekim',\n",
       " 'dawood',\n",
       " 'vaulter',\n",
       " 'colmar',\n",
       " 'juliacraven',\n",
       " 'alchouin',\n",
       " 'comedido',\n",
       " 'prestigio',\n",
       " 'elmsford',\n",
       " 'falkland',\n",
       " 'babysat',\n",
       " 'acquit',\n",
       " 'palpit',\n",
       " 'milley',\n",
       " 'dirigimo',\n",
       " 'lesar',\n",
       " 'oechsli',\n",
       " 'popula',\n",
       " 'willdani',\n",
       " 'hempse',\n",
       " 'dure',\n",
       " 'cra',\n",
       " 'prenderlo',\n",
       " 'bazinga',\n",
       " 'justo',\n",
       " 'silenta',\n",
       " 'tular',\n",
       " 'dubuffet',\n",
       " 'blessed',\n",
       " 'corsi',\n",
       " 'ziggi',\n",
       " 'bahahahaaa',\n",
       " 'wuow',\n",
       " 'lazili',\n",
       " 'historiador',\n",
       " 'havok',\n",
       " 'victimhood',\n",
       " 'troop',\n",
       " 'cessionnism',\n",
       " 'knud',\n",
       " 'kerker',\n",
       " 'pinotti',\n",
       " 'muham',\n",
       " 'barnabi',\n",
       " 'yunnan',\n",
       " 'maior',\n",
       " 'agenzia',\n",
       " 'suppositori',\n",
       " 'hobson',\n",
       " 'desarticulan',\n",
       " 'fizzi',\n",
       " 'rosindel',\n",
       " 'hubieran',\n",
       " 'uro',\n",
       " 'sangreaal',\n",
       " 'turismo',\n",
       " 'cocksur',\n",
       " 'deepli',\n",
       " 'shim',\n",
       " 'lutsenko',\n",
       " 'truism',\n",
       " 'accueil',\n",
       " 'becker',\n",
       " 'ballo',\n",
       " 'enza',\n",
       " 'valk',\n",
       " 'chemtrail',\n",
       " 'burress',\n",
       " 'mannesmann',\n",
       " 'floriss',\n",
       " 'want',\n",
       " 'agp',\n",
       " 'nansaturday',\n",
       " 'preemptiv',\n",
       " 'rmalar',\n",
       " 'rulemak',\n",
       " 'instanti',\n",
       " 'glavin',\n",
       " 'chili',\n",
       " 'ggpzdmgbnw',\n",
       " 'cammyyyedgar',\n",
       " 'imbroglio',\n",
       " 'cinderblock',\n",
       " 'lefkowitz',\n",
       " 'debrief',\n",
       " 'ago',\n",
       " 'boisineau',\n",
       " 'athen',\n",
       " 'boat',\n",
       " 'poof',\n",
       " 'willaim',\n",
       " 'lycopen',\n",
       " 'leert',\n",
       " 'rome',\n",
       " 'redstat',\n",
       " 'regionu',\n",
       " 'itp',\n",
       " 'billclinton',\n",
       " 'polnewsnetwork',\n",
       " 'kilt',\n",
       " 'michaelkeaton',\n",
       " 'doel',\n",
       " 'neocoloni',\n",
       " 'ordenando',\n",
       " 'respeta',\n",
       " 'nuclearwarforh',\n",
       " 'paltrow',\n",
       " 'kiso',\n",
       " 'wlwt',\n",
       " 'saleha',\n",
       " 'fionnuala',\n",
       " 'appelunt',\n",
       " 'bandera',\n",
       " 'novsak',\n",
       " 'lhdzfscaqo',\n",
       " 'kliment',\n",
       " 'tonn',\n",
       " 'amazonian',\n",
       " 'leg',\n",
       " 'tigantourin',\n",
       " 'mamet',\n",
       " 'lunchbox',\n",
       " 'jash',\n",
       " 'standingrocksolidaritynetwork',\n",
       " 'syrah',\n",
       " 'gavino',\n",
       " 'fomalhaut',\n",
       " 'aristotl',\n",
       " 'bellic',\n",
       " 'lenni',\n",
       " 'lunaflora',\n",
       " 'migratoir',\n",
       " 'promulg',\n",
       " 'aprendido',\n",
       " 'piraci',\n",
       " 'forfeitur',\n",
       " 'singwang',\n",
       " 'nandecemb',\n",
       " 'excesivament',\n",
       " 'integrationsinitiativen',\n",
       " 'foxx',\n",
       " 'batist',\n",
       " 'lymphomyosot',\n",
       " 'nanomaha',\n",
       " 'encrust',\n",
       " 'hunchun',\n",
       " 'trampl',\n",
       " 'firstrebutt',\n",
       " 'dataporn',\n",
       " 'brecher',\n",
       " 'katehon',\n",
       " 'agostino',\n",
       " 'triomph',\n",
       " 'gbp',\n",
       " 'batallon',\n",
       " 'dailystar',\n",
       " 'mattvesp',\n",
       " 'uwe',\n",
       " 'coronado',\n",
       " 'rooflin',\n",
       " 'mascara',\n",
       " 'newsgroup',\n",
       " 'infoba',\n",
       " 'mattioli',\n",
       " 'desatada',\n",
       " 'catercorn',\n",
       " 'dthe',\n",
       " 'unlikelihood',\n",
       " 'jeffsess',\n",
       " 'palaszczuk',\n",
       " 'spanier',\n",
       " 'stabilit',\n",
       " 'gimbel',\n",
       " 'insalubri',\n",
       " 'vkontakt',\n",
       " 'veiligheidsfout',\n",
       " 'basicli',\n",
       " 'ruiz',\n",
       " 'guinda',\n",
       " 'timopt',\n",
       " 'majest',\n",
       " 'arborist',\n",
       " 'levanten',\n",
       " 'curso',\n",
       " 'tirmemesi',\n",
       " 'jasika',\n",
       " 'plac',\n",
       " 'logotyp',\n",
       " 'milosz',\n",
       " 'sanwar',\n",
       " 'litiasi',\n",
       " 'sayaghi',\n",
       " 'furthermor',\n",
       " 'stumpf',\n",
       " 'hackensack',\n",
       " 'metralleta',\n",
       " 'mwprinc',\n",
       " 'lewdli',\n",
       " 'heist',\n",
       " 'stoicism',\n",
       " 'bukam',\n",
       " 'congdon',\n",
       " 'fpr',\n",
       " 'formo',\n",
       " 'kamparw',\n",
       " 'isight',\n",
       " 'corea',\n",
       " 'newsmak',\n",
       " 'megastar',\n",
       " 'maneuv',\n",
       " 'extebxtawh',\n",
       " 'niet',\n",
       " 'slavin',\n",
       " 'salom',\n",
       " 'stressor',\n",
       " 'rule',\n",
       " 'magrepha',\n",
       " 'fame',\n",
       " 'hyperborean',\n",
       " 'zamieszkami',\n",
       " 'abul',\n",
       " 'ficarem',\n",
       " 'quelqu',\n",
       " 'withold',\n",
       " 'shandera',\n",
       " 'junkscienc',\n",
       " 'reino',\n",
       " 'xiasha',\n",
       " 'xeon',\n",
       " 'boroughwid',\n",
       " 'friendless',\n",
       " 'ladonna',\n",
       " 'unterlegen',\n",
       " 'keisha',\n",
       " 'sonnet',\n",
       " 'nevin',\n",
       " 'dusha',\n",
       " 'devious',\n",
       " 'miracl',\n",
       " 'gobal',\n",
       " 'bilal',\n",
       " 'kalinin',\n",
       " 'patchi',\n",
       " 'ict',\n",
       " 'karsh',\n",
       " 'andneedlessli',\n",
       " 'complaint',\n",
       " 'arabien',\n",
       " 'jaym',\n",
       " 'incivil',\n",
       " 'parkm',\n",
       " 'halv',\n",
       " 'optimista',\n",
       " 'dbp',\n",
       " 'chanda',\n",
       " 'queermor',\n",
       " 'aduanero',\n",
       " 'cutpic',\n",
       " 'indicada',\n",
       " 'millonaria',\n",
       " 'shame',\n",
       " 'noguchi',\n",
       " 'unscop',\n",
       " 'joscelyn',\n",
       " 'implementaci',\n",
       " 'stanek',\n",
       " 'pendulum',\n",
       " 'ownership',\n",
       " 'kaput',\n",
       " 'stag',\n",
       " 'lbc',\n",
       " 'sjwism',\n",
       " 'pour',\n",
       " 'blackboxvot',\n",
       " 'huidig',\n",
       " 'straddl',\n",
       " 'basketbal',\n",
       " 'kna',\n",
       " 'lucho',\n",
       " 'anabaptist',\n",
       " 'situa',\n",
       " 'bulger',\n",
       " 'dlf',\n",
       " 'crackerjack',\n",
       " 'nonhuman',\n",
       " 'islandia',\n",
       " 'mathi',\n",
       " 'imbalanc',\n",
       " 'biochip',\n",
       " 'french',\n",
       " 'habibati',\n",
       " 'webmast',\n",
       " 'rompen',\n",
       " 'rohe',\n",
       " 'arinaminpathi',\n",
       " 'daumier',\n",
       " 'jonrappoport',\n",
       " 'arcelormitt',\n",
       " 'captia',\n",
       " 'drcarolyndean',\n",
       " 'julian',\n",
       " 'journalistenlaufbahn',\n",
       " 'rideshar',\n",
       " 'semin',\n",
       " 'drafthous',\n",
       " 'herzegovina',\n",
       " 'cabaret',\n",
       " 'littlespottedhors',\n",
       " 'mullah',\n",
       " 'meiosi',\n",
       " 'toulousain',\n",
       " 'nipp',\n",
       " 'shtwipg',\n",
       " 'shutdownloganriv',\n",
       " 'dreising',\n",
       " 'ramesh',\n",
       " 'lipscomb',\n",
       " 'jina',\n",
       " 'schatmeest',\n",
       " 'bamboozl',\n",
       " 'mirahmad',\n",
       " 'sigtarp',\n",
       " 'noteworthi',\n",
       " 'hominida',\n",
       " 'taster',\n",
       " 'gauzier',\n",
       " 'coincidiendo',\n",
       " 'contralto',\n",
       " 'micklethwait',\n",
       " 'destigmat',\n",
       " 'openli',\n",
       " 'coopertown',\n",
       " 'cattolica',\n",
       " 'bdi',\n",
       " 'myron',\n",
       " 'floranc',\n",
       " 'decisiva',\n",
       " 'bliss',\n",
       " 'steamrol',\n",
       " 'ahlburn',\n",
       " 'boxshal',\n",
       " 'identifisert',\n",
       " 'kitten',\n",
       " 'inexhaust',\n",
       " 'rodulfo',\n",
       " 'giacomo',\n",
       " 'shapiro',\n",
       " 'adoptara',\n",
       " 'arbi',\n",
       " 'cessna',\n",
       " 'pomilda',\n",
       " 'nonbind',\n",
       " 'admiss',\n",
       " 'cdp',\n",
       " 'lograrlo',\n",
       " 'bayoakomolaf',\n",
       " 'nichtssagend',\n",
       " 'zaterdag',\n",
       " 'idolat',\n",
       " 'bohn',\n",
       " 'sternupd',\n",
       " 'niigata',\n",
       " 'biochem',\n",
       " 'skye',\n",
       " 'neunzigerjahr',\n",
       " 'tahvil',\n",
       " 'narkosemittel',\n",
       " 'hornqvist',\n",
       " 'elodi',\n",
       " 'windowless',\n",
       " 'hayn',\n",
       " 'turi',\n",
       " 'omiso',\n",
       " 'gewerkschaft',\n",
       " 'starter',\n",
       " 'calamit',\n",
       " 'acaben',\n",
       " 'seelbach',\n",
       " 'roofer',\n",
       " 'gjeldand',\n",
       " 'efmarfauuazpad',\n",
       " 'concav',\n",
       " 'convoc',\n",
       " 'commentari',\n",
       " 'coniston',\n",
       " 'augustan',\n",
       " 'frow',\n",
       " 'voluptuari',\n",
       " 'tempranillo',\n",
       " 'shamsolvaezin',\n",
       " 'greenman',\n",
       " 'franquismo',\n",
       " 'repouss',\n",
       " 'giladi',\n",
       " 'bronco',\n",
       " 'saleswoman',\n",
       " 'hoaxer',\n",
       " 'minicomput',\n",
       " 'assicurandoli',\n",
       " 'couloumbi',\n",
       " 'lytl',\n",
       " 'stromsto',\n",
       " 'majdalani',\n",
       " 'nnlf',\n",
       " 'subtyp',\n",
       " 'ghani',\n",
       " 'roesel',\n",
       " 'corfu',\n",
       " 'ulul',\n",
       " 'jimq',\n",
       " 'patrouillen',\n",
       " 'bekanntmachung',\n",
       " 'submunit',\n",
       " 'odessan',\n",
       " 'gastronomi',\n",
       " 'cgrueskin',\n",
       " 'boza',\n",
       " 'preferido',\n",
       " 'pirogova',\n",
       " 'tipperari',\n",
       " 'ballator',\n",
       " 'displeas',\n",
       " 'maldef',\n",
       " 'tonya',\n",
       " 'unrwa',\n",
       " 'frye',\n",
       " 'fronti',\n",
       " 'donata',\n",
       " 'pramoedya',\n",
       " 'mongabay',\n",
       " 'freakonom',\n",
       " 'metodista',\n",
       " 'pleasantri',\n",
       " 'spell',\n",
       " 'linsan',\n",
       " 'carplay',\n",
       " 'gotnew',\n",
       " 'brother',\n",
       " 'erinschrod',\n",
       " 'pumpenherstel',\n",
       " 'adeli',\n",
       " 'kfilli',\n",
       " 'keewaywin',\n",
       " 'woud',\n",
       " 'goodli',\n",
       " 'strahan',\n",
       " 'countercurr',\n",
       " 'madhwani',\n",
       " 'boeselag',\n",
       " 'mcafe',\n",
       " 'alhag',\n",
       " 'bohr',\n",
       " 'gorel',\n",
       " 'menthofuran',\n",
       " 'grammarian',\n",
       " 'hender',\n",
       " 'makeamericamexicoagain',\n",
       " 'penney',\n",
       " 'geposteten',\n",
       " 'appui',\n",
       " 'lhakim',\n",
       " 'hydropow',\n",
       " 'steenland',\n",
       " 'fakeid',\n",
       " 'sino',\n",
       " 'bossi',\n",
       " 'restent',\n",
       " 'mellom',\n",
       " 'zxua',\n",
       " 'prankish',\n",
       " 'usar',\n",
       " 'genitalia',\n",
       " 'menait',\n",
       " 'nimitz',\n",
       " 'arecibo',\n",
       " 'kalina',\n",
       " 'yimbi',\n",
       " 'searchabl',\n",
       " 'lockup',\n",
       " 'kinesthet',\n",
       " 'jraethefanat',\n",
       " 'michelang',\n",
       " 'sonnenfeld',\n",
       " 'dice',\n",
       " 'quipag',\n",
       " 'napoleon',\n",
       " 'sustantiva',\n",
       " 'ineffici',\n",
       " 'sikh',\n",
       " 'hagrid',\n",
       " 'bulhak',\n",
       " 'pompid',\n",
       " 'offload',\n",
       " 'cepr',\n",
       " 'unconcern',\n",
       " 'foibl',\n",
       " 'untest',\n",
       " 'monosodium',\n",
       " 'cellebrit',\n",
       " 'vasconi',\n",
       " 'shorelin',\n",
       " 'cuccinelli',\n",
       " 'hyon',\n",
       " 'sydney',\n",
       " 'stavropol',\n",
       " 'texasth',\n",
       " 'filipina',\n",
       " 'easyway',\n",
       " 'vlock',\n",
       " 'schaper',\n",
       " 'lemmonii',\n",
       " 'barranquilla',\n",
       " 'kushchyenko',\n",
       " 'ipap',\n",
       " 'usuncut',\n",
       " 'infograph',\n",
       " 'needlepoint',\n",
       " 'crire',\n",
       " 'cytoskeleton',\n",
       " 'inmuebl',\n",
       " 'lisa',\n",
       " 'inamov',\n",
       " 'fabricant',\n",
       " 'bemba',\n",
       " 'qarahunj',\n",
       " 'spelunk',\n",
       " 'pemsel',\n",
       " 'zenki',\n",
       " 'routefifti',\n",
       " 'prabhoo',\n",
       " 'saman',\n",
       " 'inattent',\n",
       " 'agallup',\n",
       " 'erteilt',\n",
       " 'micr',\n",
       " 'getti',\n",
       " 'xiaomi',\n",
       " 'titel',\n",
       " 'rhodiola',\n",
       " 'hypothec',\n",
       " 'nosocomi',\n",
       " 'yobra',\n",
       " 'time',\n",
       " 'linen',\n",
       " 'purr',\n",
       " 'corset',\n",
       " 'thirteen',\n",
       " 'raggedi',\n",
       " 'mtlichen',\n",
       " 'pinot',\n",
       " 'parkland',\n",
       " 'lythcott',\n",
       " 'ceil',\n",
       " 'existent',\n",
       " 'ruvkun',\n",
       " 'pipedream',\n",
       " 'znikn',\n",
       " 'cleofila',\n",
       " 'saeta',\n",
       " 'slivka',\n",
       " 'popul',\n",
       " 'naza',\n",
       " 'groundhog',\n",
       " 'leduc',\n",
       " 'harwel',\n",
       " 'anno',\n",
       " ...}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = pd.read_csv('data/reg_data_token.csv')\n",
    "voc_list = training.text.tolist()\n",
    "voc_list_str = [str(x) for x in voc_list]\n",
    "# voc_list_len = [len(x) for x in voc_list_str]\n",
    "voc_string = ''.join(voc_list_str)\n",
    "voc_set = set(voc_string.split(' '))\n",
    "try:\n",
    "    voc_set.remove('')\n",
    "except:\n",
    "    pass\n",
    "print('   ',len(voc_set))\n",
    "voc_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1539.312791613646 2361.826171146817 2 62702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(65.0, 695.0, 2373.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(np.mean(voc_list_len) , np.std(voc_list_len) , np.min(voc_list_len) , np.max(voc_list_len) )\n",
    "# np.quantile(voc_list_len,0.25) , np.quantile(voc_list_len,0.5) , np.quantile(voc_list_len,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1539.312791613646, 33006)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.mean(voc_list_len) , len(voc_list_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer words to Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/33006 [02:39<162:33:07, 17.73s/it]"
     ]
    }
   ],
   "source": [
    "# voc_list = list(voc_set)\n",
    "# num_df = pd.DataFrame(data={'term':voc_list})\n",
    "# train_X = []\n",
    "# for news in tqdm(voc_list_str):\n",
    "#     trainX = []\n",
    "#     for term in news:\n",
    "#         try:\n",
    "#             idx = num_df[num_df['term'] == term].index[0]\n",
    "#         except:\n",
    "#             pass\n",
    "#         trainX.append(idx)\n",
    "#     train_X.append(np.array(trainX))\n",
    "# #     num_df\n",
    "# # num_df[num_df['term'] == 'ck'].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235.94686105325795 366.8819509536414 1 10268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10.0, 105.0, 361.0, 235.94686105325795)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(voc_list_str)\n",
    "tokens_enc = tokenizer.texts_to_sequences(voc_list_str)\n",
    "tokens_enc_len = [len(tokens) for tokens in tokens_enc]\n",
    "print(np.mean(tokens_enc_len) , np.std(tokens_enc_len) , np.min(tokens_enc_len) , np.max(tokens_enc_len) )\n",
    "np.quantile(tokens_enc_len,0.25) , np.quantile(tokens_enc_len,0.5) , np.quantile(tokens_enc_len,0.75) , np.mean(tokens_enc_len)\n",
    "# mean = 237\n",
    "# iqr = 885/887.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1.0\n",
      "    2.0\n",
      "    4.0\n",
      "    4.0\n",
      "    5.0\n",
      "    5.0\n",
      "    5.0\n",
      "    6.0\n",
      "    6.0\n",
      "    6.0\n",
      "    6.0\n",
      "    7.0\n",
      "    7.0\n",
      "    7.0\n",
      "    7.0\n",
      "    8.0\n",
      "    8.0\n",
      "    8.0\n",
      "    8.0\n",
      "    9.0\n",
      "    9.0\n",
      "    9.0\n",
      "    9.0\n",
      "    9.0\n",
      "    10.0\n",
      "    10.0\n",
      "    10.0\n",
      "    11.0\n",
      "    11.0\n",
      "    11.0\n",
      "    12.0\n",
      "    12.0\n",
      "    12.0\n",
      "    13.0\n",
      "    13.0\n",
      "    14.0\n",
      "    14.0\n",
      "    15.0\n",
      "    16.0\n",
      "    17.0\n",
      "    18.0\n",
      "    20.0\n",
      "    26.779999999998836\n",
      "    40.0\n",
      "    53.0\n",
      "    62.05000000000109\n",
      "    71.0\n",
      "    80.0\n",
      "    89.0\n",
      "    97.0\n",
      "    105.0\n",
      "    114.0\n",
      "    122.0\n",
      "    130.0\n",
      "    139.0\n",
      "    147.0\n",
      "    155.0\n",
      "    164.0\n",
      "    171.0\n",
      "    179.0\n",
      "    187.0\n",
      "    196.0\n",
      "    204.0\n",
      "    214.0\n",
      "    224.0\n",
      "    234.85000000000218\n",
      "    244.0\n",
      "    255.0\n",
      "    266.0\n",
      "    279.0\n",
      "    292.0\n",
      "    306.0\n",
      "    318.0\n",
      "    332.0\n",
      "    346.0\n",
      "    361.0\n",
      "    375.0\n",
      "    390.0\n",
      "    407.0\n",
      "    422.0\n",
      "    437.0\n",
      "    454.0\n",
      "    471.0\n",
      "    490.0\n",
      "    509.0\n",
      "    527.0\n",
      "    546.0\n",
      "    565.0\n",
      "    584.0\n",
      "    604.0\n",
      "    623.0\n",
      "    646.0\n",
      "    670.0\n",
      "    697.3700000000026\n",
      "    732.0\n",
      "    775.5500000000029\n",
      "    836.0\n",
      "    940.0\n",
      "    1106.8199999999997\n",
      "    1534.9099999999962\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print('   ',np.quantile(tokens_enc_len,i*0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(tokens_enc_len,0.9655)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33610, 33610)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY = training.label.tolist()\n",
    "len(trainY) , len(tokens_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49933, 49933)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 237\n",
    "train_X = []\n",
    "train_Y = []\n",
    "for tokens,label in zip(tokens_enc,trainY):\n",
    "    length = len(tokens)\n",
    "    iter_ = int(length / max_len)\n",
    "    if iter_ > 0:\n",
    "        temp = tokens\n",
    "        for it in range(iter_):\n",
    "#             bound = (it+1)*max_len\n",
    "            pre = temp[:max_len]\n",
    "            temp = temp[max_len:]\n",
    "#             print('   ',len(pre))\n",
    "            train_X.append(pre)\n",
    "            train_Y.append(label)\n",
    "            if (len(temp) <= max_len) and (len(temp)>=9):\n",
    "#                 print('   ',len(temp))\n",
    "                train_X.append(temp)\n",
    "                train_Y.append(label)\n",
    "    else:\n",
    "        if len(tokens) >= 9:\n",
    "            train_X.append(tokens)\n",
    "            train_Y.append(label)\n",
    "len(train_X) , len(train_Y)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_ = [len(x) for x in train_X]\n",
    "np.max(len_) , np.min(len_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49933, 237), (49933,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = pad_sequences(train_X, maxlen=max_len, padding= 'post' )\n",
    "train_Y = np.array(train_Y)\n",
    "train_X.shape , train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33610, 361), (33610,))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_X = pad_sequences(tokens_enc, maxlen=361, padding= 'post' )\n",
    "# train_Y = np.array(trainY)\n",
    "# train_X.shape , train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110894"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_valid_set(X_all, Y_all, percentage):\n",
    "    all_data_size = len(X_all)\n",
    "    valid_data_size = int(floor(all_data_size * percentage))\n",
    "\n",
    "    X_all, Y_all = _shuffle(X_all, Y_all)\n",
    "\n",
    "    X_train, Y_train = X_all[0:valid_data_size], Y_all[0:valid_data_size]\n",
    "    X_valid, Y_valid = X_all[valid_data_size:], Y_all[valid_data_size:]\n",
    "\n",
    "    return X_train, Y_train, X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, Y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "#     print(X.shape, Y.shape)\n",
    "    return (X[randomize], Y[randomize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_valid_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5888ce050459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_valid_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'split_valid_set' is not defined"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, valid_X, valid_Y = split_valid_set(train_X, train_Y, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(obj=(train_X,train_Y,valid_X,valid_Y),file=open('data/train_set.pkl','wb'))\n",
    "train_X, train_Y, valid_X, valid_Y = pickle.load(open('data/train_set.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 237)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 237, 128)          14194432  \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 237, 64)           73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 237, 64)           256       \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 237, 128)          66048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 237, 128)          512       \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 128)               74112     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 14,426,817\n",
      "Trainable params: 14,425,921\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_len = train_X.shape[1]\n",
    "\n",
    "inputs = Input(shape=(max_len,))\n",
    "emb_vec = Embedding(input_dim=np.max(train_X),output_dim=128,input_length=max_len)(inputs)\n",
    "conv = Conv1D(64,kernel_size=(9,), strides=1, padding='causal', data_format='channels_last')(emb_vec)\n",
    "bn = BatchNormalization()(conv)\n",
    "BiLSTM = Bidirectional(LSTM(64,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(bn)#64\n",
    "bn = BatchNormalization()(BiLSTM)\n",
    "rnn = Bidirectional(GRU(64,return_sequences=False,dropout=0.2,recurrent_dropout=0.2))(bn) #64\n",
    "bn1 = BatchNormalization()(rnn) # +FC*3? selu? 64? 3rd Dense shortcut from rnn/? kernel_initializer? dropout\n",
    "dense = Dense(128,activation='selu',kernel_initializer='lecun_normal')(bn1)\n",
    "bn2 = BatchNormalization()(dense)\n",
    "do = Dropout(0.3)(bn2)\n",
    "# bn = Concatenate()([do,bn1])\n",
    "# dense = Dense(64,activation='selu',kernel_initializer='lecun_normal')(bn)\n",
    "# do = Dropout(0.4)(dense)\n",
    "# bn = BatchNormalization()(do)\n",
    "# bn = Concatenate()([bn,bn2])\n",
    "# dense = Dense(64,activation='selu',kernel_initializer='lecun_normal')(bn)\n",
    "# do = Dropout(0.5)(dense)\n",
    "# bn = BatchNormalization()(do)\n",
    "output = Dense(1,activation='sigmoid')(do)\n",
    "\n",
    "model = Model(inputs,output)\n",
    "# model = multi_gpu_model(model,gpus=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 237\n"
     ]
    }
   ],
   "source": [
    "opt=Adam() #Nadam()\n",
    "batchSize=2048 #32\n",
    "patien=25\n",
    "epoch=1000\n",
    "# hidden_dims=128\n",
    "# io_dim=128\n",
    "# input_lengths=train_X.shape[1] #profile_Q3\n",
    "# output_lengths= train_Y2.shape[1]#rep_max size\n",
    "# depths=1\n",
    "# dp = 0.01\n",
    "saveP = 'model/Reg_keras2.h5' #1: 00010: val_mean_absolute_error improved from 0.10455 to 0.09934, saving model to model/Reg_keras.h5\n",
    "logD = './model/logs/'\n",
    "history = History()\n",
    "print(\"input:\",train_X.shape[1])#,'output_length:',train_Y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44939 samples, validate on 4994 samples\n",
      "Epoch 1/1000\n",
      "44939/44939 [==============================] - 96s 2ms/step - loss: 0.1989 - mean_absolute_error: 0.3391 - val_loss: 0.1343 - val_mean_absolute_error: 0.1827\n",
      "\n",
      "Epoch 00001: val_mean_absolute_error improved from inf to 0.18269, saving model to model/Reg_keras2.h5\n",
      "Epoch 2/1000\n",
      "44939/44939 [==============================] - 81s 2ms/step - loss: 0.0768 - mean_absolute_error: 0.1461 - val_loss: 0.0853 - val_mean_absolute_error: 0.1331\n",
      "\n",
      "Epoch 00002: val_mean_absolute_error improved from 0.18269 to 0.13308, saving model to model/Reg_keras2.h5\n",
      "Epoch 3/1000\n",
      "44939/44939 [==============================] - 80s 2ms/step - loss: 0.0493 - mean_absolute_error: 0.1031 - val_loss: 0.0801 - val_mean_absolute_error: 0.1221\n",
      "\n",
      "Epoch 00003: val_mean_absolute_error improved from 0.13308 to 0.12208, saving model to model/Reg_keras2.h5\n",
      "Epoch 4/1000\n",
      "44939/44939 [==============================] - 79s 2ms/step - loss: 0.0363 - mean_absolute_error: 0.0805 - val_loss: 0.0713 - val_mean_absolute_error: 0.1096\n",
      "\n",
      "Epoch 00004: val_mean_absolute_error improved from 0.12208 to 0.10965, saving model to model/Reg_keras2.h5\n",
      "Epoch 5/1000\n",
      "44939/44939 [==============================] - 80s 2ms/step - loss: 0.0285 - mean_absolute_error: 0.0660 - val_loss: 0.0678 - val_mean_absolute_error: 0.1061\n",
      "\n",
      "Epoch 00005: val_mean_absolute_error improved from 0.10965 to 0.10605, saving model to model/Reg_keras2.h5\n",
      "Epoch 6/1000\n",
      "44939/44939 [==============================] - 80s 2ms/step - loss: 0.0240 - mean_absolute_error: 0.0577 - val_loss: 0.0656 - val_mean_absolute_error: 0.1033\n",
      "\n",
      "Epoch 00006: val_mean_absolute_error improved from 0.10605 to 0.10330, saving model to model/Reg_keras2.h5\n",
      "Epoch 7/1000\n",
      "44939/44939 [==============================] - 81s 2ms/step - loss: 0.0205 - mean_absolute_error: 0.0523 - val_loss: 0.0668 - val_mean_absolute_error: 0.1030\n",
      "\n",
      "Epoch 00007: val_mean_absolute_error improved from 0.10330 to 0.10297, saving model to model/Reg_keras2.h5\n",
      "Epoch 8/1000\n",
      "44939/44939 [==============================] - 80s 2ms/step - loss: 0.0161 - mean_absolute_error: 0.0447 - val_loss: 0.0673 - val_mean_absolute_error: 0.1023\n",
      "\n",
      "Epoch 00008: val_mean_absolute_error improved from 0.10297 to 0.10229, saving model to model/Reg_keras2.h5\n",
      "Epoch 9/1000\n",
      "44939/44939 [==============================] - 81s 2ms/step - loss: 0.0137 - mean_absolute_error: 0.0395 - val_loss: 0.0673 - val_mean_absolute_error: 0.1020\n",
      "\n",
      "Epoch 00009: val_mean_absolute_error improved from 0.10229 to 0.10196, saving model to model/Reg_keras2.h5\n",
      "Epoch 10/1000\n",
      "44939/44939 [==============================] - 81s 2ms/step - loss: 0.0123 - mean_absolute_error: 0.0370 - val_loss: 0.0684 - val_mean_absolute_error: 0.1026\n",
      "\n",
      "Epoch 00010: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 11/1000\n",
      "44939/44939 [==============================] - 81s 2ms/step - loss: 0.0105 - mean_absolute_error: 0.0338 - val_loss: 0.0694 - val_mean_absolute_error: 0.1029\n",
      "\n",
      "Epoch 00011: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 12/1000\n",
      "44939/44939 [==============================] - 80s 2ms/step - loss: 0.0089 - mean_absolute_error: 0.0306 - val_loss: 0.0700 - val_mean_absolute_error: 0.1023\n",
      "\n",
      "Epoch 00012: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 13/1000\n",
      "44939/44939 [==============================] - 81s 2ms/step - loss: 0.0081 - mean_absolute_error: 0.0284 - val_loss: 0.0706 - val_mean_absolute_error: 0.1039\n",
      "\n",
      "Epoch 00013: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 14/1000\n",
      "44939/44939 [==============================] - 81s 2ms/step - loss: 0.0071 - mean_absolute_error: 0.0267 - val_loss: 0.0715 - val_mean_absolute_error: 0.1036\n",
      "\n",
      "Epoch 00014: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 15/1000\n",
      "44939/44939 [==============================] - 73s 2ms/step - loss: 0.0064 - mean_absolute_error: 0.0250 - val_loss: 0.0716 - val_mean_absolute_error: 0.1042\n",
      "\n",
      "Epoch 00015: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 16/1000\n",
      "44939/44939 [==============================] - 64s 1ms/step - loss: 0.0057 - mean_absolute_error: 0.0233 - val_loss: 0.0716 - val_mean_absolute_error: 0.1038\n",
      "\n",
      "Epoch 00016: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 17/1000\n",
      "44939/44939 [==============================] - 64s 1ms/step - loss: 0.0052 - mean_absolute_error: 0.0221 - val_loss: 0.0741 - val_mean_absolute_error: 0.1044\n",
      "\n",
      "Epoch 00017: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 18/1000\n",
      "44939/44939 [==============================] - 64s 1ms/step - loss: 0.0050 - mean_absolute_error: 0.0218 - val_loss: 0.0735 - val_mean_absolute_error: 0.1051\n",
      "\n",
      "Epoch 00018: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 19/1000\n",
      "44939/44939 [==============================] - 64s 1ms/step - loss: 0.0048 - mean_absolute_error: 0.0217 - val_loss: 0.0736 - val_mean_absolute_error: 0.1044\n",
      "\n",
      "Epoch 00019: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 20/1000\n",
      "44939/44939 [==============================] - 63s 1ms/step - loss: 0.0042 - mean_absolute_error: 0.0202 - val_loss: 0.0741 - val_mean_absolute_error: 0.1049\n",
      "\n",
      "Epoch 00020: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 21/1000\n",
      "44939/44939 [==============================] - 63s 1ms/step - loss: 0.0041 - mean_absolute_error: 0.0201 - val_loss: 0.0739 - val_mean_absolute_error: 0.1052\n",
      "\n",
      "Epoch 00021: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 22/1000\n",
      "44939/44939 [==============================] - 63s 1ms/step - loss: 0.0039 - mean_absolute_error: 0.0195 - val_loss: 0.0724 - val_mean_absolute_error: 0.1034\n",
      "\n",
      "Epoch 00022: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 23/1000\n",
      "44939/44939 [==============================] - 64s 1ms/step - loss: 0.0036 - mean_absolute_error: 0.0188 - val_loss: 0.0724 - val_mean_absolute_error: 0.1034\n",
      "\n",
      "Epoch 00023: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 24/1000\n",
      "44939/44939 [==============================] - 63s 1ms/step - loss: 0.0036 - mean_absolute_error: 0.0186 - val_loss: 0.0725 - val_mean_absolute_error: 0.1033\n",
      "\n",
      "Epoch 00024: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 25/1000\n",
      "44939/44939 [==============================] - 63s 1ms/step - loss: 0.0032 - mean_absolute_error: 0.0179 - val_loss: 0.0717 - val_mean_absolute_error: 0.1031\n",
      "\n",
      "Epoch 00025: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 26/1000\n",
      "44939/44939 [==============================] - 64s 1ms/step - loss: 0.0030 - mean_absolute_error: 0.0172 - val_loss: 0.0714 - val_mean_absolute_error: 0.1031\n",
      "\n",
      "Epoch 00026: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 27/1000\n",
      "44939/44939 [==============================] - 63s 1ms/step - loss: 0.0031 - mean_absolute_error: 0.0175 - val_loss: 0.0720 - val_mean_absolute_error: 0.1045\n",
      "\n",
      "Epoch 00027: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 28/1000\n",
      "44939/44939 [==============================] - 63s 1ms/step - loss: 0.0027 - mean_absolute_error: 0.0167 - val_loss: 0.0717 - val_mean_absolute_error: 0.1037\n",
      "\n",
      "Epoch 00028: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 29/1000\n",
      "44939/44939 [==============================] - 65s 1ms/step - loss: 0.0028 - mean_absolute_error: 0.0166 - val_loss: 0.0732 - val_mean_absolute_error: 0.1054\n",
      "\n",
      "Epoch 00029: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 30/1000\n",
      "44939/44939 [==============================] - 63s 1ms/step - loss: 0.0025 - mean_absolute_error: 0.0159 - val_loss: 0.0742 - val_mean_absolute_error: 0.1055\n",
      "\n",
      "Epoch 00030: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 31/1000\n",
      "44939/44939 [==============================] - 63s 1ms/step - loss: 0.0024 - mean_absolute_error: 0.0156 - val_loss: 0.0738 - val_mean_absolute_error: 0.1054\n",
      "\n",
      "Epoch 00031: val_mean_absolute_error did not improve from 0.10196\n",
      "Epoch 00031: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "callback=[\n",
    "    ReduceLROnPlateau(monitor='loss', factor=0.5, patience=int(patien/3),min_lr=1e-6,mode='min' ),\n",
    "    EarlyStopping(patience=patien,monitor='val_loss',verbose=1),\n",
    "    ModelCheckpoint(saveP,monitor='val_mean_absolute_error',verbose=1,save_best_only=True, save_weights_only=True),\n",
    "    TensorBoard(log_dir=logD), \n",
    "    history,\n",
    "]\n",
    "model.fit(train_X, train_Y,\n",
    "                epochs=epoch,\n",
    "                batch_size=batchSize,\n",
    "                shuffle=True,\n",
    "                validation_data=(valid_X, valid_Y),\n",
    "                callbacks=callback, \n",
    "#                 class_weight='auto'\n",
    "                )\n",
    "model.save(saveP+\"_all.h5\") #184sec/0.04/0.19\n",
    "# 127s 3ms/step - loss: 0.0072 - mean_absolute_error: 0.0380 - val_loss: 0.0920 - val_mean_absolute_error: 0.1414"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd time\n",
    "* fix back lyers\n",
    "* lower lr\n",
    "* lower patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 237\n"
     ]
    }
   ],
   "source": [
    "opt=Nadam(lr=0.0005) \n",
    "batchSize=512\n",
    "patien=15\n",
    "epoch=100\n",
    "# hidden_dims=128\n",
    "# io_dim=128\n",
    "# input_lengths=train_X.shape[1] #profile_Q3\n",
    "# output_lengths= train_Y2.shape[1]#rep_max size\n",
    "# depths=1\n",
    "# dp = 0.01\n",
    "saveP = 'model/Reg_keras2.h5' #2: val_mean_absolute_error did not improve from 0.10362\n",
    "logD = './model/logs/'\n",
    "history = History()\n",
    "print(\"input:\",train_X.shape[1])#,'output_length:',train_Y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44939 samples, validate on 4994 samples\n",
      "Epoch 1/100\n",
      "44939/44939 [==============================] - 303s 7ms/step - loss: 0.0175 - mean_absolute_error: 0.0462 - val_loss: 0.0694 - val_mean_absolute_error: 0.1047\n",
      "\n",
      "Epoch 00001: val_mean_absolute_error improved from inf to 0.10473, saving model to model/Reg_keras2.h5\n",
      "Epoch 2/100\n",
      "44939/44939 [==============================] - 291s 6ms/step - loss: 0.0153 - mean_absolute_error: 0.0428 - val_loss: 0.0735 - val_mean_absolute_error: 0.1073\n",
      "\n",
      "Epoch 00002: val_mean_absolute_error did not improve from 0.10473\n",
      "Epoch 3/100\n",
      "44939/44939 [==============================] - 373s 8ms/step - loss: 0.0137 - mean_absolute_error: 0.0400 - val_loss: 0.0704 - val_mean_absolute_error: 0.1056\n",
      "\n",
      "Epoch 00003: val_mean_absolute_error did not improve from 0.10473\n",
      "Epoch 4/100\n",
      "44939/44939 [==============================] - 296s 7ms/step - loss: 0.0111 - mean_absolute_error: 0.0352 - val_loss: 0.0704 - val_mean_absolute_error: 0.1063\n",
      "\n",
      "Epoch 00004: val_mean_absolute_error did not improve from 0.10473\n",
      "Epoch 5/100\n",
      "44939/44939 [==============================] - 291s 6ms/step - loss: 0.0091 - mean_absolute_error: 0.0315 - val_loss: 0.0696 - val_mean_absolute_error: 0.1038\n",
      "\n",
      "Epoch 00005: val_mean_absolute_error improved from 0.10473 to 0.10382, saving model to model/Reg_keras2.h5\n",
      "Epoch 6/100\n",
      "44939/44939 [==============================] - 320s 7ms/step - loss: 0.0071 - mean_absolute_error: 0.0279 - val_loss: 0.0692 - val_mean_absolute_error: 0.1038\n",
      "\n",
      "Epoch 00006: val_mean_absolute_error improved from 0.10382 to 0.10376, saving model to model/Reg_keras2.h5\n",
      "Epoch 7/100\n",
      "44939/44939 [==============================] - 317s 7ms/step - loss: 0.0061 - mean_absolute_error: 0.0258 - val_loss: 0.0707 - val_mean_absolute_error: 0.1047\n",
      "\n",
      "Epoch 00007: val_mean_absolute_error did not improve from 0.10376\n",
      "Epoch 8/100\n",
      "44939/44939 [==============================] - 321s 7ms/step - loss: 0.0059 - mean_absolute_error: 0.0252 - val_loss: 0.0714 - val_mean_absolute_error: 0.1058\n",
      "\n",
      "Epoch 00008: val_mean_absolute_error did not improve from 0.10376\n",
      "Epoch 9/100\n",
      "44939/44939 [==============================] - 323s 7ms/step - loss: 0.0054 - mean_absolute_error: 0.0242 - val_loss: 0.0715 - val_mean_absolute_error: 0.1061\n",
      "\n",
      "Epoch 00009: val_mean_absolute_error did not improve from 0.10376\n",
      "Epoch 10/100\n",
      "44939/44939 [==============================] - 318s 7ms/step - loss: 0.0050 - mean_absolute_error: 0.0236 - val_loss: 0.0723 - val_mean_absolute_error: 0.1071\n",
      "\n",
      "Epoch 00010: val_mean_absolute_error did not improve from 0.10376\n",
      "Epoch 11/100\n",
      "44939/44939 [==============================] - 319s 7ms/step - loss: 0.0047 - mean_absolute_error: 0.0227 - val_loss: 0.0728 - val_mean_absolute_error: 0.1067\n",
      "\n",
      "Epoch 00011: val_mean_absolute_error did not improve from 0.10376\n",
      "Epoch 12/100\n",
      "44939/44939 [==============================] - 320s 7ms/step - loss: 0.0042 - mean_absolute_error: 0.0215 - val_loss: 0.0703 - val_mean_absolute_error: 0.1040\n",
      "\n",
      "Epoch 00012: val_mean_absolute_error did not improve from 0.10376\n",
      "Epoch 13/100\n",
      "44939/44939 [==============================] - 324s 7ms/step - loss: 0.0036 - mean_absolute_error: 0.0204 - val_loss: 0.0725 - val_mean_absolute_error: 0.1068\n",
      "\n",
      "Epoch 00013: val_mean_absolute_error did not improve from 0.10376\n",
      "Epoch 14/100\n",
      "44939/44939 [==============================] - 291s 6ms/step - loss: 0.0035 - mean_absolute_error: 0.0201 - val_loss: 0.0720 - val_mean_absolute_error: 0.1062\n",
      "\n",
      "Epoch 00014: val_mean_absolute_error did not improve from 0.10376\n",
      "Epoch 15/100\n",
      "44939/44939 [==============================] - 293s 7ms/step - loss: 0.0034 - mean_absolute_error: 0.0196 - val_loss: 0.0700 - val_mean_absolute_error: 0.1053\n",
      "\n",
      "Epoch 00015: val_mean_absolute_error did not improve from 0.10376\n",
      "Epoch 16/100\n",
      "44939/44939 [==============================] - 292s 6ms/step - loss: 0.0031 - mean_absolute_error: 0.0188 - val_loss: 0.0699 - val_mean_absolute_error: 0.1048\n",
      "\n",
      "Epoch 00016: val_mean_absolute_error did not improve from 0.10376\n",
      "Epoch 17/100\n",
      "44939/44939 [==============================] - 292s 7ms/step - loss: 0.0028 - mean_absolute_error: 0.0184 - val_loss: 0.0698 - val_mean_absolute_error: 0.1047\n",
      "\n",
      "Epoch 00017: val_mean_absolute_error did not improve from 0.10376\n",
      "Epoch 18/100\n",
      "44939/44939 [==============================] - 295s 7ms/step - loss: 0.0023 - mean_absolute_error: 0.0167 - val_loss: 0.0698 - val_mean_absolute_error: 0.1040\n",
      "\n",
      "Epoch 00018: val_mean_absolute_error did not improve from 0.10376\n",
      "Epoch 19/100\n",
      "44939/44939 [==============================] - 293s 7ms/step - loss: 0.0021 - mean_absolute_error: 0.0162 - val_loss: 0.0694 - val_mean_absolute_error: 0.1042\n",
      "\n",
      "Epoch 00019: val_mean_absolute_error did not improve from 0.10376\n",
      "Epoch 20/100\n",
      "44939/44939 [==============================] - 292s 6ms/step - loss: 0.0020 - mean_absolute_error: 0.0158 - val_loss: 0.0687 - val_mean_absolute_error: 0.1038\n",
      "\n",
      "Epoch 00020: val_mean_absolute_error improved from 0.10376 to 0.10376, saving model to model/Reg_keras2.h5\n",
      "Epoch 21/100\n",
      "44939/44939 [==============================] - 294s 7ms/step - loss: 0.0021 - mean_absolute_error: 0.0159 - val_loss: 0.0699 - val_mean_absolute_error: 0.1045\n",
      "\n",
      "Epoch 00021: val_mean_absolute_error did not improve from 0.10376\n",
      "Epoch 22/100\n",
      "44939/44939 [==============================] - 294s 7ms/step - loss: 0.0020 - mean_absolute_error: 0.0158 - val_loss: 0.0690 - val_mean_absolute_error: 0.1036\n",
      "\n",
      "Epoch 00022: val_mean_absolute_error improved from 0.10376 to 0.10358, saving model to model/Reg_keras2.h5\n",
      "Epoch 23/100\n",
      "44939/44939 [==============================] - 292s 7ms/step - loss: 0.0020 - mean_absolute_error: 0.0158 - val_loss: 0.0686 - val_mean_absolute_error: 0.1037\n",
      "\n",
      "Epoch 00023: val_mean_absolute_error did not improve from 0.10358\n",
      "Epoch 24/100\n",
      "44939/44939 [==============================] - 292s 7ms/step - loss: 0.0018 - mean_absolute_error: 0.0154 - val_loss: 0.0686 - val_mean_absolute_error: 0.1033\n",
      "\n",
      "Epoch 00024: val_mean_absolute_error improved from 0.10358 to 0.10334, saving model to model/Reg_keras2.h5\n",
      "Epoch 25/100\n",
      "44939/44939 [==============================] - 294s 7ms/step - loss: 0.0018 - mean_absolute_error: 0.0153 - val_loss: 0.0687 - val_mean_absolute_error: 0.1034\n",
      "\n",
      "Epoch 00025: val_mean_absolute_error did not improve from 0.10334\n",
      "Epoch 26/100\n",
      "44939/44939 [==============================] - 293s 7ms/step - loss: 0.0018 - mean_absolute_error: 0.0152 - val_loss: 0.0695 - val_mean_absolute_error: 0.1047\n",
      "\n",
      "Epoch 00026: val_mean_absolute_error did not improve from 0.10334\n",
      "Epoch 27/100\n",
      "44939/44939 [==============================] - 292s 7ms/step - loss: 0.0018 - mean_absolute_error: 0.0150 - val_loss: 0.0686 - val_mean_absolute_error: 0.1032\n",
      "\n",
      "Epoch 00027: val_mean_absolute_error improved from 0.10334 to 0.10325, saving model to model/Reg_keras2.h5\n",
      "Epoch 28/100\n",
      "44939/44939 [==============================] - 292s 7ms/step - loss: 0.0017 - mean_absolute_error: 0.0149 - val_loss: 0.0684 - val_mean_absolute_error: 0.1033\n",
      "\n",
      "Epoch 00028: val_mean_absolute_error did not improve from 0.10325\n",
      "Epoch 29/100\n",
      "44939/44939 [==============================] - 293s 7ms/step - loss: 0.0016 - mean_absolute_error: 0.0144 - val_loss: 0.0678 - val_mean_absolute_error: 0.1024\n",
      "\n",
      "Epoch 00029: val_mean_absolute_error improved from 0.10325 to 0.10238, saving model to model/Reg_keras2.h5\n",
      "Epoch 30/100\n",
      "44939/44939 [==============================] - 293s 7ms/step - loss: 0.0015 - mean_absolute_error: 0.0143 - val_loss: 0.0678 - val_mean_absolute_error: 0.1029\n",
      "\n",
      "Epoch 00030: val_mean_absolute_error did not improve from 0.10238\n",
      "Epoch 31/100\n",
      "44939/44939 [==============================] - 292s 7ms/step - loss: 0.0016 - mean_absolute_error: 0.0144 - val_loss: 0.0686 - val_mean_absolute_error: 0.1040\n",
      "\n",
      "Epoch 00031: val_mean_absolute_error did not improve from 0.10238\n",
      "Epoch 32/100\n",
      "44939/44939 [==============================] - 294s 7ms/step - loss: 0.0015 - mean_absolute_error: 0.0140 - val_loss: 0.0683 - val_mean_absolute_error: 0.1030\n",
      "\n",
      "Epoch 00032: val_mean_absolute_error did not improve from 0.10238\n",
      "Epoch 33/100\n",
      "44939/44939 [==============================] - 292s 6ms/step - loss: 0.0014 - mean_absolute_error: 0.0137 - val_loss: 0.0681 - val_mean_absolute_error: 0.1026\n",
      "\n",
      "Epoch 00033: val_mean_absolute_error did not improve from 0.10238\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44939/44939 [==============================] - 292s 7ms/step - loss: 0.0017 - mean_absolute_error: 0.0146 - val_loss: 0.0672 - val_mean_absolute_error: 0.1030\n",
      "\n",
      "Epoch 00034: val_mean_absolute_error did not improve from 0.10238\n",
      "Epoch 35/100\n",
      "44939/44939 [==============================] - 295s 7ms/step - loss: 0.0014 - mean_absolute_error: 0.0137 - val_loss: 0.0672 - val_mean_absolute_error: 0.1020\n",
      "\n",
      "Epoch 00035: val_mean_absolute_error improved from 0.10238 to 0.10202, saving model to model/Reg_keras2.h5\n",
      "Epoch 36/100\n",
      "44939/44939 [==============================] - 293s 7ms/step - loss: 0.0014 - mean_absolute_error: 0.0137 - val_loss: 0.0681 - val_mean_absolute_error: 0.1033\n",
      "\n",
      "Epoch 00036: val_mean_absolute_error did not improve from 0.10202\n",
      "Epoch 37/100\n",
      "44939/44939 [==============================] - 296s 7ms/step - loss: 0.0014 - mean_absolute_error: 0.0135 - val_loss: 0.0683 - val_mean_absolute_error: 0.1041\n",
      "\n",
      "Epoch 00037: val_mean_absolute_error did not improve from 0.10202\n",
      "Epoch 38/100\n",
      "44939/44939 [==============================] - 294s 7ms/step - loss: 0.0013 - mean_absolute_error: 0.0130 - val_loss: 0.0680 - val_mean_absolute_error: 0.1027\n",
      "\n",
      "Epoch 00038: val_mean_absolute_error did not improve from 0.10202\n",
      "Epoch 39/100\n",
      "44939/44939 [==============================] - 293s 7ms/step - loss: 0.0013 - mean_absolute_error: 0.0131 - val_loss: 0.0675 - val_mean_absolute_error: 0.1030\n",
      "\n",
      "Epoch 00039: val_mean_absolute_error did not improve from 0.10202\n",
      "Epoch 40/100\n",
      "44939/44939 [==============================] - 292s 6ms/step - loss: 0.0013 - mean_absolute_error: 0.0133 - val_loss: 0.0684 - val_mean_absolute_error: 0.1039\n",
      "\n",
      "Epoch 00040: val_mean_absolute_error did not improve from 0.10202\n",
      "Epoch 41/100\n",
      "44939/44939 [==============================] - 294s 7ms/step - loss: 0.0012 - mean_absolute_error: 0.0129 - val_loss: 0.0682 - val_mean_absolute_error: 0.1035\n",
      "\n",
      "Epoch 00041: val_mean_absolute_error did not improve from 0.10202\n",
      "Epoch 42/100\n",
      "44939/44939 [==============================] - 294s 7ms/step - loss: 0.0012 - mean_absolute_error: 0.0125 - val_loss: 0.0689 - val_mean_absolute_error: 0.1039\n",
      "\n",
      "Epoch 00042: val_mean_absolute_error did not improve from 0.10202\n",
      "Epoch 43/100\n",
      "44939/44939 [==============================] - 292s 7ms/step - loss: 0.0012 - mean_absolute_error: 0.0126 - val_loss: 0.0682 - val_mean_absolute_error: 0.1033\n",
      "\n",
      "Epoch 00043: val_mean_absolute_error did not improve from 0.10202\n",
      "Epoch 44/100\n",
      "44939/44939 [==============================] - 291s 6ms/step - loss: 0.0013 - mean_absolute_error: 0.0129 - val_loss: 0.0688 - val_mean_absolute_error: 0.1037\n",
      "\n",
      "Epoch 00044: val_mean_absolute_error did not improve from 0.10202\n",
      "Epoch 45/100\n",
      "44939/44939 [==============================] - 294s 7ms/step - loss: 0.0011 - mean_absolute_error: 0.0121 - val_loss: 0.0689 - val_mean_absolute_error: 0.1036\n",
      "\n",
      "Epoch 00045: val_mean_absolute_error did not improve from 0.10202\n",
      "Epoch 46/100\n",
      "44939/44939 [==============================] - 294s 7ms/step - loss: 9.8844e-04 - mean_absolute_error: 0.0116 - val_loss: 0.0689 - val_mean_absolute_error: 0.1037\n",
      "\n",
      "Epoch 00046: val_mean_absolute_error did not improve from 0.10202\n",
      "Epoch 47/100\n",
      "44939/44939 [==============================] - 295s 7ms/step - loss: 9.5162e-04 - mean_absolute_error: 0.0114 - val_loss: 0.0689 - val_mean_absolute_error: 0.1037\n",
      "\n",
      "Epoch 00047: val_mean_absolute_error did not improve from 0.10202\n",
      "Epoch 48/100\n",
      "44939/44939 [==============================] - 294s 7ms/step - loss: 8.7143e-04 - mean_absolute_error: 0.0110 - val_loss: 0.0688 - val_mean_absolute_error: 0.1037\n",
      "\n",
      "Epoch 00048: val_mean_absolute_error did not improve from 0.10202\n",
      "Epoch 49/100\n",
      "44939/44939 [==============================] - 293s 7ms/step - loss: 8.7981e-04 - mean_absolute_error: 0.0111 - val_loss: 0.0689 - val_mean_absolute_error: 0.1034\n",
      "\n",
      "Epoch 00049: val_mean_absolute_error did not improve from 0.10202\n",
      "Epoch 00049: early stopping\n"
     ]
    }
   ],
   "source": [
    "# model = load_model(saveP+\"_all.h5\")\n",
    "model.load_weights(saveP)\n",
    "model = multi_gpu_model(model,gpus=2)\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "\n",
    "callback=[\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=int(patien/1.5),min_lr=1e-6,mode='min' ),\n",
    "    EarlyStopping(patience=patien,monitor='val_loss',verbose=1),\n",
    "    ModelCheckpoint(saveP,monitor='val_mean_absolute_error',verbose=1,save_best_only=True, save_weights_only=True),\n",
    "    TensorBoard(log_dir=logD), \n",
    "    history,\n",
    "]\n",
    "model.fit(train_X, train_Y,\n",
    "                epochs=epoch,\n",
    "                batch_size=batchSize,\n",
    "                shuffle=True,\n",
    "                validation_data=(valid_X, valid_Y),\n",
    "                callbacks=callback, \n",
    "                class_weight='auto'\n",
    "                )\n",
    "model.save(saveP+\"_all.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4994/4994 [==============================] - 210s 42ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06722560542694177, 0.10202433797875833]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = load_model('model/Reg_keras2.h5_all.h5')\n",
    "model2.load_weights('model/Reg_keras2.h5')\n",
    "model2.evaluate(valid_X,valid_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "33610/33610 [==============================] - 81s 2ms/step - loss: 0.1581\n",
      "Epoch 2/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.1028\n",
      "Epoch 3/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0908\n",
      "Epoch 4/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0814\n",
      "Epoch 5/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0746\n",
      "Epoch 6/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0671\n",
      "Epoch 7/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0606\n",
      "Epoch 8/50\n",
      "33610/33610 [==============================] - 80s 2ms/step - loss: 0.0550\n",
      "Epoch 9/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0507\n",
      "Epoch 10/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0466\n",
      "Epoch 11/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0433\n",
      "Epoch 12/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0405\n",
      "Epoch 13/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0385\n",
      "Epoch 14/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0366\n",
      "Epoch 15/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0349\n",
      "Epoch 16/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0334\n",
      "Epoch 17/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0318\n",
      "Epoch 18/50\n",
      "33610/33610 [==============================] - 80s 2ms/step - loss: 0.0304\n",
      "Epoch 19/50\n",
      "33610/33610 [==============================] - 78s 2ms/step - loss: 0.0293\n",
      "Epoch 20/50\n",
      "33610/33610 [==============================] - 80s 2ms/step - loss: 0.0278\n",
      "Epoch 21/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0271\n",
      "Epoch 22/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0258\n",
      "Epoch 23/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0248\n",
      "Epoch 24/50\n",
      "33610/33610 [==============================] - 80s 2ms/step - loss: 0.0238\n",
      "Epoch 25/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0229\n",
      "Epoch 26/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0221\n",
      "Epoch 27/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0211\n",
      "Epoch 28/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0204\n",
      "Epoch 29/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0200\n",
      "Epoch 30/50\n",
      "33610/33610 [==============================] - 78s 2ms/step - loss: 0.0187\n",
      "Epoch 31/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0181\n",
      "Epoch 32/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0174\n",
      "Epoch 33/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0167\n",
      "Epoch 34/50\n",
      "33610/33610 [==============================] - 80s 2ms/step - loss: 0.0159\n",
      "Epoch 35/50\n",
      "33610/33610 [==============================] - 78s 2ms/step - loss: 0.0155\n",
      "Epoch 36/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0147\n",
      "Epoch 37/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0143\n",
      "Epoch 38/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0136\n",
      "Epoch 39/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0132\n",
      "Epoch 40/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0127\n",
      "Epoch 41/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0120\n",
      "Epoch 42/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0117\n",
      "Epoch 43/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0112\n",
      "Epoch 44/50\n",
      "33610/33610 [==============================] - 80s 2ms/step - loss: 0.0105\n",
      "Epoch 45/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0106\n",
      "Epoch 46/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0098\n",
      "Epoch 47/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0097\n",
      "Epoch 48/50\n",
      "33610/33610 [==============================] - 80s 2ms/step - loss: 0.0093\n",
      "Epoch 49/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0088\n",
      "Epoch 50/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72c45dac18>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "model.fit(train_X,train_Y,epochs=50,batch_size=512) #GRU-BI，361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "49933/49933 [==============================] - 1237s 25ms/step - loss: 0.0943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72c4c6eb00>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "model.fit(train_X,train_Y) #GRU-BI，237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "33610/33610 [==============================] - 3810s 113ms/step - loss: 0.1102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72c85607b8>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "model.fit(train_X,train_Y) #LSTM-BI、885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "49933/49933 [==============================] - 1541s 31ms/step - loss: 0.0878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72c49be2e8>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "model.fit(train_X,train_Y) #LSTM-BI、237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "49933/49933 [==============================] - 80s 2ms/step - loss: 0.1553\n",
      "Epoch 2/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0988\n",
      "Epoch 3/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0762\n",
      "Epoch 4/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0539\n",
      "Epoch 5/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0459\n",
      "Epoch 6/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0351\n",
      "Epoch 7/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0291\n",
      "Epoch 8/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0257\n",
      "Epoch 9/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0232\n",
      "Epoch 10/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0208\n",
      "Epoch 11/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0195\n",
      "Epoch 12/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0180\n",
      "Epoch 13/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0170\n",
      "Epoch 14/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0162\n",
      "Epoch 15/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0151\n",
      "Epoch 16/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0146\n",
      "Epoch 17/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0140\n",
      "Epoch 18/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0125\n",
      "Epoch 19/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0121\n",
      "Epoch 20/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0116\n",
      "Epoch 21/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0108\n",
      "Epoch 22/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0105\n",
      "Epoch 23/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0096\n",
      "Epoch 24/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0089\n",
      "Epoch 25/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0089\n",
      "Epoch 26/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0084\n",
      "Epoch 27/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0077\n",
      "Epoch 28/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0071\n",
      "Epoch 29/50\n",
      "49933/49933 [==============================] - 76s 2ms/step - loss: 0.0069\n",
      "Epoch 30/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0064\n",
      "Epoch 31/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0062\n",
      "Epoch 32/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0056\n",
      "Epoch 33/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0057\n",
      "Epoch 34/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0050\n",
      "Epoch 35/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0047\n",
      "Epoch 36/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0044\n",
      "Epoch 37/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0041\n",
      "Epoch 38/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0040\n",
      "Epoch 39/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0038\n",
      "Epoch 40/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0036\n",
      "Epoch 41/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0034\n",
      "Epoch 42/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0035\n",
      "Epoch 43/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0028\n",
      "Epoch 44/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0029\n",
      "Epoch 45/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0028\n",
      "Epoch 46/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0026\n",
      "Epoch 47/50\n",
      "49933/49933 [==============================] - 75s 2ms/step - loss: 0.0024\n",
      "Epoch 48/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0024\n",
      "Epoch 49/50\n",
      "49933/49933 [==============================] - 75s 2ms/step - loss: 0.0024\n",
      "Epoch 50/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72c451f940>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "model.fit(train_X,train_Y,epochs=50,batch_size=512) #LSTM-BI、237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(input_shape=(None, N_features)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
