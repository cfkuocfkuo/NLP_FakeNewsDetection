{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\" \n",
    "import keras\n",
    "import sent2vec\n",
    "import seq2seq\n",
    "from seq2seq.models import AttentionSeq2Seq\n",
    "from seq2seq.models import Seq2Seq\n",
    "from keras.utils import multi_gpu_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from math import log, floor\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import *\n",
    "from keras.utils import *\n",
    "from sklearn.model_selection import *\n",
    "import random\n",
    "from random import shuffle\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from keras.utils.generic_utils import *\n",
    "from keras import regularizers\n",
    "import string\n",
    "import unicodedata as udata\n",
    "import pickle\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "import pause, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import *\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    111023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cypherpunk',\n",
       " 'scowl',\n",
       " 'ilovaisk',\n",
       " 'khafi',\n",
       " 'nayarit',\n",
       " 'dalkey',\n",
       " 'ncaa',\n",
       " 'onu',\n",
       " 'burj',\n",
       " 'shantal',\n",
       " 'leftwich',\n",
       " 'niezal',\n",
       " 'waken',\n",
       " 'clearer',\n",
       " 'mauriupol',\n",
       " 'vormachtstellung',\n",
       " 'irvington',\n",
       " 'segurament',\n",
       " 'mazi',\n",
       " 'lewington',\n",
       " 'backblast',\n",
       " 'stesso',\n",
       " 'escuela',\n",
       " 'kasperski',\n",
       " 'steingart',\n",
       " 'shrew',\n",
       " 'iucfcrsg',\n",
       " 'agrichem',\n",
       " 'julianna',\n",
       " 'longev',\n",
       " 'roywoodjr',\n",
       " 'howarth',\n",
       " 'preshow',\n",
       " 'ruder',\n",
       " 'yoon',\n",
       " 'raatz',\n",
       " 'balibous',\n",
       " 'cisgend',\n",
       " 'jamon',\n",
       " 'day',\n",
       " 'hirsch',\n",
       " 'latham',\n",
       " 'lang',\n",
       " 'wootan',\n",
       " 'praxi',\n",
       " 'bylock',\n",
       " 'sternshowaddict',\n",
       " 'mari',\n",
       " 'apm',\n",
       " 'sohr',\n",
       " 'feticid',\n",
       " 'alimov',\n",
       " 'beigebracht',\n",
       " 'cynergi',\n",
       " 'elzeyadi',\n",
       " 'baccellieri',\n",
       " 'paladio',\n",
       " 'humblebrag',\n",
       " 'brade',\n",
       " 'achil',\n",
       " 'flourish',\n",
       " 'opep',\n",
       " 'ecuatoriano',\n",
       " 'interestingli',\n",
       " 'sanctifi',\n",
       " 'redict',\n",
       " 'genotox',\n",
       " 'monetari',\n",
       " 'asidero',\n",
       " 'twattish',\n",
       " 'horst',\n",
       " 'spackman',\n",
       " 'uresti',\n",
       " 'poliv',\n",
       " 'margaeri',\n",
       " 'conquistar',\n",
       " 'ballcap',\n",
       " 'oteachjohn',\n",
       " 'jostl',\n",
       " 'noncompetit',\n",
       " 'qrator',\n",
       " 'lamarr',\n",
       " 'grazziotin',\n",
       " 'cam',\n",
       " 'felin',\n",
       " 'pilfer',\n",
       " 'hemings',\n",
       " 'maasssaid',\n",
       " 'gesprekken',\n",
       " 'footag',\n",
       " 'machida',\n",
       " 'surest',\n",
       " 'overjoy',\n",
       " 'terrapin',\n",
       " 'trajectori',\n",
       " 'kayle',\n",
       " 'keeneland',\n",
       " 'toss',\n",
       " 'embol',\n",
       " 'pianism',\n",
       " 'mcmansion',\n",
       " 'garantir',\n",
       " 'bioinsecticid',\n",
       " 'savithramma',\n",
       " 'cuatro',\n",
       " 'teamet',\n",
       " 'biegen',\n",
       " 'regul',\n",
       " 'sorprendi',\n",
       " 'landgenoten',\n",
       " 'fabel',\n",
       " 'lcmtlfe',\n",
       " 'bromin',\n",
       " 'shou',\n",
       " 'afyar',\n",
       " 'nonread',\n",
       " 'alberg',\n",
       " 'arrabye',\n",
       " 'empanada',\n",
       " 'ultralow',\n",
       " 'greensboro',\n",
       " 'figura',\n",
       " 'teamax',\n",
       " 'recipi',\n",
       " 'irrev',\n",
       " 'zorn',\n",
       " 'conduir',\n",
       " 'braxton',\n",
       " 'muster',\n",
       " 'yugoslavia',\n",
       " 'volksdeutsch',\n",
       " 'crematori',\n",
       " 'aasif',\n",
       " 'inert',\n",
       " 'yourcenar',\n",
       " 'widvey',\n",
       " 'penguala',\n",
       " 'dont',\n",
       " 'cryptom',\n",
       " 'innkeep',\n",
       " 'mbe',\n",
       " 'lamak',\n",
       " 'discoteca',\n",
       " 'teatim',\n",
       " 'dh',\n",
       " 'coleen',\n",
       " 'maybush',\n",
       " 'itsgoingdown',\n",
       " 'east',\n",
       " 'goodel',\n",
       " 'elid',\n",
       " 'lorelai',\n",
       " 'basada',\n",
       " 'www',\n",
       " 'ploie',\n",
       " 'sqoq',\n",
       " 'hauslohn',\n",
       " 'souffrent',\n",
       " 'phew',\n",
       " 'hurriyah',\n",
       " 'estrategia',\n",
       " 'escalad',\n",
       " 'veon',\n",
       " 'frn',\n",
       " 'mendica',\n",
       " 'verfolgung',\n",
       " 'unii',\n",
       " 'prolaps',\n",
       " 'tensor',\n",
       " 'ofili',\n",
       " 'curioso',\n",
       " 'disperso',\n",
       " 'vandross',\n",
       " 'karimov',\n",
       " 'moviedom',\n",
       " 'maza',\n",
       " 'cozier',\n",
       " 'shaul',\n",
       " 'disconnected',\n",
       " 'snowsho',\n",
       " 'demitiu',\n",
       " 'perrett',\n",
       " 'workman',\n",
       " 'elmar',\n",
       " 'frolic',\n",
       " 'bradstreet',\n",
       " 'reconciliaton',\n",
       " 'cyberintrud',\n",
       " 'azraq',\n",
       " 'letteron',\n",
       " 'loot',\n",
       " 'terrorism',\n",
       " 'srebrenica',\n",
       " 'qaboun',\n",
       " 'anglic',\n",
       " 'brult',\n",
       " 'declin',\n",
       " 'entstandenen',\n",
       " 'druggist',\n",
       " 'hallucin',\n",
       " 'austan',\n",
       " 'bensouda',\n",
       " 'maxx',\n",
       " 'stepanova',\n",
       " 'carapac',\n",
       " 'concussiom',\n",
       " 'louderback',\n",
       " 'spiral',\n",
       " 'mazzaroth',\n",
       " 'madhu',\n",
       " 'chidiac',\n",
       " 'frag',\n",
       " 'ayah',\n",
       " 'sferic',\n",
       " 'cielsa',\n",
       " 'bennet',\n",
       " 'maglion',\n",
       " 'gate',\n",
       " 'fuallofy',\n",
       " 'parlera',\n",
       " 'gevorgyan',\n",
       " 'cremo',\n",
       " 'ensembl',\n",
       " 'semani',\n",
       " 'afr',\n",
       " 'trilog',\n",
       " 'ostlund',\n",
       " 'correlaci',\n",
       " 'petruk',\n",
       " 'starboard',\n",
       " 'rpi',\n",
       " 'felo',\n",
       " 'ckwunsch',\n",
       " 'apuesta',\n",
       " 'alge',\n",
       " 'aispuro',\n",
       " 'alegando',\n",
       " 'mirrle',\n",
       " 'qezjfcweeh',\n",
       " 'rockhound',\n",
       " 'flyth',\n",
       " 'bermen',\n",
       " 'egemenli',\n",
       " 'armpit',\n",
       " 'ruah',\n",
       " 'femen',\n",
       " 'bekleniyor',\n",
       " 'odish',\n",
       " 'recraft',\n",
       " 'truell',\n",
       " 'arnais',\n",
       " 'analyz',\n",
       " 'ook',\n",
       " 'occ',\n",
       " 'appli',\n",
       " 'egot',\n",
       " 'ndono',\n",
       " 'awn',\n",
       " 'flopola',\n",
       " 'koreaha',\n",
       " 'pharoah',\n",
       " 'opinion',\n",
       " 'aridawari',\n",
       " 'ristorant',\n",
       " 'creara',\n",
       " 'mobieltj',\n",
       " 'minefield',\n",
       " 'barnett',\n",
       " 'filipino',\n",
       " 'peninsula',\n",
       " 'prioriti',\n",
       " 'hedaya',\n",
       " 'balaz',\n",
       " 'sanitarium',\n",
       " 'countrysid',\n",
       " 'velopp',\n",
       " 'malusi',\n",
       " 'mislaid',\n",
       " 'command',\n",
       " 'worldwid',\n",
       " 'aqualad',\n",
       " 'glockner',\n",
       " 'stealthi',\n",
       " 'mi',\n",
       " 'waynemadsenreport',\n",
       " 'hilft',\n",
       " 'aprovecharlo',\n",
       " 'hidden',\n",
       " 'infofield',\n",
       " 'mislykt',\n",
       " 'ajeena',\n",
       " 'libi',\n",
       " 'lorri',\n",
       " 'ilrc',\n",
       " 'gertz',\n",
       " 'presentars',\n",
       " 'smart',\n",
       " 'crisper',\n",
       " 'montejo',\n",
       " 'nouri',\n",
       " 'overpack',\n",
       " 'frankensteinish',\n",
       " 'fond',\n",
       " 'cardio',\n",
       " 'vallejo',\n",
       " 'embrey',\n",
       " 'aaj',\n",
       " 'mrcheckpoint',\n",
       " 'grandchildren',\n",
       " 'squirt',\n",
       " 'underactiv',\n",
       " 'trinitrotoluen',\n",
       " 'brimp',\n",
       " 'hurti',\n",
       " 'ghislain',\n",
       " 'comerciai',\n",
       " 'villag',\n",
       " 'shazad',\n",
       " 'panatag',\n",
       " 'deveolp',\n",
       " 'elysium',\n",
       " 'responsibl',\n",
       " 'francotirador',\n",
       " 'godblessamerica',\n",
       " 'vpaym',\n",
       " 'gepland',\n",
       " 'unerkl',\n",
       " 'gounod',\n",
       " 'eyeo',\n",
       " 'spart',\n",
       " 'conselho',\n",
       " 'escamoteado',\n",
       " 'conspiraci',\n",
       " 'trole',\n",
       " 'lewiston',\n",
       " 'taught',\n",
       " 'hoskin',\n",
       " 'buisson',\n",
       " 'katayoon',\n",
       " 'frontag',\n",
       " 'blubutterfli',\n",
       " 'culb',\n",
       " 'hl',\n",
       " 'ramach',\n",
       " 'miro',\n",
       " 'pasars',\n",
       " 'wandira',\n",
       " 'individualis',\n",
       " 'painter',\n",
       " 'macaray',\n",
       " 'identifica',\n",
       " 'handymusik',\n",
       " 'aplaz',\n",
       " 'schlosser',\n",
       " 'schrill',\n",
       " 'useabl',\n",
       " 'faniel',\n",
       " 'rupprecht',\n",
       " 'beaufort',\n",
       " 'frigat',\n",
       " 'establecido',\n",
       " 'je',\n",
       " 'angiogen',\n",
       " 'meechai',\n",
       " 'nasfaa',\n",
       " 'reedley',\n",
       " 'insenst',\n",
       " 'kilgor',\n",
       " 'cleaner',\n",
       " 'petroyuan',\n",
       " 'farmal',\n",
       " 'wolv',\n",
       " 'rummag',\n",
       " 'engordado',\n",
       " 'tudiant',\n",
       " 'taberski',\n",
       " 'nandi',\n",
       " 'valeriya',\n",
       " 'hagglund',\n",
       " 'saud',\n",
       " 'berschritten',\n",
       " 'nake',\n",
       " 'negligencia',\n",
       " 'quotabl',\n",
       " 'tuft',\n",
       " 'ganglia',\n",
       " 'hardwir',\n",
       " 'dojo',\n",
       " 'gewaltigen',\n",
       " 'bootlegg',\n",
       " 'skime',\n",
       " 'fx',\n",
       " 'question',\n",
       " 'ettalhi',\n",
       " 'aground',\n",
       " 'bundesnachrichtendienst',\n",
       " 'papa',\n",
       " 'forbad',\n",
       " 'nonendang',\n",
       " 'adegbil',\n",
       " 'coreligionist',\n",
       " 'tixtla',\n",
       " 'ardley',\n",
       " 'naccio',\n",
       " 'viewfind',\n",
       " 'cscl',\n",
       " 'saleh',\n",
       " 'tuc',\n",
       " 'mpa',\n",
       " 'feral',\n",
       " 'crowder',\n",
       " 'sharmini',\n",
       " 'tampa',\n",
       " 'ronnlund',\n",
       " 'wgrz',\n",
       " 'bankrupt',\n",
       " 'rizzo',\n",
       " 'trt',\n",
       " 'arbeitsf',\n",
       " 'trepper',\n",
       " 'cricket',\n",
       " 'dabney',\n",
       " 'mumbo',\n",
       " 'heteron',\n",
       " 'bispehnol',\n",
       " 'founderchurch',\n",
       " 'shikha',\n",
       " 'vernacular',\n",
       " 'crean',\n",
       " 'yiftah',\n",
       " 'wiata',\n",
       " 'denial',\n",
       " 'valu',\n",
       " 'duncanfyf',\n",
       " 'bekim',\n",
       " 'dawood',\n",
       " 'vaulter',\n",
       " 'colmar',\n",
       " 'juliacraven',\n",
       " 'alchouin',\n",
       " 'comedido',\n",
       " 'prestigio',\n",
       " 'elmsford',\n",
       " 'falkland',\n",
       " 'babysat',\n",
       " 'acquit',\n",
       " 'palpit',\n",
       " 'milley',\n",
       " 'dirigimo',\n",
       " 'lesar',\n",
       " 'oechsli',\n",
       " 'popula',\n",
       " 'willdani',\n",
       " 'hempse',\n",
       " 'dure',\n",
       " 'cra',\n",
       " 'prenderlo',\n",
       " 'bazinga',\n",
       " 'justo',\n",
       " 'silenta',\n",
       " 'tular',\n",
       " 'dubuffet',\n",
       " 'blessed',\n",
       " 'corsi',\n",
       " 'ziggi',\n",
       " 'bahahahaaa',\n",
       " 'wuow',\n",
       " 'lazili',\n",
       " 'historiador',\n",
       " 'havok',\n",
       " 'victimhood',\n",
       " 'troop',\n",
       " 'cessionnism',\n",
       " 'knud',\n",
       " 'kerker',\n",
       " 'pinotti',\n",
       " 'muham',\n",
       " 'barnabi',\n",
       " 'yunnan',\n",
       " 'maior',\n",
       " 'agenzia',\n",
       " 'suppositori',\n",
       " 'hobson',\n",
       " 'desarticulan',\n",
       " 'fizzi',\n",
       " 'rosindel',\n",
       " 'hubieran',\n",
       " 'uro',\n",
       " 'sangreaal',\n",
       " 'turismo',\n",
       " 'cocksur',\n",
       " 'deepli',\n",
       " 'shim',\n",
       " 'lutsenko',\n",
       " 'truism',\n",
       " 'accueil',\n",
       " 'becker',\n",
       " 'ballo',\n",
       " 'enza',\n",
       " 'valk',\n",
       " 'chemtrail',\n",
       " 'burress',\n",
       " 'mannesmann',\n",
       " 'floriss',\n",
       " 'want',\n",
       " 'agp',\n",
       " 'nansaturday',\n",
       " 'preemptiv',\n",
       " 'rmalar',\n",
       " 'rulemak',\n",
       " 'instanti',\n",
       " 'glavin',\n",
       " 'chili',\n",
       " 'ggpzdmgbnw',\n",
       " 'cammyyyedgar',\n",
       " 'imbroglio',\n",
       " 'cinderblock',\n",
       " 'lefkowitz',\n",
       " 'debrief',\n",
       " 'ago',\n",
       " 'boisineau',\n",
       " 'athen',\n",
       " 'boat',\n",
       " 'poof',\n",
       " 'willaim',\n",
       " 'lycopen',\n",
       " 'leert',\n",
       " 'rome',\n",
       " 'redstat',\n",
       " 'regionu',\n",
       " 'itp',\n",
       " 'billclinton',\n",
       " 'polnewsnetwork',\n",
       " 'kilt',\n",
       " 'michaelkeaton',\n",
       " 'doel',\n",
       " 'neocoloni',\n",
       " 'ordenando',\n",
       " 'respeta',\n",
       " 'nuclearwarforh',\n",
       " 'paltrow',\n",
       " 'kiso',\n",
       " 'wlwt',\n",
       " 'saleha',\n",
       " 'fionnuala',\n",
       " 'appelunt',\n",
       " 'bandera',\n",
       " 'novsak',\n",
       " 'lhdzfscaqo',\n",
       " 'kliment',\n",
       " 'tonn',\n",
       " 'amazonian',\n",
       " 'leg',\n",
       " 'tigantourin',\n",
       " 'mamet',\n",
       " 'lunchbox',\n",
       " 'jash',\n",
       " 'standingrocksolidaritynetwork',\n",
       " 'syrah',\n",
       " 'gavino',\n",
       " 'fomalhaut',\n",
       " 'aristotl',\n",
       " 'bellic',\n",
       " 'lenni',\n",
       " 'lunaflora',\n",
       " 'migratoir',\n",
       " 'promulg',\n",
       " 'aprendido',\n",
       " 'piraci',\n",
       " 'forfeitur',\n",
       " 'singwang',\n",
       " 'nandecemb',\n",
       " 'excesivament',\n",
       " 'integrationsinitiativen',\n",
       " 'foxx',\n",
       " 'batist',\n",
       " 'lymphomyosot',\n",
       " 'nanomaha',\n",
       " 'encrust',\n",
       " 'hunchun',\n",
       " 'trampl',\n",
       " 'firstrebutt',\n",
       " 'dataporn',\n",
       " 'brecher',\n",
       " 'katehon',\n",
       " 'agostino',\n",
       " 'triomph',\n",
       " 'gbp',\n",
       " 'batallon',\n",
       " 'dailystar',\n",
       " 'mattvesp',\n",
       " 'uwe',\n",
       " 'coronado',\n",
       " 'rooflin',\n",
       " 'mascara',\n",
       " 'newsgroup',\n",
       " 'infoba',\n",
       " 'mattioli',\n",
       " 'desatada',\n",
       " 'catercorn',\n",
       " 'dthe',\n",
       " 'unlikelihood',\n",
       " 'jeffsess',\n",
       " 'palaszczuk',\n",
       " 'spanier',\n",
       " 'stabilit',\n",
       " 'gimbel',\n",
       " 'insalubri',\n",
       " 'vkontakt',\n",
       " 'veiligheidsfout',\n",
       " 'basicli',\n",
       " 'ruiz',\n",
       " 'guinda',\n",
       " 'timopt',\n",
       " 'majest',\n",
       " 'arborist',\n",
       " 'levanten',\n",
       " 'curso',\n",
       " 'tirmemesi',\n",
       " 'jasika',\n",
       " 'plac',\n",
       " 'logotyp',\n",
       " 'milosz',\n",
       " 'sanwar',\n",
       " 'litiasi',\n",
       " 'sayaghi',\n",
       " 'furthermor',\n",
       " 'stumpf',\n",
       " 'hackensack',\n",
       " 'metralleta',\n",
       " 'mwprinc',\n",
       " 'lewdli',\n",
       " 'heist',\n",
       " 'stoicism',\n",
       " 'bukam',\n",
       " 'congdon',\n",
       " 'fpr',\n",
       " 'formo',\n",
       " 'kamparw',\n",
       " 'isight',\n",
       " 'corea',\n",
       " 'newsmak',\n",
       " 'megastar',\n",
       " 'maneuv',\n",
       " 'extebxtawh',\n",
       " 'niet',\n",
       " 'slavin',\n",
       " 'salom',\n",
       " 'stressor',\n",
       " 'rule',\n",
       " 'magrepha',\n",
       " 'fame',\n",
       " 'hyperborean',\n",
       " 'zamieszkami',\n",
       " 'abul',\n",
       " 'ficarem',\n",
       " 'quelqu',\n",
       " 'withold',\n",
       " 'shandera',\n",
       " 'junkscienc',\n",
       " 'reino',\n",
       " 'xiasha',\n",
       " 'xeon',\n",
       " 'boroughwid',\n",
       " 'friendless',\n",
       " 'ladonna',\n",
       " 'unterlegen',\n",
       " 'keisha',\n",
       " 'sonnet',\n",
       " 'nevin',\n",
       " 'dusha',\n",
       " 'devious',\n",
       " 'miracl',\n",
       " 'gobal',\n",
       " 'bilal',\n",
       " 'kalinin',\n",
       " 'patchi',\n",
       " 'ict',\n",
       " 'karsh',\n",
       " 'andneedlessli',\n",
       " 'complaint',\n",
       " 'arabien',\n",
       " 'jaym',\n",
       " 'incivil',\n",
       " 'parkm',\n",
       " 'halv',\n",
       " 'optimista',\n",
       " 'dbp',\n",
       " 'chanda',\n",
       " 'queermor',\n",
       " 'aduanero',\n",
       " 'cutpic',\n",
       " 'indicada',\n",
       " 'millonaria',\n",
       " 'shame',\n",
       " 'noguchi',\n",
       " 'unscop',\n",
       " 'joscelyn',\n",
       " 'implementaci',\n",
       " 'stanek',\n",
       " 'pendulum',\n",
       " 'ownership',\n",
       " 'kaput',\n",
       " 'stag',\n",
       " 'lbc',\n",
       " 'sjwism',\n",
       " 'pour',\n",
       " 'blackboxvot',\n",
       " 'huidig',\n",
       " 'straddl',\n",
       " 'basketbal',\n",
       " 'kna',\n",
       " 'lucho',\n",
       " 'anabaptist',\n",
       " 'situa',\n",
       " 'bulger',\n",
       " 'dlf',\n",
       " 'crackerjack',\n",
       " 'nonhuman',\n",
       " 'islandia',\n",
       " 'mathi',\n",
       " 'imbalanc',\n",
       " 'biochip',\n",
       " 'french',\n",
       " 'habibati',\n",
       " 'webmast',\n",
       " 'rompen',\n",
       " 'rohe',\n",
       " 'arinaminpathi',\n",
       " 'daumier',\n",
       " 'jonrappoport',\n",
       " 'arcelormitt',\n",
       " 'captia',\n",
       " 'drcarolyndean',\n",
       " 'julian',\n",
       " 'journalistenlaufbahn',\n",
       " 'rideshar',\n",
       " 'semin',\n",
       " 'drafthous',\n",
       " 'herzegovina',\n",
       " 'cabaret',\n",
       " 'littlespottedhors',\n",
       " 'mullah',\n",
       " 'meiosi',\n",
       " 'toulousain',\n",
       " 'nipp',\n",
       " 'shtwipg',\n",
       " 'shutdownloganriv',\n",
       " 'dreising',\n",
       " 'ramesh',\n",
       " 'lipscomb',\n",
       " 'jina',\n",
       " 'schatmeest',\n",
       " 'bamboozl',\n",
       " 'mirahmad',\n",
       " 'sigtarp',\n",
       " 'noteworthi',\n",
       " 'hominida',\n",
       " 'taster',\n",
       " 'gauzier',\n",
       " 'coincidiendo',\n",
       " 'contralto',\n",
       " 'micklethwait',\n",
       " 'destigmat',\n",
       " 'openli',\n",
       " 'coopertown',\n",
       " 'cattolica',\n",
       " 'bdi',\n",
       " 'myron',\n",
       " 'floranc',\n",
       " 'decisiva',\n",
       " 'bliss',\n",
       " 'steamrol',\n",
       " 'ahlburn',\n",
       " 'boxshal',\n",
       " 'identifisert',\n",
       " 'kitten',\n",
       " 'inexhaust',\n",
       " 'rodulfo',\n",
       " 'giacomo',\n",
       " 'shapiro',\n",
       " 'adoptara',\n",
       " 'arbi',\n",
       " 'cessna',\n",
       " 'pomilda',\n",
       " 'nonbind',\n",
       " 'admiss',\n",
       " 'cdp',\n",
       " 'lograrlo',\n",
       " 'bayoakomolaf',\n",
       " 'nichtssagend',\n",
       " 'zaterdag',\n",
       " 'idolat',\n",
       " 'bohn',\n",
       " 'sternupd',\n",
       " 'niigata',\n",
       " 'biochem',\n",
       " 'skye',\n",
       " 'neunzigerjahr',\n",
       " 'tahvil',\n",
       " 'narkosemittel',\n",
       " 'hornqvist',\n",
       " 'elodi',\n",
       " 'windowless',\n",
       " 'hayn',\n",
       " 'turi',\n",
       " 'omiso',\n",
       " 'gewerkschaft',\n",
       " 'starter',\n",
       " 'calamit',\n",
       " 'acaben',\n",
       " 'seelbach',\n",
       " 'roofer',\n",
       " 'gjeldand',\n",
       " 'efmarfauuazpad',\n",
       " 'concav',\n",
       " 'convoc',\n",
       " 'commentari',\n",
       " 'coniston',\n",
       " 'augustan',\n",
       " 'frow',\n",
       " 'voluptuari',\n",
       " 'tempranillo',\n",
       " 'shamsolvaezin',\n",
       " 'greenman',\n",
       " 'franquismo',\n",
       " 'repouss',\n",
       " 'giladi',\n",
       " 'bronco',\n",
       " 'saleswoman',\n",
       " 'hoaxer',\n",
       " 'minicomput',\n",
       " 'assicurandoli',\n",
       " 'couloumbi',\n",
       " 'lytl',\n",
       " 'stromsto',\n",
       " 'majdalani',\n",
       " 'nnlf',\n",
       " 'subtyp',\n",
       " 'ghani',\n",
       " 'roesel',\n",
       " 'corfu',\n",
       " 'ulul',\n",
       " 'jimq',\n",
       " 'patrouillen',\n",
       " 'bekanntmachung',\n",
       " 'submunit',\n",
       " 'odessan',\n",
       " 'gastronomi',\n",
       " 'cgrueskin',\n",
       " 'boza',\n",
       " 'preferido',\n",
       " 'pirogova',\n",
       " 'tipperari',\n",
       " 'ballator',\n",
       " 'displeas',\n",
       " 'maldef',\n",
       " 'tonya',\n",
       " 'unrwa',\n",
       " 'frye',\n",
       " 'fronti',\n",
       " 'donata',\n",
       " 'pramoedya',\n",
       " 'mongabay',\n",
       " 'freakonom',\n",
       " 'metodista',\n",
       " 'pleasantri',\n",
       " 'spell',\n",
       " 'linsan',\n",
       " 'carplay',\n",
       " 'gotnew',\n",
       " 'brother',\n",
       " 'erinschrod',\n",
       " 'pumpenherstel',\n",
       " 'adeli',\n",
       " 'kfilli',\n",
       " 'keewaywin',\n",
       " 'woud',\n",
       " 'goodli',\n",
       " 'strahan',\n",
       " 'countercurr',\n",
       " 'madhwani',\n",
       " 'boeselag',\n",
       " 'mcafe',\n",
       " 'alhag',\n",
       " 'bohr',\n",
       " 'gorel',\n",
       " 'menthofuran',\n",
       " 'grammarian',\n",
       " 'hender',\n",
       " 'makeamericamexicoagain',\n",
       " 'penney',\n",
       " 'geposteten',\n",
       " 'appui',\n",
       " 'lhakim',\n",
       " 'hydropow',\n",
       " 'steenland',\n",
       " 'fakeid',\n",
       " 'sino',\n",
       " 'bossi',\n",
       " 'restent',\n",
       " 'mellom',\n",
       " 'zxua',\n",
       " 'prankish',\n",
       " 'usar',\n",
       " 'genitalia',\n",
       " 'menait',\n",
       " 'nimitz',\n",
       " 'arecibo',\n",
       " 'kalina',\n",
       " 'yimbi',\n",
       " 'searchabl',\n",
       " 'lockup',\n",
       " 'kinesthet',\n",
       " 'jraethefanat',\n",
       " 'michelang',\n",
       " 'sonnenfeld',\n",
       " 'dice',\n",
       " 'quipag',\n",
       " 'napoleon',\n",
       " 'sustantiva',\n",
       " 'ineffici',\n",
       " 'sikh',\n",
       " 'hagrid',\n",
       " 'bulhak',\n",
       " 'pompid',\n",
       " 'offload',\n",
       " 'cepr',\n",
       " 'unconcern',\n",
       " 'foibl',\n",
       " 'untest',\n",
       " 'monosodium',\n",
       " 'cellebrit',\n",
       " 'vasconi',\n",
       " 'shorelin',\n",
       " 'cuccinelli',\n",
       " 'hyon',\n",
       " 'sydney',\n",
       " 'stavropol',\n",
       " 'texasth',\n",
       " 'filipina',\n",
       " 'easyway',\n",
       " 'vlock',\n",
       " 'schaper',\n",
       " 'lemmonii',\n",
       " 'barranquilla',\n",
       " 'kushchyenko',\n",
       " 'ipap',\n",
       " 'usuncut',\n",
       " 'infograph',\n",
       " 'needlepoint',\n",
       " 'crire',\n",
       " 'cytoskeleton',\n",
       " 'inmuebl',\n",
       " 'lisa',\n",
       " 'inamov',\n",
       " 'fabricant',\n",
       " 'bemba',\n",
       " 'qarahunj',\n",
       " 'spelunk',\n",
       " 'pemsel',\n",
       " 'zenki',\n",
       " 'routefifti',\n",
       " 'prabhoo',\n",
       " 'saman',\n",
       " 'inattent',\n",
       " 'agallup',\n",
       " 'erteilt',\n",
       " 'micr',\n",
       " 'getti',\n",
       " 'xiaomi',\n",
       " 'titel',\n",
       " 'rhodiola',\n",
       " 'hypothec',\n",
       " 'nosocomi',\n",
       " 'yobra',\n",
       " 'time',\n",
       " 'linen',\n",
       " 'purr',\n",
       " 'corset',\n",
       " 'thirteen',\n",
       " 'raggedi',\n",
       " 'mtlichen',\n",
       " 'pinot',\n",
       " 'parkland',\n",
       " 'lythcott',\n",
       " 'ceil',\n",
       " 'existent',\n",
       " 'ruvkun',\n",
       " 'pipedream',\n",
       " 'znikn',\n",
       " 'cleofila',\n",
       " 'saeta',\n",
       " 'slivka',\n",
       " 'popul',\n",
       " 'naza',\n",
       " 'groundhog',\n",
       " 'leduc',\n",
       " 'harwel',\n",
       " 'anno',\n",
       " ...}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = pd.read_csv('data/reg_data_token.csv')\n",
    "voc_list = training.text.tolist()\n",
    "voc_list_str = [str(x) for x in voc_list]\n",
    "# voc_list_len = [len(x) for x in voc_list_str]\n",
    "voc_string = ''.join(voc_list_str)\n",
    "voc_set = set(voc_string.split(' '))\n",
    "try:\n",
    "    voc_set.remove('')\n",
    "except:\n",
    "    pass\n",
    "print('   ',len(voc_set))\n",
    "voc_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1539.312791613646 2361.826171146817 2 62702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(65.0, 695.0, 2373.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(np.mean(voc_list_len) , np.std(voc_list_len) , np.min(voc_list_len) , np.max(voc_list_len) )\n",
    "# np.quantile(voc_list_len,0.25) , np.quantile(voc_list_len,0.5) , np.quantile(voc_list_len,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1539.312791613646, 33006)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.mean(voc_list_len) , len(voc_list_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer words to Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/33006 [02:39<162:33:07, 17.73s/it]"
     ]
    }
   ],
   "source": [
    "# voc_list = list(voc_set)\n",
    "# num_df = pd.DataFrame(data={'term':voc_list})\n",
    "# train_X = []\n",
    "# for news in tqdm(voc_list_str):\n",
    "#     trainX = []\n",
    "#     for term in news:\n",
    "#         try:\n",
    "#             idx = num_df[num_df['term'] == term].index[0]\n",
    "#         except:\n",
    "#             pass\n",
    "#         trainX.append(idx)\n",
    "#     train_X.append(np.array(trainX))\n",
    "# #     num_df\n",
    "# # num_df[num_df['term'] == 'ck'].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235.94686105325795 366.8819509536414 1 10268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10.0, 105.0, 361.0, 235.94686105325795)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(voc_list_str)\n",
    "tokens_enc = tokenizer.texts_to_sequences(voc_list_str)\n",
    "tokens_enc_len = [len(tokens) for tokens in tokens_enc]\n",
    "print(np.mean(tokens_enc_len) , np.std(tokens_enc_len) , np.min(tokens_enc_len) , np.max(tokens_enc_len) )\n",
    "np.quantile(tokens_enc_len,0.25) , np.quantile(tokens_enc_len,0.5) , np.quantile(tokens_enc_len,0.75) , np.mean(tokens_enc_len)\n",
    "# mean = 237\n",
    "# iqr = 885/887.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1.0\n",
      "    2.0\n",
      "    4.0\n",
      "    4.0\n",
      "    5.0\n",
      "    5.0\n",
      "    5.0\n",
      "    6.0\n",
      "    6.0\n",
      "    6.0\n",
      "    6.0\n",
      "    7.0\n",
      "    7.0\n",
      "    7.0\n",
      "    7.0\n",
      "    8.0\n",
      "    8.0\n",
      "    8.0\n",
      "    8.0\n",
      "    9.0\n",
      "    9.0\n",
      "    9.0\n",
      "    9.0\n",
      "    9.0\n",
      "    10.0\n",
      "    10.0\n",
      "    10.0\n",
      "    11.0\n",
      "    11.0\n",
      "    11.0\n",
      "    12.0\n",
      "    12.0\n",
      "    12.0\n",
      "    13.0\n",
      "    13.0\n",
      "    14.0\n",
      "    14.0\n",
      "    15.0\n",
      "    16.0\n",
      "    17.0\n",
      "    18.0\n",
      "    20.0\n",
      "    26.779999999998836\n",
      "    40.0\n",
      "    53.0\n",
      "    62.05000000000109\n",
      "    71.0\n",
      "    80.0\n",
      "    89.0\n",
      "    97.0\n",
      "    105.0\n",
      "    114.0\n",
      "    122.0\n",
      "    130.0\n",
      "    139.0\n",
      "    147.0\n",
      "    155.0\n",
      "    164.0\n",
      "    171.0\n",
      "    179.0\n",
      "    187.0\n",
      "    196.0\n",
      "    204.0\n",
      "    214.0\n",
      "    224.0\n",
      "    234.85000000000218\n",
      "    244.0\n",
      "    255.0\n",
      "    266.0\n",
      "    279.0\n",
      "    292.0\n",
      "    306.0\n",
      "    318.0\n",
      "    332.0\n",
      "    346.0\n",
      "    361.0\n",
      "    375.0\n",
      "    390.0\n",
      "    407.0\n",
      "    422.0\n",
      "    437.0\n",
      "    454.0\n",
      "    471.0\n",
      "    490.0\n",
      "    509.0\n",
      "    527.0\n",
      "    546.0\n",
      "    565.0\n",
      "    584.0\n",
      "    604.0\n",
      "    623.0\n",
      "    646.0\n",
      "    670.0\n",
      "    697.3700000000026\n",
      "    732.0\n",
      "    775.5500000000029\n",
      "    836.0\n",
      "    940.0\n",
      "    1106.8199999999997\n",
      "    1534.9099999999962\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print('   ',np.quantile(tokens_enc_len,i*0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(tokens_enc_len,0.9655)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33610, 33610)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY = training.label.tolist()\n",
    "len(trainY) , len(tokens_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49933, 49933)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 237\n",
    "train_X = []\n",
    "train_Y = []\n",
    "for tokens,label in zip(tokens_enc,trainY):\n",
    "    length = len(tokens)\n",
    "    iter_ = int(length / max_len)\n",
    "    if iter_ > 0:\n",
    "        temp = tokens\n",
    "        for it in range(iter_):\n",
    "#             bound = (it+1)*max_len\n",
    "            pre = temp[:max_len]\n",
    "            temp = temp[max_len:]\n",
    "#             print('   ',len(pre))\n",
    "            train_X.append(pre)\n",
    "            train_Y.append(label)\n",
    "            if (len(temp) <= max_len) and (len(temp)>=9):\n",
    "#                 print('   ',len(temp))\n",
    "                train_X.append(temp)\n",
    "                train_Y.append(label)\n",
    "    else:\n",
    "        if len(tokens) >= 9:\n",
    "            train_X.append(tokens)\n",
    "            train_Y.append(label)\n",
    "len(train_X) , len(train_Y)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_ = [len(x) for x in train_X]\n",
    "np.max(len_) , np.min(len_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49933, 237), (49933,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = pad_sequences(train_X, maxlen=max_len, padding= 'post' )\n",
    "train_Y = np.array(train_Y)\n",
    "train_X.shape , train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33610, 361), (33610,))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_X = pad_sequences(tokens_enc, maxlen=361, padding= 'post' )\n",
    "# train_Y = np.array(trainY)\n",
    "# train_X.shape , train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110894"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_valid_set(X_all, Y_all, percentage):\n",
    "    all_data_size = len(X_all)\n",
    "    valid_data_size = int(floor(all_data_size * percentage))\n",
    "\n",
    "    X_all, Y_all = _shuffle(X_all, Y_all)\n",
    "\n",
    "    X_train, Y_train = X_all[0:valid_data_size], Y_all[0:valid_data_size]\n",
    "    X_valid, Y_valid = X_all[valid_data_size:], Y_all[valid_data_size:]\n",
    "\n",
    "    return X_train, Y_train, X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shuffle(X, Y):\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "#     print(X.shape, Y.shape)\n",
    "    return (X[randomize], Y[randomize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_valid_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5888ce050459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_valid_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'split_valid_set' is not defined"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, valid_X, valid_Y = split_valid_set(train_X, train_Y, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(obj=(train_X,train_Y,valid_X,valid_Y),file=open('data/train_set.pkl','wb'))\n",
    "train_X, train_Y, valid_X, valid_Y = pickle.load(open('data/train_set.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 237)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 237, 128)     14194432    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 237, 64)      73792       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 237, 64)      256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 237, 128)     66048       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 237, 128)     512         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 128)          74112       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128)          512         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           dropout_1[0][0]                  \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           16448       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64)           256         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 192)          0           batch_normalization_8[0][0]      \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           12352       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64)           256         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            65          batch_normalization_9[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 14,456,065\n",
      "Trainable params: 14,454,913\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_len = train_X.shape[1]\n",
    "\n",
    "inputs = Input(shape=(max_len,))\n",
    "emb_vec = Embedding(input_dim=np.max(train_X),output_dim=128,input_length=max_len)(inputs)\n",
    "conv = Conv1D(64,kernel_size=(9,), strides=1, padding='causal', data_format='channels_last')(emb_vec)\n",
    "bn = BatchNormalization()(conv)\n",
    "BiLSTM = Bidirectional(LSTM(64,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(bn)#64\n",
    "bn = BatchNormalization()(BiLSTM)\n",
    "rnn = Bidirectional(GRU(64,return_sequences=False,dropout=0.2,recurrent_dropout=0.2))(bn) #64\n",
    "bn1 = BatchNormalization()(rnn) # +FC*3? selu? 64? 3rd Dense shortcut from rnn/? kernel_initializer? dropout\n",
    "dense = Dense(128,activation='selu',kernel_initializer='lecun_normal')(bn1)\n",
    "bn2 = BatchNormalization()(dense)\n",
    "do = Dropout(0.3)(bn2)\n",
    "bn = Concatenate()([do,bn1])\n",
    "dense = Dense(64,activation='selu',kernel_initializer='lecun_normal')(bn)\n",
    "do = Dropout(0.4)(dense)\n",
    "bn = BatchNormalization()(do)\n",
    "bn = Concatenate()([bn,bn2])\n",
    "dense = Dense(64,activation='selu',kernel_initializer='lecun_normal')(bn)\n",
    "do = Dropout(0.5)(dense)\n",
    "bn = BatchNormalization()(do)\n",
    "output = Dense(1,activation='sigmoid')(bn)\n",
    "\n",
    "model = Model(inputs,output)\n",
    "# model = multi_gpu_model(model,gpus=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 237\n"
     ]
    }
   ],
   "source": [
    "opt=Adam() #Nadam()\n",
    "batchSize=1024 #32\n",
    "patien=25\n",
    "epoch=1000\n",
    "# hidden_dims=128\n",
    "# io_dim=128\n",
    "# input_lengths=train_X.shape[1] #profile_Q3\n",
    "# output_lengths= train_Y2.shape[1]#rep_max size\n",
    "# depths=1\n",
    "# dp = 0.01\n",
    "saveP = 'model/Reg_keras.h5'\n",
    "logD = './model/logs/'\n",
    "history = History()\n",
    "print(\"input:\",train_X.shape[1])#,'output_length:',train_Y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44939 samples, validate on 4994 samples\n",
      "Epoch 1/1000\n",
      "44939/44939 [==============================] - 98s 2ms/step - loss: 0.1687 - mean_absolute_error: 0.2944 - val_loss: 0.1009 - val_mean_absolute_error: 0.1437\n",
      "\n",
      "Epoch 00001: val_mean_absolute_error improved from inf to 0.14366, saving model to model/Reg_keras.h5\n",
      "Epoch 2/1000\n",
      "44939/44939 [==============================] - 92s 2ms/step - loss: 0.0640 - mean_absolute_error: 0.1333 - val_loss: 0.0679 - val_mean_absolute_error: 0.1106\n",
      "\n",
      "Epoch 00002: val_mean_absolute_error improved from 0.14366 to 0.11064, saving model to model/Reg_keras.h5\n",
      "Epoch 3/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0417 - mean_absolute_error: 0.0951 - val_loss: 0.0664 - val_mean_absolute_error: 0.1051\n",
      "\n",
      "Epoch 00003: val_mean_absolute_error improved from 0.11064 to 0.10509, saving model to model/Reg_keras.h5\n",
      "Epoch 4/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0332 - mean_absolute_error: 0.0801 - val_loss: 0.0640 - val_mean_absolute_error: 0.0993\n",
      "\n",
      "Epoch 00004: val_mean_absolute_error improved from 0.10509 to 0.09929, saving model to model/Reg_keras.h5\n",
      "Epoch 5/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0248 - mean_absolute_error: 0.0661 - val_loss: 0.0653 - val_mean_absolute_error: 0.0995\n",
      "\n",
      "Epoch 00005: val_mean_absolute_error did not improve from 0.09929\n",
      "Epoch 6/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0192 - mean_absolute_error: 0.0555 - val_loss: 0.0637 - val_mean_absolute_error: 0.0988\n",
      "\n",
      "Epoch 00006: val_mean_absolute_error improved from 0.09929 to 0.09879, saving model to model/Reg_keras.h5\n",
      "Epoch 7/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0155 - mean_absolute_error: 0.0483 - val_loss: 0.0645 - val_mean_absolute_error: 0.0984\n",
      "\n",
      "Epoch 00007: val_mean_absolute_error improved from 0.09879 to 0.09836, saving model to model/Reg_keras.h5\n",
      "Epoch 8/1000\n",
      "44939/44939 [==============================] - 92s 2ms/step - loss: 0.0124 - mean_absolute_error: 0.0420 - val_loss: 0.0671 - val_mean_absolute_error: 0.1009\n",
      "\n",
      "Epoch 00008: val_mean_absolute_error did not improve from 0.09836\n",
      "Epoch 9/1000\n",
      "44939/44939 [==============================] - 92s 2ms/step - loss: 0.0105 - mean_absolute_error: 0.0374 - val_loss: 0.0659 - val_mean_absolute_error: 0.0998\n",
      "\n",
      "Epoch 00009: val_mean_absolute_error did not improve from 0.09836\n",
      "Epoch 10/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0090 - mean_absolute_error: 0.0343 - val_loss: 0.0675 - val_mean_absolute_error: 0.1009\n",
      "\n",
      "Epoch 00010: val_mean_absolute_error did not improve from 0.09836\n",
      "Epoch 11/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0079 - mean_absolute_error: 0.0318 - val_loss: 0.0690 - val_mean_absolute_error: 0.1020\n",
      "\n",
      "Epoch 00011: val_mean_absolute_error did not improve from 0.09836\n",
      "Epoch 12/1000\n",
      "44939/44939 [==============================] - 92s 2ms/step - loss: 0.0069 - mean_absolute_error: 0.0297 - val_loss: 0.0702 - val_mean_absolute_error: 0.1037\n",
      "\n",
      "Epoch 00012: val_mean_absolute_error did not improve from 0.09836\n",
      "Epoch 13/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0065 - mean_absolute_error: 0.0285 - val_loss: 0.0718 - val_mean_absolute_error: 0.1052\n",
      "\n",
      "Epoch 00013: val_mean_absolute_error did not improve from 0.09836\n",
      "Epoch 14/1000\n",
      "44939/44939 [==============================] - 92s 2ms/step - loss: 0.0063 - mean_absolute_error: 0.0280 - val_loss: 0.0726 - val_mean_absolute_error: 0.1056\n",
      "\n",
      "Epoch 00014: val_mean_absolute_error did not improve from 0.09836\n",
      "Epoch 15/1000\n",
      "44939/44939 [==============================] - 92s 2ms/step - loss: 0.0059 - mean_absolute_error: 0.0275 - val_loss: 0.0727 - val_mean_absolute_error: 0.1075\n",
      "\n",
      "Epoch 00015: val_mean_absolute_error did not improve from 0.09836\n",
      "Epoch 16/1000\n",
      "44939/44939 [==============================] - 92s 2ms/step - loss: 0.0057 - mean_absolute_error: 0.0269 - val_loss: 0.0705 - val_mean_absolute_error: 0.1033\n",
      "\n",
      "Epoch 00016: val_mean_absolute_error did not improve from 0.09836\n",
      "Epoch 17/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0052 - mean_absolute_error: 0.0257 - val_loss: 0.0680 - val_mean_absolute_error: 0.1005\n",
      "\n",
      "Epoch 00017: val_mean_absolute_error did not improve from 0.09836\n",
      "Epoch 18/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0047 - mean_absolute_error: 0.0240 - val_loss: 0.0684 - val_mean_absolute_error: 0.1023\n",
      "\n",
      "Epoch 00018: val_mean_absolute_error did not improve from 0.09836\n",
      "Epoch 19/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0043 - mean_absolute_error: 0.0235 - val_loss: 0.0674 - val_mean_absolute_error: 0.1005\n",
      "\n",
      "Epoch 00019: val_mean_absolute_error did not improve from 0.09836\n",
      "Epoch 20/1000\n",
      "44939/44939 [==============================] - 92s 2ms/step - loss: 0.0037 - mean_absolute_error: 0.0220 - val_loss: 0.0663 - val_mean_absolute_error: 0.0995\n",
      "\n",
      "Epoch 00020: val_mean_absolute_error did not improve from 0.09836\n",
      "Epoch 21/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0036 - mean_absolute_error: 0.0212 - val_loss: 0.0661 - val_mean_absolute_error: 0.0982\n",
      "\n",
      "Epoch 00021: val_mean_absolute_error improved from 0.09836 to 0.09821, saving model to model/Reg_keras.h5\n",
      "Epoch 22/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0033 - mean_absolute_error: 0.0205 - val_loss: 0.0685 - val_mean_absolute_error: 0.1002\n",
      "\n",
      "Epoch 00022: val_mean_absolute_error did not improve from 0.09821\n",
      "Epoch 23/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0035 - mean_absolute_error: 0.0208 - val_loss: 0.0686 - val_mean_absolute_error: 0.1009\n",
      "\n",
      "Epoch 00023: val_mean_absolute_error did not improve from 0.09821\n",
      "Epoch 24/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0033 - mean_absolute_error: 0.0204 - val_loss: 0.0696 - val_mean_absolute_error: 0.1021\n",
      "\n",
      "Epoch 00024: val_mean_absolute_error did not improve from 0.09821\n",
      "Epoch 25/1000\n",
      "44939/44939 [==============================] - 92s 2ms/step - loss: 0.0033 - mean_absolute_error: 0.0200 - val_loss: 0.0672 - val_mean_absolute_error: 0.0991\n",
      "\n",
      "Epoch 00025: val_mean_absolute_error did not improve from 0.09821\n",
      "Epoch 26/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0030 - mean_absolute_error: 0.0194 - val_loss: 0.0668 - val_mean_absolute_error: 0.0989\n",
      "\n",
      "Epoch 00026: val_mean_absolute_error did not improve from 0.09821\n",
      "Epoch 27/1000\n",
      "44939/44939 [==============================] - 92s 2ms/step - loss: 0.0027 - mean_absolute_error: 0.0183 - val_loss: 0.0682 - val_mean_absolute_error: 0.1008\n",
      "\n",
      "Epoch 00027: val_mean_absolute_error did not improve from 0.09821\n",
      "Epoch 28/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0028 - mean_absolute_error: 0.0185 - val_loss: 0.0691 - val_mean_absolute_error: 0.1026\n",
      "\n",
      "Epoch 00028: val_mean_absolute_error did not improve from 0.09821\n",
      "Epoch 29/1000\n",
      "44939/44939 [==============================] - 92s 2ms/step - loss: 0.0026 - mean_absolute_error: 0.0180 - val_loss: 0.0676 - val_mean_absolute_error: 0.0997\n",
      "\n",
      "Epoch 00029: val_mean_absolute_error did not improve from 0.09821\n",
      "Epoch 30/1000\n",
      "44939/44939 [==============================] - 93s 2ms/step - loss: 0.0024 - mean_absolute_error: 0.0174 - val_loss: 0.0677 - val_mean_absolute_error: 0.1004\n",
      "\n",
      "Epoch 00030: val_mean_absolute_error did not improve from 0.09821\n",
      "Epoch 31/1000\n",
      "44939/44939 [==============================] - 91s 2ms/step - loss: 0.0024 - mean_absolute_error: 0.0173 - val_loss: 0.0675 - val_mean_absolute_error: 0.1010\n",
      "\n",
      "Epoch 00031: val_mean_absolute_error did not improve from 0.09821\n",
      "Epoch 00031: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "callback=[\n",
    "    ReduceLROnPlateau(monitor='loss', factor=0.5, patience=int(patien/3),min_lr=1e-6,mode='min' ),\n",
    "    EarlyStopping(patience=patien,monitor='val_loss',verbose=1),\n",
    "    ModelCheckpoint(saveP,monitor='val_mean_absolute_error',verbose=1,save_best_only=True, save_weights_only=True),\n",
    "    TensorBoard(log_dir=logD), \n",
    "    history,\n",
    "]\n",
    "model.fit(train_X, train_Y,\n",
    "                epochs=epoch,\n",
    "                batch_size=batchSize,\n",
    "                shuffle=True,\n",
    "                validation_data=(valid_X, valid_Y),\n",
    "                callbacks=callback, \n",
    "#                 class_weight='auto'\n",
    "                )\n",
    "model.save(saveP+\"_all.h5\") #184sec/0.04/0.19\n",
    "# 127s 3ms/step - loss: 0.0072 - mean_absolute_error: 0.0380 - val_loss: 0.0920 - val_mean_absolute_error: 0.1414"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd time\n",
    "* fix back lyers\n",
    "* lower lr\n",
    "* lower patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 237\n"
     ]
    }
   ],
   "source": [
    "opt=Nadam(lr=0.0005) \n",
    "batchSize=128\n",
    "patien=10\n",
    "epoch=100\n",
    "# hidden_dims=128\n",
    "# io_dim=128\n",
    "# input_lengths=train_X.shape[1] #profile_Q3\n",
    "# output_lengths= train_Y2.shape[1]#rep_max size\n",
    "# depths=1\n",
    "# dp = 0.01\n",
    "saveP = 'model/Reg_keras.h5' #2: val_mean_absolute_error did not improve from 0.10362\n",
    "logD = './model/logs/'\n",
    "history = History()\n",
    "print(\"input:\",train_X.shape[1])#,'output_length:',train_Y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44939 samples, validate on 4994 samples\n",
      "Epoch 1/100\n",
      "44939/44939 [==============================] - 1173s 26ms/step - loss: 0.0131 - mean_absolute_error: 0.0411 - val_loss: 0.0785 - val_mean_absolute_error: 0.1177\n",
      "\n",
      "Epoch 00001: val_mean_absolute_error improved from inf to 0.11774, saving model to model/Reg_keras.h5\n",
      "Epoch 2/100\n",
      "44939/44939 [==============================] - 1169s 26ms/step - loss: 0.0155 - mean_absolute_error: 0.0466 - val_loss: 0.0657 - val_mean_absolute_error: 0.1048\n",
      "\n",
      "Epoch 00002: val_mean_absolute_error improved from 0.11774 to 0.10485, saving model to model/Reg_keras.h5\n",
      "Epoch 3/100\n",
      "44939/44939 [==============================] - 1174s 26ms/step - loss: 0.0124 - mean_absolute_error: 0.0417 - val_loss: 0.0729 - val_mean_absolute_error: 0.1113\n",
      "\n",
      "Epoch 00003: val_mean_absolute_error did not improve from 0.10485\n",
      "Epoch 4/100\n",
      "44939/44939 [==============================] - 1167s 26ms/step - loss: 0.0103 - mean_absolute_error: 0.0380 - val_loss: 0.0737 - val_mean_absolute_error: 0.1164\n",
      "\n",
      "Epoch 00004: val_mean_absolute_error did not improve from 0.10485\n",
      "Epoch 5/100\n",
      "44939/44939 [==============================] - 1172s 26ms/step - loss: 0.0096 - mean_absolute_error: 0.0363 - val_loss: 0.0691 - val_mean_absolute_error: 0.1066\n",
      "\n",
      "Epoch 00005: val_mean_absolute_error did not improve from 0.10485\n",
      "Epoch 6/100\n",
      "44939/44939 [==============================] - 1175s 26ms/step - loss: 0.0083 - mean_absolute_error: 0.0333 - val_loss: 0.0739 - val_mean_absolute_error: 0.1125\n",
      "\n",
      "Epoch 00006: val_mean_absolute_error did not improve from 0.10485\n",
      "Epoch 7/100\n",
      "44939/44939 [==============================] - 1175s 26ms/step - loss: 0.0073 - mean_absolute_error: 0.0315 - val_loss: 0.0705 - val_mean_absolute_error: 0.1062\n",
      "\n",
      "Epoch 00007: val_mean_absolute_error did not improve from 0.10485\n",
      "Epoch 8/100\n",
      "44939/44939 [==============================] - 1176s 26ms/step - loss: 0.0066 - mean_absolute_error: 0.0298 - val_loss: 0.0715 - val_mean_absolute_error: 0.1048\n",
      "\n",
      "Epoch 00008: val_mean_absolute_error improved from 0.10485 to 0.10482, saving model to model/Reg_keras.h5\n",
      "Epoch 9/100\n",
      "44939/44939 [==============================] - 1173s 26ms/step - loss: 0.0052 - mean_absolute_error: 0.0266 - val_loss: 0.0691 - val_mean_absolute_error: 0.1046\n",
      "\n",
      "Epoch 00009: val_mean_absolute_error improved from 0.10482 to 0.10455, saving model to model/Reg_keras.h5\n",
      "Epoch 10/100\n",
      "44939/44939 [==============================] - 1169s 26ms/step - loss: 0.0040 - mean_absolute_error: 0.0241 - val_loss: 0.0674 - val_mean_absolute_error: 0.0993\n",
      "\n",
      "Epoch 00010: val_mean_absolute_error improved from 0.10455 to 0.09934, saving model to model/Reg_keras.h5\n",
      "Epoch 11/100\n",
      "18048/44939 [===========>..................] - ETA: 11:02 - loss: 0.0035 - mean_absolute_error: 0.0225"
     ]
    }
   ],
   "source": [
    "# model = load_model(saveP+\"_all.h5\")\n",
    "model.load_weights(saveP)\n",
    "model = multi_gpu_model(model,gpus=2)\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
    "\n",
    "callback=[\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=int(patien/1.5),min_lr=1e-6,mode='min' ),\n",
    "    EarlyStopping(patience=patien,monitor='val_loss',verbose=1),\n",
    "    ModelCheckpoint(saveP,monitor='val_mean_absolute_error',verbose=1,save_best_only=True, save_weights_only=True),\n",
    "    TensorBoard(log_dir=logD), \n",
    "    history,\n",
    "]\n",
    "model.fit(train_X, train_Y,\n",
    "                epochs=epoch,\n",
    "                batch_size=batchSize,\n",
    "                shuffle=True,\n",
    "                validation_data=(valid_X, valid_Y),\n",
    "                callbacks=callback, \n",
    "                class_weight='auto'\n",
    "                )\n",
    "model.save(saveP+\"_all.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "33610/33610 [==============================] - 81s 2ms/step - loss: 0.1581\n",
      "Epoch 2/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.1028\n",
      "Epoch 3/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0908\n",
      "Epoch 4/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0814\n",
      "Epoch 5/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0746\n",
      "Epoch 6/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0671\n",
      "Epoch 7/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0606\n",
      "Epoch 8/50\n",
      "33610/33610 [==============================] - 80s 2ms/step - loss: 0.0550\n",
      "Epoch 9/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0507\n",
      "Epoch 10/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0466\n",
      "Epoch 11/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0433\n",
      "Epoch 12/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0405\n",
      "Epoch 13/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0385\n",
      "Epoch 14/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0366\n",
      "Epoch 15/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0349\n",
      "Epoch 16/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0334\n",
      "Epoch 17/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0318\n",
      "Epoch 18/50\n",
      "33610/33610 [==============================] - 80s 2ms/step - loss: 0.0304\n",
      "Epoch 19/50\n",
      "33610/33610 [==============================] - 78s 2ms/step - loss: 0.0293\n",
      "Epoch 20/50\n",
      "33610/33610 [==============================] - 80s 2ms/step - loss: 0.0278\n",
      "Epoch 21/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0271\n",
      "Epoch 22/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0258\n",
      "Epoch 23/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0248\n",
      "Epoch 24/50\n",
      "33610/33610 [==============================] - 80s 2ms/step - loss: 0.0238\n",
      "Epoch 25/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0229\n",
      "Epoch 26/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0221\n",
      "Epoch 27/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0211\n",
      "Epoch 28/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0204\n",
      "Epoch 29/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0200\n",
      "Epoch 30/50\n",
      "33610/33610 [==============================] - 78s 2ms/step - loss: 0.0187\n",
      "Epoch 31/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0181\n",
      "Epoch 32/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0174\n",
      "Epoch 33/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0167\n",
      "Epoch 34/50\n",
      "33610/33610 [==============================] - 80s 2ms/step - loss: 0.0159\n",
      "Epoch 35/50\n",
      "33610/33610 [==============================] - 78s 2ms/step - loss: 0.0155\n",
      "Epoch 36/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0147\n",
      "Epoch 37/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0143\n",
      "Epoch 38/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0136\n",
      "Epoch 39/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0132\n",
      "Epoch 40/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0127\n",
      "Epoch 41/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0120\n",
      "Epoch 42/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0117\n",
      "Epoch 43/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0112\n",
      "Epoch 44/50\n",
      "33610/33610 [==============================] - 80s 2ms/step - loss: 0.0105\n",
      "Epoch 45/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0106\n",
      "Epoch 46/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0098\n",
      "Epoch 47/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0097\n",
      "Epoch 48/50\n",
      "33610/33610 [==============================] - 80s 2ms/step - loss: 0.0093\n",
      "Epoch 49/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0088\n",
      "Epoch 50/50\n",
      "33610/33610 [==============================] - 79s 2ms/step - loss: 0.0087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72c45dac18>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "model.fit(train_X,train_Y,epochs=50,batch_size=512) #GRU-BI，361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "49933/49933 [==============================] - 1237s 25ms/step - loss: 0.0943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72c4c6eb00>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "model.fit(train_X,train_Y) #GRU-BI，237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "33610/33610 [==============================] - 3810s 113ms/step - loss: 0.1102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72c85607b8>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "model.fit(train_X,train_Y) #LSTM-BI、885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "49933/49933 [==============================] - 1541s 31ms/step - loss: 0.0878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72c49be2e8>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "model.fit(train_X,train_Y) #LSTM-BI、237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "49933/49933 [==============================] - 80s 2ms/step - loss: 0.1553\n",
      "Epoch 2/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0988\n",
      "Epoch 3/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0762\n",
      "Epoch 4/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0539\n",
      "Epoch 5/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0459\n",
      "Epoch 6/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0351\n",
      "Epoch 7/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0291\n",
      "Epoch 8/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0257\n",
      "Epoch 9/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0232\n",
      "Epoch 10/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0208\n",
      "Epoch 11/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0195\n",
      "Epoch 12/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0180\n",
      "Epoch 13/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0170\n",
      "Epoch 14/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0162\n",
      "Epoch 15/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0151\n",
      "Epoch 16/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0146\n",
      "Epoch 17/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0140\n",
      "Epoch 18/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0125\n",
      "Epoch 19/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0121\n",
      "Epoch 20/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0116\n",
      "Epoch 21/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0108\n",
      "Epoch 22/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0105\n",
      "Epoch 23/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0096\n",
      "Epoch 24/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0089\n",
      "Epoch 25/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0089\n",
      "Epoch 26/50\n",
      "49933/49933 [==============================] - 78s 2ms/step - loss: 0.0084\n",
      "Epoch 27/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0077\n",
      "Epoch 28/50\n",
      "49933/49933 [==============================] - 79s 2ms/step - loss: 0.0071\n",
      "Epoch 29/50\n",
      "49933/49933 [==============================] - 76s 2ms/step - loss: 0.0069\n",
      "Epoch 30/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0064\n",
      "Epoch 31/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0062\n",
      "Epoch 32/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0056\n",
      "Epoch 33/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0057\n",
      "Epoch 34/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0050\n",
      "Epoch 35/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0047\n",
      "Epoch 36/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0044\n",
      "Epoch 37/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0041\n",
      "Epoch 38/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0040\n",
      "Epoch 39/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0038\n",
      "Epoch 40/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0036\n",
      "Epoch 41/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0034\n",
      "Epoch 42/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0035\n",
      "Epoch 43/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0028\n",
      "Epoch 44/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0029\n",
      "Epoch 45/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0028\n",
      "Epoch 46/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0026\n",
      "Epoch 47/50\n",
      "49933/49933 [==============================] - 75s 2ms/step - loss: 0.0024\n",
      "Epoch 48/50\n",
      "49933/49933 [==============================] - 75s 1ms/step - loss: 0.0024\n",
      "Epoch 49/50\n",
      "49933/49933 [==============================] - 75s 2ms/step - loss: 0.0024\n",
      "Epoch 50/50\n",
      "49933/49933 [==============================] - 74s 1ms/step - loss: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72c451f940>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "model.fit(train_X,train_Y,epochs=50,batch_size=512) #LSTM-BI、237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(input_shape=(None, N_features)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
